# dWeb Whitepaper
##### Authored By: Jared Rice Sr.
##### Date Published: August 26th, 2019
##### Date Revised: December 10th, 2020

## Preface
The World Wide Web, while it was once thought to be somewhat decentralized, has been transformed over the years into an overly centralized cesspool of authoritarians that are ultimately controlled by governments from around the world, as well as a handful of globally recognized tech companies. That was not how the visionaries of the web intended for it to be, nor would any of them agree with how the web is being ran today.

Beyond the evidence that was revealed by Booz Allen Hamilton contractor Edward Snowden, understanding the fact that the CIA, NSA and various private sector companies, including Google, Facebook and others are highly involved in manipulating mass thought, is crucial, in understanding why the web has to be reimagined [ACKE14]. The clear Chinese-based control over these companies, along with many elected officials within our government has become obvious to many Americans, which should put into plain English why all of the mass censorship and massive election interference is taking place.

It is us, the denizens of the World Wide Web, that have become extremely reliant on how quickly we can communicate with friends and access information via the WWW's many facilities. Although, it is during the process of these various communications and periods of research, that our information, our actions and even our reactions, are collected and then stored within the databases of not only the providers of these facilities but the "virtual, centralized grand database" once envisioned by President Reagan's former national security advisor John Poindexter [ROSE02]. It was Poindexter who was the brainchild of the "Total Information Awareness" program, which became the NSA's Advanced Research and Development Activity (ARDA), was later renamed "Terrorism Information Awareness" [IWAR] and eventually reincarnated as an alliance between the government and Silicon Valley for-profit companies. Enter Booz Allen Hamilton and other for-profit companies like Palantir, ran by so called "libertarians" like Peter Theil, who are paid by the CIA and the NSA to spy on and gather data  about law-abiding American citizens in the name of "terrorism" [GREE13].

History clearly shows us, even through the admissions of companies like Facebook and academics at Cornell University that they're clearly running psychological experiments on their users [ROBI14], [KRAM14], [GOEL14], [KRISH14]. While they do what all CIA and NSA contractors do and attempt to cover-up their tyrannical behavior with their typical talking points of "civil liberties" and "improvements to their service," it is clear to most that they're actively doing more than simply attempting to improve their services - and the dog regarding their care for our "civil liberties" will never hunt with any true American patriot.

Companies like Facebook, who have clear relationships with the Chinese Government, are far more interested in controlling what we see and quite frankly, what we believe and it's these constant internal studies that allow them to do just that. For years, these private sector companies have been developing internal algorithms, that are used each and every day to study your habits, your mannerisms and other psychological traits in the name of helping improve their various advertising services. What truly opened the eyes of Silicon Valley brainchilds like Mark Zuckerberg though, had little to do with predicting what you would buy - these authoritarians were blown away when their own studies revealed that voting behavior could be influenced by "undetectable social networking maneuvering" [LANI14].

This research I just mentioned happened in 2014 and don't worry, the source I just cited was the ***New York Times*** - just in case they say there is not "widespread evidence of voter manipulation." That's why I felt it was important to cite a New York Times article, regarding a study that Facebook conducted themselves! Facebook and Google's interest in our elections has grown extensively since 2014, where they collectively contributed hundreds of millions of dollars to local governments and various companies, in an attempt to support candidates that had unusual relationships with the Chinese government, like Joe Biden, whose son is the subject of an active FBI investigation for his dealings with Chinese companies as of December 10th, 2020.

What they used to refer as "undetectable maneuvering" is no longer truly undetectable, due to the fact that social networks are not only quietly choosing what you see in your news feed, but are now openly censoring those whose posts rail against the political propaganda machine most Silicon Valley-based social networks have become. This also takes place within search engines like Google, where conservative news stories and websites are rarely shown to those they know are registered democrats (look no further than the blacklisted Hunter Biden story). Their efforts to interfere in the 2020 election were extraordinary and quite obvious to anyone who is concerned with freedom in America.

All of this is made possible by a clear invasion of our privacy that we have allowed to go on for as long as most of us have been alive. This disregard for our own privacy has allowed companies in the private sector to gather data from every corner of our personalities, our lives and our interactions, which they've sold back to the government, in a clear violation of our Fourth Amendment rights. They've used this data to build a clear profile of all of us. They know every African American democrat from Dallas, Texas that ate McDonalds last week, while watching Netflix and they'll make sure that each and every one of them are fed anti-American leftist propaganda until each and every one of them join on with the "American woke."

It was this sort of invasion of privacy that the English administrators over the American colonies took part in, which in turn, according to John Adams, spirited the American Revolution, as much as any other factor. A young John Adams in 1761 once documented a speech of James Otis saying "every man of a crowded audience appeared to me to go away, as I did, ready to take up arms against writs of assistance." Otis was delivering a speech where he openly denounced the "writs of assistance," a clear and open demand for privacy, which became a clear motivator in the establishment of our republic. I believe the invasion of privacy we face today, far exceeds what any of our founders could have imagined. Alice Walker once said, "the most common way people give up their power, is by thinking they don't have any," but they have also clearly given up their rights to tech companies across our land, in exchange for convenience and style. Who could have imagined that?

I have said, for quite some time, that we're overdue for another revolution, but this time, a portion of that revolution will certainly be digital. As Chief John Roberts fairly pointed out in Riley vs. California, the Fourth Amendment doesn't simply cover our physical homes, but our digital ones as well. Likewise, true American patriots don't face just a violent physical revolution in order to secure their freedoms, but a digital one as well as and they'll have to have a place in the digital realm, where their voices can be heard, without any possible interference from the left. I say that because building alternatives on the centralized web to Facebook and Google, is not a long-term solution, nor is it a viable one. Their control over us on the web, goes much further than what we see or what we read. They now control our companies, our domain names, what we publish, what we sell and yes, our ability to accept online payments.

Andrew Torba, the founder of Gab, a Facebook alternative for conservatives, has already been banned from most online payment platforms, has had his domain name seized, his servers taken offline and even his family members have been blacklisted by VISA. While that sounds like a conspiracy theory, it's an unimaginable reality that is taking place with companies like Gab today. Organizations like ICANN and ARIN, who control IP address delegation amongst Internet Service Providers and who became domain registrars, are quickly becoming activists for the left's causes and enacting policies that will soon force domain registrars like GoDaddy and web hosting providers like Amazon Web Services (AWS), to move on from WWW-based facilities that allow for the distribution of information that goes against the left's propaganda, which is exactly what Gab is used for and it's why Gab was used as a test-case in their effort to shun conservative facilities form the web itself.

Beyond just the web and the threat of ICANN, ARIN and quite frankly Biden's FCC silencing conservatives, is the additional threat of Google and Apple doing the same to mobile applications like Ales Jone's InfoWars, which was abruptly removed from both app stores once it surpassed CNN as the most downloaded news application, within hours of its launch. To put all of this into perspective, companies in Silicon Valley are selling user data to communist governments, running psychological experiments on their users, silencing those who expose their spread of propaganda, interfering in our American elections and are now forming a powerful resistance to alternative facilities that emerge as a result of their total disregard for American civil liberties.

This is why I have embarked on a journey for the past several years, to reimagine the web and build a medium where the people themselves are in control; instead of governments and/or the private sector. A quick look at how long this GitHub profile has been around as well as the amount of research that has gone or here, should reveal that to any open-minded layman. The point of this paper is to make the case for a decentralized alternative to the web that we've been developing for the past four years, known as the "dWeb." It will also explain how the dWeb works and will lay down the clear scientific and mathematical proof, that ensures that the dWeb itself, nor anything on it, can be altered or removed by a single entity, other than the author. At the very same time, this paper will also explain, in-depth, how an elected governance, a distributed election system and a decentralized reporting system are used to ensure that the people of the dWeb, as well as the elected governance, have the delegated authority to remove illegal activity, as well as illegal content, without any centralized systems that could otherwise be used to corrupt and take control of the dWeb by bad actors in the future, just as they have with the traditional web.

While the dWeb facilitates protections from the tyrannical and authoritarian actions of government, as well as the private sector, it also presents technological advances that allow for the global distribution of websites, web applications and various forms of data, without the need for costly infrastructure or the threat of hackers. Those advances, as well as others, are covered in this paper as well.

The need for a decentralized web, during a time where the web has become over-centralized, has become obvious to the many who avidly use the web and have witnessed the constant violation of the human, civil and constitutional rights of the web's users. The influx of countries and their governments whom use the web, as well as the Internet at large, to spy on their citizens has gotten out of control. The lack of privacy is truly alarming and so are the security issues, where even central banks are still unable to protect worthless fiat, after over 20 years of attempting to secure their networks. Now social networks and search engines want to get involved in politics, control what we find or view on their networks and will delete our entire digital existence over a policy violation. Big tech and tyrannical governments like our own know that software developers are building ways to avoid their tactics and are using policies, as well as regulations, to scare of innovators in an attempt to maintain power and keep their usual control mechanisms in place.

Developers, innovators and disseminators of information need a better protocol. They need a completely decentralized web where their hard work can never be destroyed or shutdown by fearful organizations or governments. They simply need a better web where they can experience true privacy, freedom, security and transparency digitally, for the first first time since the inception of the web itself. Isn't that what the web was created for? It's what the dWeb was created for, only this time, we didn't forget to hardcode those rules into its immutable and irreversible existence.

Welcome to the dWeb.

"There is no central control."
***- Paul Baran - Godfather of packet switching***

"My bias was to always build decentralizations into the net. That way it would be hard for one group to gain control. I didn't trust large central organizations. It was just in my nature to distrust them."
***- Bob Taylor - ARPANET Engineer***

**Note:**
As an added plus, the dWeb is alive as you're reading this and can never be taken offline, as long as people like you are using it. You can start browsing the dWeb by downloading dBrowser [here](https://dbrowser.com) and can begin following our efforts to launch truly decentralized facilities like [dSocial](https://github.com/peepsx/dsocial-whitepaper) and [dSearch](https://peepsx.com/dsearch).


## Abstract
The dWeb is a decentralized web that is formed by a foundation of off-chain, peer-to-peer networking, data communication and data integrity protocols, which enable the exchange and cryptographic validation of data amongst peers. Many dWeb services like dDNS and its reporting system are layered on top of the ARISEN blockchain protocol suite in order to utilize ARISEN's universal authentication layer, public key authority, distributed virtual machine, decentralized network consensus, decentralized payment processing, as well as its on-chain data persistence layer.

This marriage between the dWeb and ARISEN at dWeb's low level service layers, gives way to a powerful ecosystem that can be used to develop secure, serverless and globally scalable applications that are decentralized from end-to-end. The dWeb improves upon the limitations and weaknesses of the World Wide Web, while ensuring that tyrannical governments and private sector companies are unable to regulate or control any aspect of the dWeb itself, without the approval of the dWeb's users. The dWeb is a futuristic web where infrastructure costs are nil, hackers are rendered useless, and the users themselves are placed in control of their data, along with the delegated authority to elect a governance that protects the dWeb at-large from fraud, illegal activity and illegal content, per the dWeb's ratified [Constitution](https://github.com/distributedweb/constitution).

This whitepaper explains the dWeb in-depth, from its foundational protocols and ARISEN's protocol suite, to dWeb-based services like dDNS and its decentralized reporting system.

## Foundational Protocols
The dWeb's foundation of off-chain, peer-to-peer networking, data communications and data integrity protocols, enable peers to announce data, remote peers to discover and swarm data, exchange data and validate the integrity of the data that is ultimately fetched via a dataset's swarm of peers. The following sub-sections will explain each of these foundational protocols and how their reference implementations function.

### DWDHT
DWDHT, an acronym for "dWeb DHT," is a protocol that forms a distributed hash table of dataset identifiers (dWeb network addresses) and the peers that are announcing them. Each database identifier and its announcing peers form what is referred to as a "swarm," which can be queried from connected peers or joined (announced) by a specified amount of peers. dWeb's DHT is literally how a particular dataset is discovered and downloaded from the peers that are openly seeding it (announcing it). Each peer who is announcing a dataset on a long-term basis, by design, becomes a DHT node and is responsible for storing a specific portion of dWeb's DHT. Put another way, each DHT node stores a specific portion of dWeb's DHT. Put another way, each DHT node stores a specific portion of the dWeb's dataset identifiers, along with the peers who currently posses the data related to each of those data identifiers.

While each DHT node stores various swarms, it also stores information regarding other DHT nodes, identified by `node identifiers`, which can be mathematically determined to possess information regarding a specific swarm. dWeb's DHT is based on the `Kademilia Distributed Hash Table` popularized by BitTorrent and others. Nodes and data in dWeb's DHT are assigned 160-bit integers as IDs, while swarms are stored in the form of key-value pairs, where the key is a 32 byte value generated by a one-way hash function, such as SHA-1 (normally a dWeb network address), where the value is an object containing public and LAN-based peers that are announcing the key (the dataset identifier).

#### Node & Data Distribution
The DHT defines the distance between 2 DHT nodes `i` and `j` by the bitwise exclusive OR operations `(XOR)`, i.e., `d(i,j) = i (XOR) j`. This distance means for any given key `i` and a distance `L > 0`, there can only be a single key `j` that satisfies `d(i,j) = L` [ZHAN13].

All key-value pairs are stored on `k` nodes whose UIDs are closest to the actual key. `K` is an important parameter that helps determine data redundancy and how stable the DHT is at any time. Each node `i` always maintains multiple `k`-buckets. Each `k`-bucket stores a list of other DHT nodes, which are organized in an order that reflects the most recently active nodes. The node that is most recently active is stored at the tail, while the least active node is stored at the head.

The node whose distance from node `i` is in the range of [pow(2,m), pow(2,m+1)] is stored in the `m`th `k`-bucket (node that 0 < `m` < 160). The nodes in the `k`-buckets are regarded as the neighbors of the node `i`. A dWeb DHT node dynamically updates its neighbors upon receiving any messages from them. This process can be better explained more specifically, when node `i` receives a message from another DHT node `j`, which is located in the `m`th `k`-bucket, where the `k`-bucket of node `i`, will be updated in the following way:

-If `j` already exists in the `k`-bucket, `i` moves `j` to the tail of the list, as node `j` is the most recently seen.
-If `j` is not in the `k`-bucket and the bucket has fewer than `k` nodes, node `i` inserts `j` at the tail of the list.
-If the bucket is full, `i` PINGs the node at the head of that particular `k`-bucket.
-If the head node responds, node `i` makes it to the tail and ignores node `j`.
-Otherwise, `i` removes the head node and inserts `j` at the tail.

##### DHT Primitives
-`PING` - Probes a DHT node to check whether it's online or not.
-`STORE` - Used to store a key-value pair.
-`FIND_NODE` - Finds a set of nodes that are closest to a given node.
-`FIND_VALUE` - Operates like `FIND_NODE` but returns a stored value.

These RPC-like primitives work in a recursive way, which improves the efficiency of dWeb's DHT. A `lookup` procedure is initiated by the `FIND_NODE` and `FIND_VALUE` primitives, where the lookup initiator chooses `B` nodes from its closest `k`-buckets and send many parallel `FIND_NODE` requests to these `B` nodes. If the node being searched for in a given iteration is not found, the lookup initiator resends the `FIND_NODE` to the nodes that were found during the previous recursive operation and repeats this iterative functionality.

A KV pair may be stored on multiple DHT nodes. Thanks to the recursive procedure explained above, the key-value pair spreads across the DHT network every hour. This process insures that multiple replicas of data exist across the network. Every key-value is deleted 24 hours after it is initially pushed into the network.

##### Joining and Leaving
When node `i` joins dWeb's DHT, it is assumed that it is aware of or knows about node `j`. The joining process consists of multiple steps as follows:
-1. Node `i` inserts `j` into its `k`-buckets
-2. Node `i` starts a node lookup procedure for its own ID, where `i` is made aware of other newer nodes.
-3. Node `i` updates the `k`-buckets

During this process, node `i` strengthens its `k`-buckets and inserts itself into other nodes' `k`-buckets. When other nodes leave or fail, they DO NOT notify any other node. There is no need for a special procedure to cope with node departures, as these mechanisms insure that leaving nodes will be removed from the `k`-buckets.

#### Swarm Announcement
Announcing a swarm means that you're either: announcing a dWeb network address that doesn't exist on dWeb's DHT and therefore become the first peer related to the address; or announcing a dWeb network address that does exist on dWeb's DHT and are added to a list of peers who are announcing the address.

A swarm entry in dWeb's DHT, takes on the following format:

```
Key: 8f0ab2... (32 byte hexadecimal address)
= = = = = = = = = = = = = = = = = = = = = = = = =
Value:
{
  node: { host, port },
  peers: [{ host, port }, { host, port }]
  localPeers: [{ host, port }, { host, port }]
}
```

If a peer is announcing a dWeb network address that matches a key in the DHT, the entry is mutated to include the peer's announced IP address and port. When announcing a dWeb network address, a peer must set their public port and public IP address during the announcement and can choose to set a lan-based address and port as well, so that those on the same public IP who are performing a DHT lookup, can retrieve local announcers for a particular dWeb address. It's important to note that when announcing a dWeb network address on dWeb's DHT, the announcer becomes a DHT node on the network by design.

#### Swarm Lookups
A swarm can be looked up by its dWeb network address, which is a 32 byte buffer (normally a hash of something). When performing a lookup, the querying peer is temporarily an announcing peer, but is removed as an announcing peer (unannounces), once the lookup is finalized.

A lookup for a particular dWeb key, returns the following data:
```
{
  // The dWeb DHT node that is returning this data
  node: { host, port }
  // List of Peers
  peers: [{ host, port }, ...]
  // List of LAN Peers
  localPeers: [{ host, port }, ...]
}
```

#### DHT Bootstrap Nodes
dWeb's network utilized various `bootstrap` nodes to launch the initial dWeb network. These 3 bootstrap nodes are as follows:

-`dht1.dwebx.net`
-`dht2.dwebx.net`
-`dht3.dwebx.net`

These nodes going down would not affect the dWeb or a user's ability to announce or lookup dWeb network addresses, due to the way other DHT nodes on the network share data and information regarding each other. dWeb developers have the option of launching their own bootstrap nodes so that their apps have a point of entry into the dWeb's network of DHT nodes. Those nodes would initially use a set of already existing nodes to gain access to the DHT and would no longer need access to the nodes used for bootstrapping. Any node on the DHT can be used to bootstrap a new node.

#### DWDHT Reference Implementation
The DWDHT reference implementation was written in JavaScript and was used to launch the initial dWeb DHT. You can find it [here](https://github.com/distributedweb/dht).

You can use this reference implementation to allow dWeb-based applications (desktop, mobile or web) to act as DHT nodes.

You can also launch your own dWeb DHT node via the command-line, by using the DHT CLI [here](https://github.com/distributedweb/cli).


#### dWeb Swarm API
The dWeb swarm API, also known as "Swarm Programming," is a high level API built on top of the [DWDHT(#dwdht) Protocol used for finding and connecting to peers of a particular swarm and interacting with the dWeb DHT.

It expands the default set of parameters when creating a DHT node, adds specific swarm events where callbacks can be used to react to specific swarm-based events and introduces several functions, like `join`, `leave`, and `connect`, which make it easy to programmatically interact with the dWeb's DHT.

##### DHT Node Initiation Parameters
Using the dWeb Swarm API, a DHT node can be created with the `dwebswarm()` function, using the following parameters:

-`bootstrap` - An array of bootstrap servers used to initiate the dWeb DHT node.
-`ephemeral` - A boolean that is set to `false` if this is intended to be a long running DHT node.
-`maxPeers` - The total amount of peers that the node initiator will connect to.
-`maxServerSockets` - The number to restrict the number of server socket based peer connections. Set to `Infinity` by default.
-`maxClientSockets` - The number to restrict the number of client socket based peer connections. Set to `Infinity` by default.
-`validatePeer` - A function for applying filters before connecting to a peer.
-`queue` - An object for configuring peer management behavior.
--`requeue` - An array of backoff times in milliseconds every time a failing peer connection is retried.
--`forget` - An object for when to forget certain peer characteristics and treat them as fresh connections again.
---`unresponsive` - How long to wait before forgetting that a peer has become unresponsive.
---`banned` - How long to wait before forgetting that a peer has been banned.
---`multiplex` - Set to `true` in order to reuse existing connections between peers across multiple dWeb network addresses.

##### Swarm Event Emitters
Using the dWeb Swarm API, applications can listen for the following events:

-`connection` - A new connection has been created. Event should be handled by using the socket. This emits an `info` object that describes the connection using the following details:
```
-type (string) - Should be either "tcp" or "udp."
-client (boolean) - If true, the connection was initiated by this node.
-topics (array) - The list of dWeb network addresses associated with this connection, if "multiplex" was set to true, during DHT node initiation.
-peer (object) - Object describing peer (port, host, LAN peer details, referrer and the dWeb network address the peer was discovered under).
```
Using the `info` object, the `info.ban()` method can be called to ban the connected peer and will keep the DHT node from connecting to the peer in the future. The `info.backoff()` method can be called to get the DHT node to backoff from connecting to the peer.

-`disconnection` - A connection has been dropped. Emits an `info` object that is identical to the `connection` `info` object, describing the dropped connection.

-`peer` - A new peer has been discovered on the network and has been queued for connection. Emits a `peer` object, including the port, host referrer, and dWeb network address peer was discovered under. Also includes a boolean on whether the peer is a LAN-based peer.

-`peer-rejected` - A peer has been rejected as a connection candidate. Emits a `peer` object that is identical to the `peer` event's `peer` object, describing the rejected peer.

-`updated` - Emitted once a discovery cycle for a particular dWeb network address has completed. Emits a `key` object that identifies the dWeb network address the discovery cycle is related to.

For more information on dWeb Swarm's events, take a look at the README.md file within the reference implementation of the dWeb Swarm API [here](https://github.com/distributedweb/dwebswarm).

##### Swarm API Functions
There are several functions (methods in the reference JavaScript implementation) that can be used to interact with a dWeb DHT node. These functions and their required parameters are described below.

###### `join` - Join the swarm for a given `topic` (dWeb network address). This will cause peer to be discovered for the topic (`peer` event).

###### `join()` Parameters
-`topic` (Buffer) - The dWeb network address to list under. Must be 32 bytes in length.
-`options` (Object)
--`announce` (Boolean) - List this peer under the topic as a connectable target. Defaults to `false`.
--`lookup` (Boolean) - Look for peers in the topic and attempt to connect to them. If `announce` is false, this automatically becomes true.
-`onion` - A function that is called when your topic has been fully announced to the local network and the DHT.

###### `connect()` - Establish a connection to a given peer. You usually won't need to use this function, because the DHT node connects to discovered peers automatically. Accepts a `peer` object (identical to the `peer` object emitted by the `peer` event) for the peer the function is to establish a connection with. Also has a callback function that emits either a connection error (err), the established TCP or UDP socket (socket) and details regarding the established connection (details).

###### `leave` - Leave the swarm for a given dWeb network address.

###### `leave()` Parameters
-`topic` (Buffer) - The identifier of the peer-group to delist from.
-`onleave` - A function that is called when your topic has been fully unannounced to the local network and the DHT.

For more in-depth examples on how to call these functions within dWeb-based applications, view the `README.md` file within the reference JavaScript implementation [here](https://github.com/distributedweb/dwebswarm).

### dDatabase
A dDatabase is a distributed append-only log, also referred to as a distributed and immutable data feed, which can be exchanged between peers, using the [dDatabase Protocol](#ddatabaseprotocol) and validated via functions available in a dDatabase implementation.

At a very basic level, the dWeb is made up of dDatabase feeds, but it's important to note that a dWeb network address doesn't have to represent just a dataset, it can represent an entity like a peer or a device. Although, as the dWeb stands today, most of what you see distributed across it are dDatabases.

Below is a pseudo-representation of what a dDatabase would look like:
```
Index                 Entry
= = = = = = = = = = = = = = = = = = =
0                       Hello
1                       World
```

As you will see later in this paper with the [dDrive](#ddrive) Protocol, a distributed file system can be created using a dDatabase, by converting the content and metadata of a file system's files into a particular encoding format, such as binary, and storing in a dDatabase entry like so:
```
Index                 Entry
0                       {content:010110011110...}, {metadata:1110100101...}
1                       {content:11101001111001...}, metadata:0100110000...}
```

In other words, a dDatabase can easily be used to distribute an entire file system between peers, or any dataset for that matter. This section is intended to breakdown how dDatabases work, how data is determined to have the utmost integrity, and how data is exchanged between peers using the dDatabase Protocol.

A dDatabase, represented by a generated dWeb network address, can easily be announced and discovered via dWeb's DHT, where other peers can download the dDatabase from peers in the dDatabase's swarm who have download the data and subsequently announced their joining of the swarm itself. Put another way, a dDrive that contains a website can announce its dWeb network address on the dWeb DHT and peers can join the dDrive's swarm and thereby increase the number of peers the website can be downloaded (replicated) from. This is just an example, as a dDatabase can contain an endless amount of dataset types, such as an encrypted conversation or an encrypted voice call.

#### Distributed, Immutable Data Feed
dDatabase is a secure, distributed and append-only (immutable) feed that is designed for distributing large datasets and streams of real time data between peers of the dWeb.

#### Value Encoding
The value of an append entry to a dDatabase can be encoded into the following formats:

-`binary` (Default)
-`json`
-`utf-8`

#### Data Integrity
It is important that those who download a dDatabase can verify that it actually derived from its proclaimed author. This is made possible thanks to dDatabase's use of public key cryptography, which a dDatabase's dWeb network address is derived from, alongside Merkle trees and FIO trees, in order to verify a dDatabase's root hash signature(s). This insures that the data itself derived from the author of the dDatabase and the original announcer of the dWeb network address.

##### Merkle Trees and FIO Trees
A dDatabase uses a custom encoding method when laying out dDatabase-based update logs into a Merkle tree. This particular encoding method positions hashes into a scheme known as "binary in-order interval numbering," or just "bin" numbering for short. This is a deterministic method of positioning leaf nodes in a Merkle Tree.

A Merkle Tree with seven leaf nodes will always be laid out like this:
```
0L
      1L
2L
            3L
4L
      5L
6L
```

From a computer science perspective, dDatabases are binary append-only streams, the contents of which are cryptographically hashed and signed. Therefore, any dDatabase can be verified by anyone with access to the public key of the creator. Over the HTTP protocol, datasets are shared every day, although there is no built-in support for version control or the content-addressing of specific data. dDatabase is the solution to this, allowing multiple un-trusted devices to act as a single virtual host. For the dWeb to work, it requires a data structure that authenticates the content's integrity and one that keeps a historical log of the revisions - and dDatabase feeds certainly provide that.

dDatabases are identified internally by signed Merkle trees, are identified publicly over the dWeb by a public key, and are discovered over dWeb's DHT by a dWeb network address, which in turn derives from a dDatabase's public key. A dDatabase's public key is used to verify the signature that relates to the received data. A dDatabase's internal Merkle tree is output as a "Flat In-Order Tree" or "FIO Tree."

FIO trees, per PP5P RFC 7574, are defined as "bin numbers." These FIO trees allow for numerical-based identification of each leaf node within a binary-based Merkle tree and therefore create a simplistic way of representing a binary tree as a list. These properties of FIO trees are used in the simplification of "wire protocols" that of which utilize Merkle tree structures within distributed applications.

Below is an example FIO tree, that is sized at 4 blocks of data:
```
0L
1P
2L
3P
4L
5P
6L
```

In the above FIO tree example, even numbers (0, 2, 4, 6) represent leaf nodes on the tree and odd numbers represent parent-nodes that each contain two children.

Using binary notation, we can count the total number of "trailing 1s" to calculate the depth of the tree's nodes. For example, the following numbers below are converted to binary:
```
5 = 101
3 = 011
4 = 100
```

The number 5 has one trailing 1, the number 3 has two trailing 1s and the number 4 has zero trailing 1s. In the FIO example, 1 is the parent node of (0, 2), 5 is the parent node of (4, 6) and 3 is the parent node of (1, 5). The FIO tree would only have a single root if the leaf node count is a power of 2, otherwise a FIO tree will always have more than one root.

Below is another FIO tree (pseudo-representation) with a total of 6 leaf nodes:
```
0
1
2
3
4
5
6
7
8
9
10
```

In the above example, the roots are 3 and 9.

A Merkle tree, named after cryptographer and mathematician Ralph Merkle, is best described as a tree of binary-based data, where every "leaf" (an even-numbered tree node that has no children) is a hash of a data block and every "parent" (an odd-numbered tree node that has two children) is the hash of both of its children. dDatabases are feeds represented by Merkle trees that are ultimately encoded with "bin numbers."

For example, a dDatabase of 4 values would always map to the 0, 2, 4 and 6 leaf nodes. Below is a pseudo-representation:
```
fragment0 -> 0
fragment1 -> 2
fragment2 -> 4
fragment3 -> 6
```

When converting to FIO tree-style notation, a Merkle tree spanning these data blocks looks like:
```
0 = hash(0 + 2)
2 = hash(fragment1)
3 = hash(1 + 5)
4 = hash(fragment2)
5 = hash(4 + 6)
6 = hash(fragment3)
```

The even and odd nodes store different types of information:
-`Even Numbers` - List of data hashes [f0, f1, f2, ...]
-`Odd Numbers` - List of Merkle hashes (hashes of child nodes) [h0, h1, h2, ...]

The root node within a Merkle tree hashes the entire dataset. In the example of 4 fragments, node #3 hashes the entire dataset and node #3 is used to verify the rest of the dataset. Although the root node will change every time data is added to a dDatabase.

```
0
1
2
3 (root node)
4
5
6
7
8
9 (root node)
10
```

The Merkle tree's nodes are calculated as follows:
```
0 = hash(fragment0)
1 = hash(0 + 2)
2 = hash(fragment1)
3 = hash(1 + 5)
4 = hash(fragment2)
5 = hash(4 + 6)
6 = hash(fragment6)
7 = hash (6 + 8)
8 = hash(fragment4)
9 = hash(8 + 10)
10 = hash(fragment5)
```

When there are multiple root hashes, it is convenient to capture the entire state of a dDatabase as a `fixed-size hash` by hashing all of the root hashes into one single hash, where in the example above:

`root = hash(9 + 3)`
or
`tr = h(r1 + r2)`
(tr = top root; hash = hash function; r1 = root hash 1; r2 = root hash 2)

##### Root Hash Signatures
Merkle trees are used by dDatabases to create a way of indentifying the content of a dataset through hashes. The concept is simple, if the underlying content of a dDatabases changes, the hash changes. For example, a dDatabase acts as a list that calls the `append()` mutation when an entry is added to the database feed, thereby adding a new leaf to the tree, which ultimately generates a new root hash. When a dDatabase is created, a public/private keypair is generated. The public key is used as a public identifier for data validation, whereas the private key is used to sign the root hash every time a new one is generated. This digital signature is always distributed with the root hash to verify its integrity.

##### Data Validation
Data which is received that belongs to a dDatabase goes through the following process:
-1. The root hash's signature is verified.
-2. The received data is hashed with `ancestor hashes` in order to reproduce the `root hash`.
-3. If the root hash that was calculated is an exact match of the original root hash that was received, the data has been verified.

In an example of a dDatabase containing 4 values, our tree of hashes would look like this:
```
0
1
2
3 (root hash)
4
5
6
```

If we want to verify the data for 0 (fragment0), we first read (2), which is the sibling hash, then (5), which is the uncle hash and then (3), which is the signed root hash.

```
0 = hash(fragment0)
2 = (hash received)
1 = hash(0 + 2)
5 = (hash received)
3 = hash(1 + 5)
```

If what we calculate for 3 is equal to the signed root hash we received for 3, then fragment0 is valid.

It's important to note that all new signatures very the entire dDatabase since the signature spans all data in the Merkle tree. A dDatabase is considered corrupt if a signed mutation results in a conflict against previously verified trees. When a dDatabase is considered corrupt, the dDatabase protocol is designed to stop distribution.

##### Cryptography Specification
-The hash function uses BLAKE 2B-256 encryption, while signatures are ed25519 with the SHA-512 hash function.
-Hash function inputs are prefixed with different "constants" based on the type of data being hashed. The constants include:
--0x00 -Leaf
--0x01 -Parent
--0x02 -Root

This protects against a "second preimage attack."
-Hashes, a lot of the time, will include the sizes and indexes of their content so that the structure of a tree can be described along with the tree's data.

#### Data Replication
The easiest way to describe how dDatabase replication works between a set of peers using pseudo-code examples, is in the context of a file system - so we will use a dDrive abstraction in the following examples:

Let's say we create a dDrive and add two files to it:
-`jefferson.png`
-`trump.png`

Once those files are added to the dDrive, they are split into fragments, where the content data is represented within a constructed `content` object and the metadata is represented within a constructed `metadata` object, where each file itself is split into multiple dDatabase entries. Assuming `jefferson.png` and `trump.png` are both split into three fragments a piece, that each equate to a file size of 134 Kb, the representation would be placed in a list like this:

```
jefferson1.png
jefferson2.png
jefferson3.png
trump1.png
trump2.png
trump3.png
```

These fragments of the two image files each get hashed and those hashes are organized into a Merkle tree, like the example below:

```
0L                         -hash(jefferson1)
      1L                   -hash(0L + 2L)
2L                         -hash(jefferson2)
            3L             -hash(1L + 5L)
4L                         -hash(jefferson3)
      5L                   -hash(4L + 6L)
6L                         -hash(trump1)
8L                         -hash(trump2)
      9L                   -hash(8L + 10L)
10L                       -hash(trump3)
```

The next part of the process is to calculate the root hashes of the Merkle tree, which are `3L` and `9L`, then hash the number that derives from that. Lastly, we will cryptographically sign the `combined root hash` (sH). The signed hash is utilized to validate all other leaf nodes (parent/child) within the tree, where the signature easily proves who this dDrive was published by.

The pseudo-tree above is for the hashes that derive from the fragments of our two photos. A second Merkle tree is generated as a representation of the files and their metadata, known as a `metadata.log`.

An example of a metadata log-based Merkle tree is below:

```
0L - hash(contentLog: {`4f21f567...`})
1L - hash(0 + 2)
2L - hash({"jefferson.png", first: 0, length: 3})
4L - hash({"trump.png", first: 3, length: 3})
```

A few notes regarding the above pseudo-representation of the metalog's Merkle tree:
-The 1st entry in this log is the metadata entry point to a hash of the content log (the content log that the first Merkle tree represented).
-Notice that the 3rd leaf node is not included yet. That's because 3 is the hash of 1 + 5 and 5 does not exist yet, so it will be written at a later update.

All that's left in the replication process is to send the metadata to the other device (peer). The replicating peer-to-peer messaging protocol, known as the dDatabase Protocol, is tasked with communicating between peers over what is known as a "duplex binary channel." Below is a play-by-play of the communications between two peers, Bob and Alice.

-1. Bob sends the first message, known as the `announce` message that includes a dWeb network address.
-2. Alice sends Bob a `want` message, signaling that Alice wants all leaf nodes in the metadata log of the dDrive that Bob is announcing.
-3. Alice sends three `request` messages, one for each leaf node (0, 2, 4).
-4. Bob sends back three `data` messages. The data messages contain the `contentLog` key, the hash of the sibling, which in this case is (2), the hash of the uncle root (4), and the signature for the root hashes (1, 4).
-5. Alice can validate the integrity of the first data message by hashing the metadata received for the `contentLog` metadata to produce the hash for fragment0. She then hashes the leaf node (0), with the hash (2), that was included to reproduce hash (1) and hashes (1) with the value of (4). Lastly, the digital signature that was received is used in order to verify it was the same dDatabase that was originally requested.
-6. When the next `data` message is received, a similar replication process to the one shown above is performed to verify the content of the dDrive again. Alice now has a full list of the files in the dDrive and can choose which ones she wants to download.

#### dDatabase Data Structure
Data within a dDatabase is stored as `blocks`, which are indentified by an `index`. Each `block` is signed by its creator, so that an entire dDatabase feed can be audited, where all currently stored data matches the hashes in the Merkle tree. Put another way, all data blocks must match the hashes contained within the dDatabase Merkle tree, explained previously.

This data structure ensures that peers can download a specific block range, rather than the entire block base of the dDatabase itself. The `index` is an auto-incrementing number that is zero-based.

A pseudo-representation of a dDatabase feed uses binary encoding:
```
Index              Entry
0                    0111010010011...
1                    1011000110000...
```

As data is appended to the feed, a new index is created. Those who are live replicating the feed would receive the new index in real-time.

#### dDatabase Protocol
Before diving into the dDatabase Protocol, it is important to reiterate that a dDatabase is an append-only data feed, where the data within a data feed is an abstract "blob" of data.

**dDatabase feeds can:**
-Be distributed partially between peers.
-Be distributed fully between peers.
-Be distributed to/received by multiple peers at once.

The dDatabase Protocol is a process by which two or more peers exchange a dDatabase over binary duplex streams. Remote peers are able to identify with seeders of a dDatabase whether they are looking for specific portions of a dDatabase (partial) or the entire dDatabase.

Using various message types, peers are able to ask a specific peer for a specific portion of the feed, whereas the feed distributor(s) can signal whether they have that data and can subsequently fulfill the data request. The protocol uses a sender/handler lifecycle for message exchange and message handling.

Binary duplex streams can be encrypted, use a public/private keypair for stream authentication, and utilize a NOISE-based stream handshake by default.

This section will cover all of these aspects, including dDatabase's handshake phase and message exchange phase.

##### Handshake Phase
A dDatabase protocol duplex stream can use an optional NOISE-based handshake. For the handshake, NOISE uses the `XX` pattern. Each NOISE message is sent with variant framing. After the handshake is finalized between peers, a message exchange phase can begin.

The handshake phase allows two or more peers to authenticate each other and to securely negotiate an encryption and MAC algorithm, along with cryptographic keys to be used to protect the data sent between them. After the initial handshake transport, encryption is enabled to ensure a stream is private.

As an added plus, each NOISE session is unique and is identified by a unique `handshake hash` in order to enable channel binding.

##### Message Phase
The NOISE-based message exchange phase uses a basic variant length prefixed format to send messages over the wire.

All messages contain a header indicating the type and the feedID of the dDatabase feed, and a protobuf-encoded payload:

`message = header + payload`

A header is a variant that looks like this:

`header = feedID << 4 | numeric-type`

The `feedID` is just an incrementing number for every feed shared and the `numeric-type` corresponds to which protobuf schema should be used to decode the payload.

The message is then wrapped in another variant containing the length of the message:
`wire = length(message) + message + length(message2) + message2 + ...`

A good example of how the protocol functions can be found in [Data Replication](#data-replication).

###### Message Types
There are several message types that can be used to open a channel, request data and send data. Each of the below message types correspond with a specific [Message Handler](#message-handlers). The following message types are available:

###### `open` Type
Opens a channel and signals the other end that you are sharing a dDatabase feed. Accepts a `key` parameter, which is the dDatabase public key. Before being hashed, the public key of the dDatabase is hashed and sent as the dWeb network address for any connecting peers. This protects the dDatabase's public key from being learned by remote peers who do not already posses it. Also includes a cryptographic proof that the local possesses the public key, which can be implicitly verified by using the [removeVerified API](#removeverified-api).

Also accepts an object of functions, as a parameter, for handling incoming messages, using [Message Handler](#message-handlers).

###### `close` Type
Signals to the other side that the sender does not have the key corresponding to the dWeb network address.

###### `destroy` Type
Destroys the stream and closes all dDatabase feeds as well.

###### `finalize` Type
Gracefully end the stream and close all dDatabase feeds.

###### `options` Type
Sends an `options` message on a channel.

###### `status` Type
Sends a `status` message on a channel. This message is intended to indicate state changes.

###### `have` Type
Sends a `have` message on a channel, to see if the local can fulfill a particular request. Should reference the available index range (starting index and feed length). This should be in response to a `want` message. Feed length can be defined as `Infinity` for live replication.

###### `unhave` Type
Sends an `unhave` message on a channel, indicating an index range that is no longer available.

###### `want` Type
Sends a `want` message on a channel, indicating a specific index range that is wanted.

###### `unwant` Type
Sends an `unwant` message on a channel, indicating a specific index range that is no longer wanted.

###### `request` Type
Sends a `request` message on a channel, in order to request a specific piece of data from the dDatabase feed.

###### `cancel` Type
Sends a `cancel` message on a channel, in order to cancel a `request`.

###### `data` Type
Sends a `data` message on a channel, in order to fulfill a `request`. Includes a digital signature, accompanied by a Merkle tree representation of the data.

For more information on each of these messages, see the [dDatabase Protocol ProtoBuf Schema](https://github.com/distributedweb/ddatabase-protocol/blob/patriot1/schema.proto).

###### Message Handler
Message Handlers are triggered after specific [Message Types](#message-types) are received. The following handlers can be used:

###### `onopen`
Triggered after receiving an `open` message.

###### `onoptions`
Triggered after receiving an `options` message.

###### `onhave`
Triggered after receiving a `have` message.

###### `onunhave`
Triggered after receiving an `unhave` message.

###### `onwant`
Triggered after receiving a `want` message.

###### `onunwant`
Triggered after receiving an `unwant` message.

###### `onrequest`
Triggered after receiving a `request` message.

###### `oncancel`
Triggered after receiving a `cancel` message.

###### `ondata`
Triggered after receiving a `data` message.

#### The dWeb Lifecycle
It is crucial to understand how DWDHT and dDatabase can be used in tandem and how they both, when layered on top of one another, create a truly decentralized data transfer network, which help form the dWeb. Below is an example of what I call the `dWeb Lifecycle` - how data is created, announced, discovered by a remote, downloaded and then re-announced by the remote.

-1. Bob creates a dDatabase, and appends "Hello" to the feed, at index 0.
-2. Bob opens a channel to share his dDatabase, and generates a dWeb network address for the dDatabase in the process.
-3. Bob announces the dWeb network address, along with his public IP address and the port number peers can connect with him on, via dWeb's DHT.
-4. Alice learns of Bob's network address and wants to access it, so Alice performs a lookup for Bob's network address on dWeb's DHT.
-5. The DHT returns a list of peers, which for now only includes Bob's public IP address and port number.
-6. Using the dDatabase protocol, Alice sends a `request` message for the entire index range to Bob, using the connection details found on the DHT. This also creates a binary duplex channel between Alice and Bob.
-7. Bob responds with a `data` message, with all the indices from his dDatabase.
-8. Alice chooses to keep the channel open, so that she can listen for updates to Bob's dDatabase, which she can now receive in real-time.
-9. Alice now announces Bob's dDatabase (its dWeb network address) on dWeb's DHT, adding her public IP and port to the swarm now surrounding Bob's dDatabase.
-10. Jim comes along and learns about Bob's dDatabase and performs a lookup of its network address on dWeb's DHT.
-11. dWeb's DHT returns a list of peers who are announcing (broadcasting) Bob's dDatabase (now Alice and Bob), from whom Jim can now download the dDatabase. All Jim has to do is form a `request` message, detailing the `index` range he'd like to download.
