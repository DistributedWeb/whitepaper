# dWeb Whitepaper
##### Authored By: Jared Rice Sr.
##### Date Published: August 26th, 2019
##### Date Revised: December 10th, 2020

## Preface
The World Wide Web, while it was once thought to be somewhat decentralized, has been transformed over the years into an overly centralized cesspool of authoritarians, and one that is controlled by governments from around the world and a handful of globally recognized tech companies. The current state of the web is not how the visionaries intended it to be, nor would any of them agree with how the web is being run today.

Beyond the evidence that was revealed by then Booz Allen Hamilton contractor, Edward Snowden, understanding how the CIA, NSA and the private sector companies, including Google and Facebook, are highly involved in manipulating mass thought is critical in understanding why the web has to be re-imagined [ACKE14]. The fact that China is in full control of these companies, as well as many of our own elected officials, has become very obvious to many Americans. For some, it has made clear why mass censorship and widespread election interference is taking place.

It is us, the denizens of the World Wide Web, that have become extremely dependent on how quickly and easily we can communicate with each other, and access information using the WWW's many facilities. But it is during the process of the various communications and periods of research that our information, our actions and reactions are collected and then stored within the databases of not only the providers of these facilities but the "virtual, centralized grand database" originally envisioned by President Reagan's former national security advisor, John Poindexter [ROSE02].

It was Poindexter who was behind the "Total Information Awareness" program that eventually became the NSA's Advanced Research and Development Activity (ARDA), which was later renamed "Terrorism Information Awareness" [IWAR] and then finally reincarnated as an alliance between the US government and Silicon Valley. Enter Booz Allen Hamilton and other private sector companies like Palantir - run by so-called "libertarians" like Peter Theil - that are paid by the CIA and the NSA to spy on and gather data about law-abiding American citizens, in the name of "terrorism" [GREE13].

History clearly shows us, even through the admissions of companies like Facebook and academics at Cornell University, that big tech is not only spying on us, but running psychological experiments on us as well [ROBI14, KRAM14, GOEL14, KRISH14]. All the while, they do what other CIA and NSA contractors do and attempt to hide their tyrannical behavior behind a masquerade of talking points about "civil liberties" and "improvements to service" [SCHE15], but it is clear to many that they're actively doing more than simply trying to improve their software.

Companies like Facebook, who have a clear and open relationship with the Chinese government, are far more interested in controlling what we see and quite frankly, what we believe, and it's these internal studies that allow them to accomplish exactly that. For years, these private sector companies have been developing internal algorithms that are used each and every day to study our habits, mannerisms and other psychological traits, all in the name of helping them to improve their advertising services. What truly opened the eyes of Silicon Valley elites like Mark Zuckerberg, however, had little to do with predicted what our buying behavior. Rather, these authoritarians were blown away when their own studies revealed that an individual's voting behavior could be influenced through "undetectable social network maneuvering" [LANI14].

The research I just mentioned happened in 2014, and don't worry, the source I cited was ***The New York Times*** - in case there are those out there that claim there is no "widespread evidence of election interference." That's why I felt it was important to cite a New York Times article discussing a study that Facebook conducted themselves! Facebook and Google's interest in our elections has grown extensively since 2014, when they began collectively contributing hundreds of millions of dollars to local governments and local companies in an attempt to support candidates that has unusual relationships with the Chinese government. For example, Joe Biden, whose son is currently under investigation by the FBI for his dealings with Chinese companies, as of this writing.

What they used to refer to as "undetectable maneuvering" is no longer truly undetectable, due to the fact that social networks are doing more than simply tweaking your news feed but are now overtly censoring the posts of individuals that rail against the propaganda machine that most Silicon Valley-based social networks have become. The censoring also takes place within search engines like Google, where results for conservative news stories and websites are rarely displayed to those individuals that Google knows are registered democrats (look no further than the blacklisted Hunter Biden story). The efforts of Google and other tech companies to interfere in the 2020 election were nothing short of extraordinary, and a slap in the face to anyone who is concerned with freedom in America.

All of this is made possible by a clear invasion of our private lives that we have allowed to go on for as long as some of us have been alive. The disregard for our own privacy has allowed companies in the private sector to gather data from every corner of our lives, which they have sold back to the US government in a clear violation of our Forth Amendment rights. They've used this data to build a clear profile of every one of us and know, for example, every single African American Democrat from Dallas, Texas who ate McDonalds last week while watching Netflix. Sadly, they'll make sure that all of them are fed a steady stream of anti-American leftists propaganda until each one of them align themselves with the "American woke."

It was this sort of invasion of privacy that the English administrators over the American colonies took part in, according to John Adams, that as much as any other factor was instrumental in sparking the American Revolution. In 1761, a young John Adams documented a speech given by James Otis in which Adams was quoted as saying, "every man of a crowded audience appeared to me to go away, as I did, ready to take up arms against writs of assistance." In this speech, Otis openly denounced "writs of assistance," in the process making a clear and open demand for privacy which would later become a foundational aspect in the establishment of our republic. I believe the invasion of privacy we face today far exceeds what any of our founders could have ever imagined. Alice Walker once said, "the most common way people give up their power is by thinking they don't have any." To that end, we have clearly given up our rights to tech companies in exchange for convenience and exposure. Who could have imagined that?

I have said for quite some time, that we're way overdue for another revolution. But this time, a majority of that revolution will be digital. As Chief Justice Roberts (a trader on most issues) pointed out in Riley vs. California, the Fourth Amendment doesn't simply cover our physical homes, but our digital ones as well. Likewise, true American patriots don't simply face a physical revolt in order to secure their freedoms, but a digital one as well. And if these patriots choose to stand idly by as their freedoms are slowly stripped away, they won't have a place in the digital realm to communicate or have their voices heard. I say this because building alternatives to Facebook and Google on the centralized web is not a viable, long-term solution. The control these companies have over us and the web goes much further than what we see or what we read. They now control our companies, our domain names, what we publish, what we sell and even our ability to accept payments.

Andrew Torba, the founder of Gab (an alternative social network for conservatives), has already been banned from most online payment platforms, his domain has been seized, his servers taken offline and his family members have been blacklisted by VISA. While that sounds like something out of 1984, it's an unimaginable reality that is actually taking place. Organizations like ARIN, who controls the delegation of IP addresses to Internet Service Providers, and ICANN, who controls the domain registrar approval process, are quickly becoming activists for leftist causes. These organizations are enacting policies that will soon force domain registrars like GoDaddy, in addition to hosting provides like Amazon Web Services (AWS), to move away from WWW-based facilities that allow for the distribution of anti-leftist propaganda. Coincidentally, that is exactly what platforms like Gab are used for and sadly, the left is ultimately in control of the entirety of Gab's digital existence.

Beyond the web, and the silencing of conservatives by ICANN, ARIN and Biden's FCC, is the additional threat of Google and Apple doing the same to mobile applications like Alex Jones' InfoWars, which was abruptly removed from both apps stores after it quickly surpassed CNN as the most downloaded news application within hours of its launch. To put all this into perspective: companies in Silicon Valley are selling user data to communist governments, running psychological experiments on their users, and interfering in our elections, all while silencing those who expose their motives and tactics, in the process forming a powerful resistance to alternatives that emerge as a result of these organizations' complete and total disregard for American civil liberties.

This is why I have embarked on a mission over the past several years to re-imagine the web and build a medium where the people are in control, instead of governments and the private sector. A quick look at how long this GitHub profile has been around and the amount of work and research that has taken place, should make this last statement obvious. The point of this paper is to make the case for a decentralized alternative to the web, one that we've been developing for the past four years, called the "dWeb." It will also lay out how the dWeb works, and describe the scientific and mathematical foundations that ensure the dWeb can never be controlled by a single entity, and why the data itself can only be modified by its creator. At the very same time, this paper will detail how an elected governance, a distributed election system and a distributed reporting system are used to ensure that the people of the dWeb, as well as its elected governance, can remove illegal activity and content without introducing any sort of centralization that might otherwise be used to undermine the dWeb's future.

While the dWeb provides protections from the tyrannical and authoritarian actions of governments and the private sector, it also presents technological advances that allow for the global distribution of websites, web applications and various forms of data, without the need for costly infrastructure or the threat from hackers. These advances and others are just as important, and certainly speak to the need for a decentralized web and the freedoms it allows for.

The need for a decentralized web at a time when the web has become overly-centralized, may not be obvious to the many who regularly use the web, or those who have not witnessed the constant violation of the human, civil, and constitutional rights of the web's users. But there is no other alternative to the left's tyranny. The influx of countries and their governments who use the web, as well as the Internet at large, to spy on their citizens, has spiraled out of control. The lack of privacy and security issues are truly alarming, where even central banks lack the capability to protect worthless fiat - after attempting for more than two decades, they are still still unable to secure their networks. Now, social networks and search engines want to get involved in politics and control what we find on their platforms, and will delete our entire digital existence over a minor policy violation. Big tech and tyrannical governments like our own, know that software developers are building ways to avoid their attacks and are using policies, as well as regulators, to scare off innovation in an attempt to maintain power and keep their usual control mechanisms in place.

Developers, innovators and disseminators of information needs a better protocol. They need a completely decentralized web where their hard work can never be destroyed or shutdown by fearful governments. They simply need a better web where they can experience true privacy, freedom, security and transparency, for the first time since the inception of the web itself. Isn't that what the web was created for in the first place? It's what the dWeb was created for, but this time, we didn't forget to hardcode those rules into its immutable and irreversible foundation.

Welcome to the dWeb.

"There is no central control."
***- Paul Baran - Godfather of packet switching***

"My bias was always to build decentralization into the net. That way, it would be hard for one group to gain control. I didn't trust large central organizations. It was just in my nature to distrust them."
***- Bob Taylor - ARPANET Engineer***

**NOTE:**
As an added plus, the dWeb is live as you read this, and can never be taken offline as long as people like you are using it. You can start browsing the dWeb by downloading dBrowser [here](http://dbrowser.com) and can begin following our efforts to launch truly decentralized facilities like [dSocial](https://github.com/peepsx/dsocial-whitepaper) and [dSearch](https://peepsx.com/dsearch).


## Abstract
The dWeb is a decentralized web that is formed through the combination of on-chain and off-chain networking, data communication and data integrity protocols, that together form the DWEB protocol suite and enable the decentralized exchange and validation of data amongst peers. While the dWeb can certainly be seen as a peer-to-peer web, and it certainly fits that description, its protocols at their most basic level combine the peer-to-peer exchange of pure binary data with decentralized user authentication. This combination opens the doors to much more than just a place to share websites and web applications.

The dWeb represents a true decentralized alternative to the World Wide Web in every facet imaginable, from decentralized web hosting to decentralized governance, all of which ensure that the dWeb remains decentralized from end-to-end. This paper was created to describe the foundational protocols that form the DWEB protocol suite, and how the combination of these on-chain and off-chain protocols, while unorthodox, provide the perfect ecosystem for developing truly decentralized technologies.

## Foundational Protocols
The dWeb's on-chain and off-chain protocols bring the following decentralized services to life:

-Peer discovery
-Data discovery
-Peer-to-peer data exchange
-Distributed databases
-Distributed and versioned file systems
-Network addressing
-User authentication
-Distributed global computer and state machine
-Smart contract engine
-Governance and consensus

This is all made possible by DWEB's `off-chain` protocols:

-`DWDHT` - Powers the dWeb's distributed hash table.
-`DDATABASE` - Protocol for exchanging abstract data blobs known as dDatabases.
-`DWEBTRIE` - A distributed key-value store built on top of dDatabase.
-`DDRIVE` - A distributed file system built on top of dWebTrie.

All of which are combined with DWEB's suite of `on-chain` protocols known as ARISEN (Asynchronous Real-Time Intelligent State Engine).

The sections that follow explain all of these protocols in-depth.

### DWDHT
DWDHT, an acronym for "dWeb DHT," is a protocol that forms a distributed hash table of dataset identifiers (dWeb network addresses) and the peers that are announcing them. Each database identifier and its announcing peers form what is referred to as a "swarm," which can be queried for connected peers or joined (announced) by a specified amount of peers. dWeb's DHT is literally how a particular dataset is discovered and downloaded from the peers that are openly seeding it (announcing it). Each peer who is announcing a dataset on a long-term basis, by design, becomes a DHT node and is responsible for storing a specific portion of dWeb's DHT. Put another way, each DHT node stores a specific portion of the dWeb's dataset identifiers, along with the peers who currently possess the data related to each of those data identifiers.

While each DHT node stores various swarms, it also stores information regarding other DHT nodes, identified by `node identifiers`, which can be mathematically determined to possess information regarding a specific swarm. dWeb's DHT is based on the `Kademilia Distributed Hash Table` popularized by BitTorrent and others. Nodes and data in dWeb's DHT are assigned 160-bit integers as IDs, while swarms are stored in the form of key-value pairs, where the key is a 32 byte value generated by a one-way hash function, such as SHA-1 (normally a dWeb network address), where the value is an object containing public and LAN-based peers that are announcing the key (the dataset identifier).

#### Node & Data Distribution
The DHT defines the distance between 2 DHT nodes `i` and `j` by the bitwise exclusive OR operations `(XOR)`, i.e., `d(i,j) = i (XOR) j`. This distance means for any given key `i` and a distance `L > 0`, there can only be a single key `j` that satisfies `d(i,j) = L` [ZHAN13].

All key-value pairs are stored on `k` nodes whose UIDs are closest to the actual key. `k` is an important parameter that helps determine data redundancy and how stable the DHT is at any time. Each node `i` always maintains multiple `k`-buckets. Each `k`-bucket stores a list of other DHT nodes, which are organized in an order that reflects the most recently active nodes. The node that is most recently active is stored at the tail, while the least active node is stored at the head.

The node whose distance from node `i` is in the range of [pow(2,m), pow(2,m+1)] is stored in the `m`th `k`-bucket (node that 0 < `m` < 160). The nodes in the `k`-buckets are regarded as the neighbors of the node `i`. A dWeb DHT node dynamically updates its neighbors upon receiving any messages from them. This process can be better explained more specifically, when node `i` receives a message from another DHT node `j`, which is located in the `m`th `k`-bucket, where the `k`-bucket of node `i`, will be updated in the following way:

-If `j` already exists in the `k`-bucket, `i` moves `j` to the tail of the list, as node `j` is the most recently seen.
-If `j` is not in the `k`-bucket and the bucket has fewer than `k` nodes, node `i` inserts `j` at the tail of the list.
-If the bucket is full, `i` PINGs the node at the head of that particular `k`-bucket.
-If the head node responds, node `i` makes it to the tail and ignores node `j`.
-Otherwise, `i` removes the head node and inserts `j` at the tail.

##### DHT Primitives
-`PING` - Probes a DHT node to check whether it's online or not.
-`STORE` - Used to store a key-value pair.
-`FIND_NODE` - Finds a set of nodes that are closest to a given node.
-`FIND_VALUE` - Operates like `FIND_NODE` but returns a stored value.

These RPC-like primitives work in a recursive way, which improves the efficiency of dWeb's DHT. A `lookup` procedure is initiated by the `FIND_NODE` and `FIND_VALUE` primitives, where the lookup initiator chooses `B` nodes from its closest `k`-buckets and send many parallel `FIND_NODE` requests to these `B` nodes. If the node being searched for in a given iteration is not found, the lookup initiator resends the `FIND_NODE` to the nodes that were found during the previous recursive operation and repeats this iterative functionality.

A KV pair may be stored on multiple DHT nodes. Thanks to the recursive procedure explained above, the key-value pair spreads across the DHT network every hour. This process insures that multiple replicas of data exist across the network. Every key-value is deleted 24 hours after it is initially pushed into the network.

##### Joining and Leaving
When node `i` joins dWeb's DHT, it is assumed that it is aware of or knows about node `j`. The joining process consists of multiple steps as follows:
-1. Node `i` inserts `j` into its `k`-buckets
-2. Node `i` starts a node lookup procedure for its own ID, where `i` is made aware of other newer nodes.
-3. Node `i` updates the `k`-buckets

During this process, node `i` strengthens its `k`-buckets and inserts itself into other nodes' `k`-buckets. When other nodes leave or fail, they DO NOT notify any other node. There is no need for a special procedure to cope with node departures, as these mechanisms insure that leaving nodes will be removed from the `k`-buckets.

#### Swarm Announcement
Announcing a swarm means that you're either: announcing a dWeb network address that doesn't exist on dWeb's DHT and therefore become the first peer related to the address; or announcing a dWeb network address that does exist on dWeb's DHT and are added to a list of peers who are announcing the address.

A swarm entry in dWeb's DHT, takes on the following format:

```
Key: 8f0ab2... (32 byte hexadecimal address)
= = = = = = = = = = = = = = = = = = = = = = = = =
Value:
{
  node: { host, port },
  peers: [{ host, port }, { host, port }]
  localPeers: [{ host, port }, { host, port }]
}
```

If a peer is announcing a dWeb network address that matches a key in the DHT, the entry is mutated to include the peer's announced IP address and port. When announcing a dWeb network address, a peer must set their public port and public IP address during the announcement and can choose to set a LAN-based address and port as well, so that those on the same public IP who are performing a DHT lookup, can retrieve local announcers for a particular dWeb address. It's important to note that when announcing a dWeb network address on dWeb's DHT, the announcer becomes a DHT node on the network by design.

#### Swarm Lookups
A swarm can be looked up by its dWeb network address, which is a 32 byte buffer (normally a hash of something). When performing a lookup, the querying peer is temporarily an announcing peer, but is removed as an announcing peer (unannounces), once the lookup is finalized.

A lookup for a particular dWeb key, returns the following data:
```
{
  // The dWeb DHT node that is returning this data
  node: { host, port }
  // List of Peers
  peers: [{ host, port }, ...]
  // List of LAN Peers
  localPeers: [{ host, port }, ...]
}
```

#### DHT Bootstrap Nodes
dWeb's network utilized various `bootstrap` nodes to launch the initial dWeb network. These 3 bootstrap nodes are as follows:

-`dht1.dwebx.net`
-`dht2.dwebx.net`
-`dht3.dwebx.net`

These nodes going down would not affect the dWeb or a user's ability to announce or lookup dWeb network addresses, due to the way other DHT nodes on the network share data and information regarding each other. dWeb developers have the option of launching their own bootstrap nodes so that their apps have a point of entry into the dWeb's network of DHT nodes. Those nodes would initially use a set of already existing nodes to gain access to the DHT and would no longer need access to the nodes used for bootstrapping. Any node on the DHT can be used to bootstrap a new node.

#### DWDHT Reference Implementation
The DWDHT reference implementation was written in JavaScript and was used to launch the initial dWeb DHT. You can find it [here](https://github.com/distributedweb/dht).

You can use this reference implementation to allow dWeb-based applications (desktop, mobile or web) to act as DHT nodes.

You can also launch your own dWeb DHT node via the command-line, by using the DHT CLI [here](https://github.com/distributedweb/cli).


#### dWeb Swarm API
The dWeb swarm API, also known as "Swarm Programming" is a high level API built on top of the [DWDHT](#dwdht) Protocol used for finding and connecting to peers of a particular swarm and interacting with the dWeb DHT.

It expands the default set of parameters when creating a DHT node, adds specific swarm events where callbacks can be used to react to specific swarm-based events and introduces several functions, like `join`, `leave`, and `connect`, which make it easy to programmatically interact with the dWeb's DHT.

##### DHT Node Initiation Parameters
Using the dWeb Swarm API, a DHT node can be created with the `dwebswarm()` function, using the following parameters:

-`bootstrap` - An array of bootstrap servers used to initiate the dWeb DHT node.
-`ephemeral` - A boolean that is set to `false` if this is intended to be a long running DHT node.
-`maxPeers` - The total amount of peers that the node initiator will connect to.
-`maxServerSockets` - The number to restrict the number of server socket based peer connections. Set to `Infinity` by default.
-`maxClientSockets` - The number to restrict the number of client socket based peer connections. Set to `Infinity` by default.
-`validatePeer` - A function for applying filters before connecting to a peer.
-`queue` - An object for configuring peer management behavior.
--`requeue` - An array of backoff times in milliseconds every time a failing peer connection is retried.
--`forget` - An object for when to forget certain peer characteristics and treat them as fresh connections again.
---`unresponsive` - How long to wait before forgetting that a peer has become unresponsive.
---`banned` - How long to wait before forgetting that a peer has been banned.
---`multiplex` - Set to `true` in order to reuse existing connections between peers across multiple dWeb network addresses.

##### Swarm Event Emitters
Using the dWeb Swarm API, applications can listen for the following events:

-`connection` - A new connection has been created. Event should be handled by using the socket. This emits an `info` object that describes the connection using the following details:
```
-type (string) - Should be either "tcp" or "udp."
-client (boolean) - If true, the connection was initiated by this node.
-topics (array) - The list of dWeb network addresses associated with this connection, if "multiplex" was set to true, during DHT node initiation.
-peer (object) - Object describing peer (port, host, LAN peer details, referrer and the dWeb network address the peer was discovered under).
```
Using the `info` object, the `info.ban()` method can be called to ban the connected peer and will keep the DHT node from connecting to the peer in the future. The `info.backoff()` method can be called to get the DHT node to backoff from connecting to the peer.

-`disconnection` - A connection has been dropped. Emits an `info` object that is identical to the `connection` `info` object, describing the dropped connection.

-`peer` - A new peer has been discovered on the network and has been queued for connection. Emits a `peer` object, including the port, host referrer, and dWeb network address peer was discovered under. Also includes a boolean on whether the peer is a LAN-based peer.

-`peer-rejected` - A peer has been rejected as a connection candidate. Emits a `peer` object that is identical to the `peer` event's `peer` object, describing the rejected peer.

-`updated` - Emitted once a discovery cycle for a particular dWeb network address has completed. Emits a `key` object that identifies the dWeb network address the discovery cycle is related to.

For more information on dWeb Swarm's events, take a look at the README.md file within the reference implementation of the dWeb Swarm API [here](https://github.com/distributedweb/dwebswarm).

##### Swarm API Functions
There are several functions (methods in the reference JavaScript implementation) that can be used to interact with a dWeb DHT node. These functions and their required parameters are described below.

###### `join` - Join the swarm for a given `topic` (dWeb network address). This will cause peer to be discovered for the topic (`peer` event).

###### `join()` Parameters
-`topic` (Buffer) - The dWeb network address to list under. Must be 32 bytes in length.
-`options` (Object)
--`announce` (Boolean) - List this peer under the topic as a connectable target. Defaults to `false`.
--`lookup` (Boolean) - Look for peers in the topic and attempt to connect to them. If `announce` is false, this automatically becomes true.
-`onion` - A function that is called when your topic has been fully announced to the local network and the DHT.

###### `connect()` - Establish a connection to a given peer. You usually won't need to use this function, because the DHT node connects to discovered peers automatically. Accepts a `peer` object (identical to the `peer` object emitted by the `peer` event) for the peer the function is to establish a connection with. Also has a callback function that emits a connection error (err), and the established TCP or UDP socket (socket) and details regarding the established connection (details).

###### `leave` - Leave the swarm for a given dWeb network address.

###### `leave()` Parameters
-`topic` (Buffer) - The identifier of the peer-group to delist from.
-`onleave` - A function that is called when your topic has been fully unannounced to the local network and the DHT.

For more in-depth examples on how to call these functions within dWeb-based applications, view the `README.md` file within the reference JavaScript implementation [here](https://github.com/distributedweb/dwebswarm).

### dDatabase
A dDatabase is a distributed append-only log, also referred to as a distributed and immutable data feed, which can be exchanged between peers, using the [dDatabase Protocol](#ddatabaseprotocol) and validated via functions available in a dDatabase implementation.

At a very basic level, the dWeb is made up of dDatabase feeds, but it's important to note that a dWeb network address doesn't have to represent just a dataset, it can represent an entity like a peer or a device. Although, as the dWeb stands today, most of what you see distributed across it are dDatabases.

Below is a pseudo-representation of what a dDatabase would look like:
```
Index                 Entry
= = = = = = = = = = = = = = = = = = =
0                       Hello
1                       World
```

As you will see later in this paper with the [dDrive](#ddrive) Protocol, a distributed file system can be created using a dDatabase, by converting the content and metadata of a file system's files into a particular encoding format, such as binary, and storing in a dDatabase entry like so:
```
Index                 Entry
0                       {content:010110011110...}, {metadata:1110100101...}
1                       {content:11101001111001...}, metadata:0100110000...}
```

In other words, a dDatabase can easily be used to distribute an entire file system between peers, or any dataset for that matter. This section is intended to breakdown how dDatabases work, how data is determined to have the utmost integrity, and how data is exchanged between peers using the dDatabase Protocol.

A dDatabase, represented by a generated dWeb network address, can easily be announced and discovered via dWeb's DHT, where other peers can download the dDatabase from peers in the dDatabase's swarm who have download the data and subsequently announced their joining of the swarm itself. Put another way, a dDrive that contains a website can announce its dWeb network address on the dWeb DHT and peers can join the dDrive's swarm and thereby increase the number of peers the website can be downloaded (replicated) from. This is just an example, as a dDatabase can contain an endless amount of dataset types, such as an encrypted conversation or an encrypted voice call.

#### Distributed, Immutable Data Feed
dDatabase is a secure, distributed and append-only (immutable) feed that is designed for distributing large datasets and streams of real time data between peers of the dWeb.

#### Value Encoding
The value of an append entry to a dDatabase can be encoded into the following formats:

-`binary` (Default)
-`json`
-`utf-8`

#### Data Integrity
It is important that those who download a dDatabase can verify that it actually derived from its proclaimed author. This is made possible thanks to dDatabase's use of public key cryptography, which a dDatabase's dWeb network address is derived from, alongside Merkle trees and FIO trees, in order to verify a dDatabase's root hash signature(s). This insures that the data itself derived from the author of the dDatabase and the original announcer of the dWeb network address.

##### Merkle Trees and FIO Trees
A dDatabase uses a custom encoding method when laying out dDatabase-based update logs into a Merkle tree. This particular encoding method positions hashes into a scheme known as "binary in-order interval numbering," or just "bin" numbering for short. This is a deterministic method of positioning leaf nodes in a Merkle Tree.

A Merkle Tree with seven leaf nodes will always be laid out like this:
```
0L
      1L
2L
            3L
4L
      5L
6L
```

From a computer science perspective, dDatabases are binary append-only streams, the contents of which are cryptographically hashed and signed. Therefore, any dDatabase can be verified by anyone with access to the public key of the creator. Over the HTTP protocol, datasets are shared every day, although there is no built-in support for version control or the content-addressing of specific data. dDatabase is the solution to this, allowing multiple un-trusted devices to act as a single virtual host. For the dWeb to work, it requires a data structure that authenticates the content's integrity and one that keeps a historical log of the revisions - and dDatabase feeds certainly provide that.

dDatabases are identified internally by signed Merkle trees, are identified publicly over the dWeb by a public key, and are discovered over dWeb's DHT by a dWeb network address, which in turn derives from a dDatabase's public key. A dDatabase's public key is used to verify the signature that relates to the received data. A dDatabase's internal Merkle tree is output as a "Flat In-Order Tree" or "FIO Tree."

FIO trees, per PP5P RFC 7574, are defined as "bin numbers." These FIO trees allow for numerical-based identification of each leaf node within a binary-based Merkle tree and therefore create a simplistic way of representing a binary tree as a list. These properties of FIO trees are used in the simplification of "wire protocols" that of which utilize Merkle tree structures within distributed applications.

Below is an example FIO tree, that is sized at 4 blocks of data:
```
0L
1P
2L
3P
4L
5P
6L
```

In the above FIO tree example, even numbers (0, 2, 4, 6) represent leaf nodes on the tree and odd numbers represent parent-nodes that each contain two children.

Using binary notation, we can count the total number of "trailing 1s" to calculate the depth of the tree's nodes. For example, the following numbers below are converted to binary:
```
5 = 101
3 = 011
4 = 100
```

The number 5 has one trailing 1, the number 3 has two trailing 1s and the number 4 has zero trailing 1s. In the FIO example, 1 is the parent node of (0, 2), 5 is the parent node of (4, 6) and 3 is the parent node of (1, 5). The FIO tree would only have a single root if the leaf node count is a power of 2, otherwise a FIO tree will always have more than one root.

Below is another FIO tree (pseudo-representation) with a total of 6 leaf nodes:
```
0
1
2
3
4
5
6
7
8
9
10
```

In the above example, the roots are 3 and 9.

A Merkle tree, named after cryptographer and mathematician Ralph Merkle, is best described as a tree of binary-based data, where every "leaf" (an even-numbered tree node that has no children) is a hash of a data block and every "parent" (an odd-numbered tree node that has two children) is the hash of both of its children. dDatabases are feeds represented by Merkle trees that are ultimately encoded with "bin numbers."

For example, a dDatabase of 4 values would always map to the 0, 2, 4 and 6 leaf nodes. Below is a pseudo-representation:
```
fragment0 -> 0
fragment1 -> 2
fragment2 -> 4
fragment3 -> 6
```

When converting to FIO tree-style notation, a Merkle tree spanning these data blocks looks like:
```
0 = hash(0 + 2)
2 = hash(fragment1)
3 = hash(1 + 5)
4 = hash(fragment2)
5 = hash(4 + 6)
6 = hash(fragment3)
```

The even and odd nodes store different types of information:
-`Even Numbers` - List of data hashes [f0, f1, f2, ...]
-`Odd Numbers` - List of Merkle hashes (hashes of child nodes) [h0, h1, h2, ...]

The root node within a Merkle tree hashes the entire dataset. In the example of 4 fragments, node #3 hashes the entire dataset and node #3 is used to verify the rest of the dataset. Although the root node will change every time data is added to a dDatabase.

```
0
1
2
3 (root node)
4
5
6
7
8
9 (root node)
10
```

The Merkle tree's nodes are calculated as follows:
```
0 = hash(fragment0)
1 = hash(0 + 2)
2 = hash(fragment1)
3 = hash(1 + 5)
4 = hash(fragment2)
5 = hash(4 + 6)
6 = hash(fragment6)
7 = hash (6 + 8)
8 = hash(fragment4)
9 = hash(8 + 10)
10 = hash(fragment5)
```

When there are multiple root hashes, it is convenient to capture the entire state of a dDatabase as a `fixed-size hash` by hashing all of the root hashes into one single hash, where in the example above:

`root = hash(9 + 3)`
or
`tr = h(r1 + r2)`
(tr = top root; hash = hash function; r1 = root hash 1; r2 = root hash 2)

##### Root Hash Signatures
Merkle trees are used by dDatabases to create a way of indentifying the content of a dataset through hashes. The concept is simple, if the underlying content of a dDatabases changes, the hash changes. For example, a dDatabase acts as a list that calls the `append()` mutation when an entry is added to the database feed, thereby adding a new leaf to the tree, which ultimately generates a new root hash. When a dDatabase is created, a public/private keypair is generated. The public key is used as a public identifier for data validation, whereas the private key is used to sign the root hash every time a new one is generated. This digital signature is always distributed with the root hash to verify its integrity.

##### Data Validation
Data which is received that belongs to a dDatabase goes through the following process:
-1. The root hash's signature is verified.
-2. The received data is hashed with `ancestor hashes` in order to reproduce the `root hash`.
-3. If the root hash that was calculated is an exact match of the original root hash that was received, the data has been verified.

In an example of a dDatabase containing 4 values, our tree of hashes would look like this:
```
0
1
2
3 (root hash)
4
5
6
```

If we want to verify the data for 0 (fragment0), we first read (2), which is the sibling hash, then (5), which is the uncle hash and then (3), which is the signed root hash.

```
0 = hash(fragment0)
2 = (hash received)
1 = hash(0 + 2)
5 = (hash received)
3 = hash(1 + 5)
```

If what we calculate for 3 is equal to the signed root hash we received for 3, then fragment0 is valid.

It's important to note that all new signatures very the entire dDatabase since the signature spans all data in the Merkle tree. A dDatabase is considered corrupt if a signed mutation results in a conflict against previously verified trees. When a dDatabase is considered corrupt, the dDatabase protocol is designed to stop distribution.

##### Cryptography Specification
-The hash function uses BLAKE 2B-256 encryption, while signatures are ed25519 with the SHA-512 hash function.
-Hash function inputs are prefixed with different "constants" based on the type of data being hashed. The constants include:
--0x00 -Leaf
--0x01 -Parent
--0x02 -Root

This protects against a "second preimage attack."
-Hashes, a lot of the time, will include the sizes and indexes of their content so that the structure of a tree can be described along with the tree's data.

#### dDatabase Data Structure
Data within a dDatabase is stored as `blocks`, which are identified by an `index`. Each `block` is signed by its creator, so that an entire dDatabase feed can be audited for whether or not all currently stored data matches the hashes in the Merkle tree. Put another away, all data blocks must match the hashes contained within the dDatabase Merkle tree, explained previously.

This data structure ensures that peers can download a specific block range, rather than the entire block base of the dDatabase itself. The `index` is an auto-incrementing number that is zero-based.

A pseudo-representation of a dDatabase feed uses binary encoding:
```
Index          Entry
==================================
0                0111010010011...
1                1011000110000...
```

As data is appended to the feed, a new index is created. Those who are replicating the feed would receive the new index in real-time.

#### dDatabase Protocol
Before diving into the dDatabase Protocol, it is important to reiterate that a dDatabase is an append-only data feed, where the data within a data feed is an abstract "blob" of data.

**dDatabase feeds can:**
- Be distributed partially between peers.
- Be distributed fully between peers.
- Be distributed to/received by multiple peers at once.

The dDatabase Protocol is a process by which two or more peers exchange a dDatabase over binary duplex streams. Remote peers are able to identify with seeders of a dDatabase whether they are looking for specific portions of a dDatabase (partial) or the entire dDatabase.

Using various message types, peers are able to ask a specific peer, for a specific portion of the feed, whereas the feed distributor(s) can signal whether they have that data and can subsequently fulfill the data request. The protocol uses a sender/handler lifecycle for message exchange and message handling.

Binary duplex streams can be encrypted, use a public/private keypair for stream authentication, and utilizes a NOISE-based stream handshake by default.

This section will cover all of these aspects, including dDatabase's handshake phase and message exchange phase. 

##### Handshake Phase
A dDatabase protocol duplex stream can use an optional NOISE-based handshake. For the handshake, NOISE uses the `XX` pattern. Each NOISE message is sent with varint framing. After the handshake is finalized between peers, a message exchange phase can begin.

The handshake phase allows two or more peers to authenticate each other and to securely negotiate an encryption and MAC algorithm, along with cryptographic keys to be used to protect the data sent between them. After the initial handshake transport, encryption is enabled to ensure a stream is private.

As an added plus, each NOISE session is unique and is identified by a unique `handshake hash` in order to enable channel binding. 

##### Message Phase
The NOISE-based message exchange phase uses a basic varint length prefixed format to send messages over the wire.

All messages contain a header indicating the type and the `feedID` of the dDatabase feed, and a protobuf-encoded payload:

`message = header + payload`

A header is a varint that looks like this:
`header = feedID << 4 | numeric-type`

The `feedID` is just an incrementing number for every feed shared and the `numeric-type` corresponds to which protobuf schema should be used to decode the payload.

The message is then wrapped in another varint containing the length of the message: 
`wire = length(message) + message + length(message2) + message2 + ...`

A good example of how the protocol functions  can be found in [Data Replication](#data-replication) and the messaging phase is expanded upon in [DWEB](#dweb).

###### Message Types
There are several message types that can be sent during the message phase, which can be used for introduction, handshakes, data Q&As, data requests and data fulfillment. Each one of those message types are explained in-depth below:

- `feed` (0) - The `feed` type acts as an introduction, where one peer lets another peer know that they'd like to speak about a particular dDatabase feed (a particular dWeb network address).
- `handshake` (1) - Allows two peers to negotiate connection parameters 
- `info` (2) - Indicate to another peer whether you're starting or stopping, uploading or downloading.
- `have` (3) - Indicate to a peer, in response to their `want` message, that you have the data they're requesting.
- `unhave` (4) - Indicate to another peer that you no longer have the data that you said you had in a previous `have` message.
- `want` (5) - Indicate to a peer, that you want a specific set of data (specific dDatabases indexes).
- `unwant` (6) - Inform a peer that you no longer want the data you requested in a previous `want` message.
- `request` (7) - Ask a peer for the data they said they had in a `have` message they previously sent. Requests can only be sent for one dDatabase index at a time. 
- `data` (8) - Send the data that a peer is requesting via a `request` message.

###### Protocol Buffers
Messages contain specific fields, depending on the message type, all of which are serialized using Protocol Buffers. To learn more about dDatabase Protocol's Protocol Buffers schema, you can read more about the specific fields, message structures, and serialization schemas for each message type [DWEB](#dweb) or you can simply review the actual `schema.proto` configuration [here](https://github.com/distributedweb/ddatabase-protocol/blob/partriot1/schema.proto).

#### Exchanging Data 
The easiest way to describe how dDatabase replication works between two peers, is by using a pseudo-code example. Consider the following dDatabase with 6 entries:

```
Index          Entry
====================
0                 We're
1                 Making
2                 The
3                 Web
4                 Great
5                 Again
```

The dDatabase's Merkle tree would look something like:
```
0L                  - hash(We're)
       1L           - hash(0L+2L)
2L                  - hash(Making)
            3L      - hash(1L+5L)
4L                  - hash(The)
       5L           - hash(4L+6L)
6L                  - hash(Web)
8L                  - hash(Great)
       9L           - hash(8L+10L)
10L                - hash(Again)
```

If you recall in previous sections, the root hashes of the Merkle tree, which are `3L` and `9L` are used to create a `combined root hash`. The has must also be signed by the dDatabase creator's private key. The Merkle tree above is a tree of hashes that derive from the dDatabase entries (0-5). This is important in the exchange process described subsequently, since the Merkle tree allows the receiving peer to verify the integrity of the data they're receiving.

Before a dDatabase can be exchange publicly with other peers, it must first be announced on dWeb's DHT. This means a dDatabase's public key must be hashed into a dWeb network address beforehand. In the example below, Bob is able to publish the dDatabase described in this section, so that Alice can download it:

- 1. Bob announces the dDatabase's dWeb network address on dWeb's DHT.
- 2. Bob broadcasts the dDatabase over his local connection and port 5000.
- 3. Alice finds out about Bob's dDatabase, as well as its address and performs a lookup on dWeb's DHT for the address.
- 4. The lookup returns Bob's IP address and port 5000.
- 5. Alice opens up a TCP connection with Bob and sends a `feed` message.
- 6. Alice then sends a `handshake` message, to which Bob sends a `handshake` message back to confirm the connection parameters (encryption, etc).
- 7. Alice sends a `want` message indicating that she wants the entire feed.
- 8. Bob sends a `have` message back, indicating that he has 0L through 10L.
- 9. Alice sends a `request` message for each leaf Bob has.
- 10. Bob sends back 10 `data` messages, one for each leaf.
- 11. Alice can validate the integrity of the 1st (0L) and 3rd (2L) `data` messages received by hashing both and comparing it to the hash in the 2nd message received (1L). The hash for `4L` and `6L` can be taken and compared to `5L`. Then the hash for `8L` and `10L` can be taken and compared to `9L`. Afterwards, Alice can take the hash of `1L`, `5L` and `9L` and compare to `3L` (the root node). Lastly, the digital signature sent by Bob can be used to verify that it derived from the dDatabase's public key.

- Once Alice has the data from Bob, she can keep the channel open and continually request Bob's entire feed, so that she will know the moment another entry has been made to Bob's dDatabase and can immediately request it. At the very same time, Alice can announce Bob's dDatabase to dWeb's DHT and add herself to the swarm. Now other peers who discover Bob's dDatabase can download from Alice, Bob or both simultaneously.

### dWebTrie
dWebTrie is an abstraction layer that utilizes [Hash Array Mapped Tries](https://en.wikipedia.org/wiki/hash_array_mapped_trie) in order to provide a general purpose, distributed key/value store over the [dDatabase Protocol](#ddatabase). dWeb is a single writer key/value store which is able to map key/value data to a matching dDatabase index using a builtin rolling hash array mapped trie.

A dWebTrie utilizes the underlying dDatabase's public/private key pair, as well as its dWeb network address, and is represented by this single dDatabase feed. A [dAppDB](https://github.com/distributedweb/dappdb) is a multiwriter distributed key/value store that can be represented by multiple dDatabase feeds and therefore multiple key pairs, although its dWeb network address is derived from the dAppDB's initial dDatabase feed's public key.

dWebTrie was created as a way of storing an entire file system, with possibly thousands or even millions of files, within a dDatabase, where keys are path-like strings (e.g., /make/the/web/great/again) and values are arbitrary binary blobs. Without the dWebTrie abstraction, a distributed file system like [dDrive](#ddrive) simply wasn't possible, since there would have been too many complexities and bottlenecks if a protocol like dDrive would have used dDatabase directly. Optionally, we could have built a trie-structured key/value API within a dDrive implementation, but that would have been far too much overhead. It made more sense to develop a separate trie-structured key/value API, layered on dDatabase, so that many other protocols and applications could use dWebTrie as an off-chain distributed database management system.

#### Database Semantics
-Keys can be any UTF-8 string (e.g. "maga")
-If a key uses path segments (like a folder or file location) (e.g., /this/folder) path segments must be separated by the forward slash character ( /). Repeated slashes (//) are not allowed.
-Leading and trailing (/) are options (e.g., "/hello" and "hello" are equal).
-A key can be both a path segment and a key at the same time (e.g., /a/b/c and /a/b can both be keys at the same time).
-Values can be any binary blob, including an empty blob (zero bit length).
-Acceptable values can be UTF-8 encoded strings, JSON encoded objects, protobuf messages, or a raw uint64 integer (endianness does not matter).
-Length of value (in bits) is the only form of type or metadata stored about the value.
-Deserialization and validation are left to library and application developers.

#### dWebTrie Representation
Below is a pseudo-representation of a dWebTrie database:
```
Key                    Value
= = = = = = = = = = = = = = = = = = = =
Name                Jared Rice Sr.
Location            Dallas, TX
```

While this database is truly logical, since it is actually just a bunch of binary-based blob entries within a dDatabase feed, using dWebTrie's abstract layer, one could easily execute `get(name)`, which would return `Jared Rice Sr.` This is far simpler than working directly with the dDatabase feed, which would require far more steps and much more programming overhead.

#### Database API
A dWebTrie-based database is created by opening an existing dDatabase feed with dWebTrie content or by simply creating a new dDatabase feed.

The following API calls are part of the dWebTrie standard:

##### `put(key, value)`
Inserts a value of an arbitrary byte size under the specified key. Requires read-write access. Returns an error via callback if there is an issue.

##### `get(key)`
Retrieves the value for a given key.

##### `delete(key)`
Retrieves the value for a given key.

##### `list(prefix)`
Returns a flat (non-nested) list of all keys currently in the database under the given prefix.

##### `batch(batch)`
Insert/delete multiple values atomically.

##### `watch(prefix)`
Watch a prefix of the DB (or key) and get notifications when it changes.

#### Overhead and Scaling
Depending on the size of the database, the metadata overhead can vary.

Consider the case of a two-path-segment key, with an entirely saturated trie and a uint32-sized feed and entry index points:

-`trie:` 4 * 2 * 64 bytes = 512 bytes
-`total:` 512 bytes

In a light-case, with few trie entries and single-byte varint feed and entry index pointers:
-`trie:` 2 * 2 * 4 bytes = 16 bytes
=`total:` 16 bytes

For a database with most keys having `N` path segments, the cost of a `get()` scales with the number of entries `M`, as `O(log(M))` with the best case `1` lookup and the worst case `4 * 32 * N - 128 * N` lookups.

The cost of `put()` or `delete()` is proportional to the cost of `get()`. The cost of `list()` is linear `(O(M))` in the number of matching entries, plus the cost of a single `get()`.

The total metadata overhead for a dWebTrie-based database with `M` entries, scales with the `O(M log(M))`.

#### dWebTrie Schemas
A dWebTrie-based dDatabase feed consists of a sequence of protobuf-encoded message of `Entry` or `InflatedEntry` type. A "protocol header" entry should be the first entry in the feed, using "dWebTrie" as the `type`. dWebTrie itself does not specify the content of the optional header extension field, as higher level protocols are left to handle this portion of the protocol.

When data doesn't fit, due to the limited size constraint, for any given value, there is a second `content` feed associated with the dWebTrie key/value feed. The optional `contentFeed` field described in the schema below is used to identify a dWebTrie key/value feed, that needs a second `content` feed.

The sequence of entries includes an incremental index, where the most-recently appended entry in the feed contains metadata pointers that can be followed to efficiently locate any key in the database, without having to perform a linear scan across the database's entire history, or generate an index data structure that's independent of the database itself. Although it is completely up to the implementation on whether they choose to implement their own index or not.

The protobuf message schemas for `Entry` and `InflatedEntry` are:
```
message Entry {
  message InflatedEntry
  required string key = 1;
  optional bytes value = 2;
  optional bool deleted = 3;
  required bytes trie = 4;
  repeated uint64 clock = 5;
  optional uint64 inflate = 6;
  repeated Feed feeds = 7;
  optional bytes contentFeed  = 8;
}
```

The fields that are common to both message types are:
-`key` - UTF-8 key that this node describes. All slashes are removed before storing in message.
-`value` - Byte array of an arbitrary size.
-`deleted` - Boolean that converts entry into a "dead" entry, if `true`. It is recommended that if `false` to keep this value `undefined`, rather than using `false`. This can also be used to store user-defined metadata related to the deletion.
-`trie` - A structured array of pointers to other `Entry` entries in the dDatabase feed. This is used for navigating the tree of keys.
-`inflate` - A reference to the feed index number of the most recent `InflatedEntry` entry in the feed. **NOTE:** This should not be set on a feed's first `InflatedEntry` entry.
-`contentLog` - For applications that require a parallel `content` dDatabase feed. This field is used to store the 43-byte public key for that feed. It is sufficient to write a single `InflatedEntry` message in the dDatabase feed, with feeds containing a single entry (a pointer to the current feed itself) and `contentLog` optionally set to a pointer to a paired content feed. The `Entry` type can be used for all other messages, with `inflate` pointing back to the single `InflatedEntry` message.

#### Key Path Hashing
Every key path has a fixed-size hash representation that is used by the trie. When all path segments are concatenated, they are combined into what is known as a `path hash array`. Similar in many ways to that of a `hash map data structure`, a `path hash array` can have collisions where one key (string) and another key have the same hash, without suffering any issues because of it. This is because each hash is a reference (pointer) to a linked-list `container` of Entries, which can be linearly iterated over until the sought after value is found.

Each element (segment) in a path equates to 32 values, which also equates to a 64-bit hash. The key `/presidents/trump` has two path segments (presidents and trump) which equates to a 65  element path has array, made up of 32 element hashes and a terminator.

The `SipHash-2-4` hash algorithm is used, along with an 8-byte output and a 16-byte key. The corresponding input is the UTF-8 encoded path string segment, excluding slashes or other separators, as well as any terminators. A 16-byte secret key is required where all zeros is used, for this particular use-case. When an 8-byte outputted hash is converted to an array of 2-bit bytes, the ordering of the hash array is handled byte-by-byte, where for each byte, take the two lowest-value bits as `byte index 0` in the hash array and the next two bits as `byte index 1`, etc. When path hashes are combined into an array of greater length, the left-most path element hash will relation to byte indices `0` to `31`. The terminator, `4`, will have the highest index in the hash array (right-most).

**Note:** dWebTrie derived from a project known as HyperDB and the following examples have been lifted from [DEP-0004[(https://datprotocol.com/deps/0004-hyperdb), which was the original specification/proposal for HyperDB. A special thanks to the incredible work of Mathias Buus, Bryan Newbold and Stephen Whitmore.

-For example, consider the key `/tree/willow`. `tree` has the following hash:
`[0xAC, 0xDC, 0x05, 0x6C, 0x63, 0x9D, 0x87, 0xCA]`

-This converts into the following array:
`[0,3,2,2,0,3,1,3,1,1,0,0,0,3,2,1,3,0,2,1,1,3,1,2,3,1,0,2,2,2,0,3]`

-`willow` has a 64-bit hash:
`[0x72, 0x30, 0x34, 0x39, 0x35, 0xA8, 0x21, 0x44]`

-This converts into the following array:
`[2,0,3,1,0,0,3,0,0,1,3,0,1,0,3,0,1,1,3,0,0,2,2,2,1,0,2,0,0,1,0,1]`

-These two combine into the unified byte array with 65 elements:
`[0,3,2,2,0,3,1,3,1,1,0,0,0,3,2,1,3,0,2,1,1,3,1,2,3,1,0,2,2,2,2,0,3,2,0,3,1,0,0,3,0,0,1,3,0,1,2,3,0,1,1,3,0,0,2,2,2,1,0,2,0,0,1,0,1,4]`

In another example, the key `/a/b/c` converts into a 96-byte hash array, i.e., `32 + 32 + 32 + 1`. (1) in this case the terminator bit (4).

##### Incremental Index Trie
Each individual node stores a prefix trie that can be used to lookup other keys and can also be used to list all keys that are related to a given prefix. The prefix trie is stored in the `trie` field within an `Entry` message, as referenced in [dWebTrie Schema(#dwebtrie-schema).

Simply put, the `trie` is equal to the `path hash array`. As mentioned in the schema, each individual element within a `trie` is referred to as a `container`. Each container that isn't empty, is a pointer to the newest `Entries`, where the path is equal (identical) up to that specific prefix location. This is true, because each `trie` has 4 values at each node, which means there can be a pointer to up to 3 other values at a given element in the trie array. (Containers can be empty, if at that particular node, there are zero `branches`).

**NOTE:** Only non-null elements will be transmitted as stored on disk.

The trie data structure is a sparse array of pointers to other `Entry` entries. Each pointer references a specific feed, which indexes to the same value.

###### Looking Up A Key In A Database
The process for key lookups, is to:
-1. Calculate the `path hash array` for the key you are looking for.
-2. Select the most recent ("latest") `Entry` in the feed.
-3. Compare `path hash arrays` for exactly matching paths.
-4. If they match exactly, then compare keys. If keys match, the lookup was successful.
-5. Check whether the `deleted` flag of the `Entry` schema is set. If so, this entry actually represents the deletion of the `Entry`.
-6. If the path segments (concatenated) match, look for a pointer in the last `trie array index` and iterate from step #3 with the new `Entry`.
-7. If the path segments (concatenated) do not match, find the first index in each `path hash array` where both arrays differ and look up the corresponding element in this `Entry's` trie array.
-8. If the element is empty, or doesn't contain a pointer corresponding to your 2-bit value, then the key does not exist in the dWebTrie.
-9. If the trie element is not empty, then follow that pointer to select the next `Entry`. Recursively repeat this process from step #3, which will allow you to descend the trie in a search where the search will either terminate in your search or find that the key is not defined in the dWebTrie.

###### Writing A Key To A Database
In order to write a key to a dWebTrie database, follow the specified process below:
-1. Calculate the `path hash array` for the key to be stored and start with an empty trie array of the same length; where writing to this trie array will start at the current index of `0`.
-2. Select the most-recent ("latest") `Entry` in the feed.
-3. Compare `path hash arrays` for exactly matching paths.
-4. If they match exactly, then compare keys. If keys match, then you are overwriting the current `Entry` and can copy the `remainder` of its trie up to the current trie index. **NOTE:** When I say "overwriting", the previous `Entry` is not removed, for the simple reason that the dDatabase feed below the dWebTrie is immutable and append-only. A more-recent `Entry` is created with its own schema, which could, for example, set the `deleted` flag to true, which would then flag this `Entry` as deleted for future lookups. This works because in immutable datasets, where the management of state is paramount, we can see the entire lifetime of the data, from the moment it was created to the moment it was deleted. It's also important to note that during the lookup process, by selecting the "most-recent" `Entry`  for a key in the database, we are pulling its most-recent state.
-5. If the path segments (concatenated) match, but not the keys, then you are adding a new key to an existing `hash container`. Copy the trie array and extend it to the full length and add a pointer at the last index of the array of the same hash as the `Entry`.
-6. If the path segments (concatenated) do no match, compare both trie arrays and find the differing portion of the array. Copy all of the elements of the trie array, between the `current index` and the `diff index`, into a new trie array. **NOTE:** It doesn't matter whether the located trie array is empty or not.
-7. In this `Entry's` trie array, look up the corresponding element at the `diff index`. If empty, then the most similar `Entry` has been located.
-8. Create a pointer to this node, to the trie at the `diff index`, and the write process is finished. **NOTE:** All remaining trie elements will be empty and can be removed.
-9. If the `diff index` has a pointer, this means it isn't empty. If a pointer exists, follow that pointer to the next `Entry`. Recursively repeat this process from step #3.

**NOTE:** To delete a key/value, follow the same write procedure above and set `deleted` to `true` in the `Entry`. `Deletion nodes` will persist in the database forever.

##### Trie Encoding
Trie data structures are encoded into a variable length byte string as the `trie` field of an `Entry` message. It's important to reiterate that trie data structures are simply sparse, indexed arrays of pointers to entries.

**NOTE:** The following encoding schema and examples were also lifted from `DEP-0004`; only `buckets` in a dWebTrie are referred to as `containers`.

Consider a trie array with `N` `containers` and `M` non-containers `(O <= M <= N)`. In the encoded trie field, there will be `M` concatenated bytestrings in the following form:

`trie index (varint) | container bitfield * (packed in varint) | pointer sets **`

- * - Bitfield encodes which of the 5 values (4 values if the index is not mod 32) at this node of the trie have pointers.
- ** - Pointer sets each reference an entry at (feed index, entry index), where `feed index` is a varint with an extra "more pointers at this value" low bit, encoded as `feed_index << 1 | more_bit` and where `entry index` is simply a varint.

When dealing with a small/sparse dWebTrie, there will be a small number of non-empty containers; put another way, a small/sparse dWebTrie will have a small `M`. For a very large/dense dWebTrie (millions of key/value pairs), there will be many non-empty containers; put another way, `M` will approach `N` and containers may have up to the full 4 pointer sets.

-Consider an entry with a path hash:
`[1,1,0,0,3,1,2,3,3,1,1,1,2,2,1,1,1,0,2,3,3,0,1,2,1,1,2,3,0,0,2,1,0,2,1,0,1,1,0,1,0,1,3,1,0,0,2,3,0,1,3,2,0,3,2,0,1,0,3,2,0,2,1,1,4]`

-and the `trie`:
`[, ]`

In this case, `N = 64` (or you could count as 2, if you ignore trailing empty entries) and `M = 1`.

There will be a single bytestring fragment (chunk):

-`trie index` varint = 1 (second element in trie array).
-`bitfield` (varint 2) = 0b0010
-There is only `pointer set` for value `1` (the second value).
-There is a single pointer in the `pointer set` which is: `feed = 0 << 1 | 0, index = 1` or `(varint 2, varint 1)`.
-Combined, the `trie bytestring` will be:
`[0x01, 0x02, 0x02, 0x02]`

For a more complex example, consider the same `path hash array` with the `trie array`:
`[,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,]`

 If we `db.put ('/a/b', '24')`, we expect to see a single `Entry` of the `InflatedEntry` type at `index1` of the underlying dDatabase feed.

For reference, the first 104 bytes in the path match those of  the `/a/b/c` example from earlier, because it shares two of its 3 path segments. Since this is the second entry, the `entry index` is `1`.

**NOTE:** There is a major different between an `entry index` and an `array index`. An `entry index` is a record (new index) added to the dWebTrie's underlying dDatabase, while a specific index within a trie array, is its position from within the array. To make sure that I kill all confusion, consider the below example:

The example of `/a/b` being `put` into the key-value store, if data was in plain English and not binary, would look something like this pseudo-representation, once inside a dDatabase feed:

```
Index                           Value
= = = = = = = = = = = = = = = = = = = = = = = = = = =
1                                 {
                                     type: dwebtrie,
                                     key: Entry,
                                     value: 24,
                                     deleted: null,
                                     trie: [0x01, 0x02, 0x02, 0x02],
                                     ...
                                   {
```

The placement (index) with a dDatabase and the placement of an element in the above trie array, are clearly apples and oranges and I did not want there to be any confusion.

Now, if we `db.put('/a/c', 'hello')` and expect a second `Entry` of `Entry` type, the `path hash array` for this key (index 2) is:
`[1,2,0,1,2,0,2,2,3,0,1,2,1,3,0,3,0,0,2,1,0,2,0,0,2,0,0,3,2,1,1,2,0,1,1,0,1,2,3,2,2,2,0,0,3,1,2,1,3,3,3,3,3,3,0,3,3,2,3,2,3,0,1,0,4]`

The first 32 characters of this path are common with the first `Entry` (they share a common prefix, `/a`). `trie` is defined, but is mostly sparse. The first 32 elements of common prefix match the first `Entry` and then two additional hash elements ([0,1]) happen to match as well. There is not a `diff index`, until `index 34` of the array (zero-indexed). At this entry, there is a reference pointing to the first `Entry`. An additional 29 trailing null entries have been trimmed in the reduced metadata overhead.

Next, we insert a third node with `db.put('/x/y', 'other')`, and get a third `Entry`, where the `path hash array` is (index 3) is:
`[1,1,0,0,3,1,2,3,3,1,1,1,2,2,1,1,1,0,2,3,3,0,1,2,1,1,2,3,0,0,2,1,0,2,1,0,1,1,0,1,0,1,0,3,1,0,0,2,3,0,1,3,2,0,1,3,2,0,1,0,3,2,0,2,1,1,4]`

Consider the lookup process for `db.get('/a/b')`, which we expect to return `24`, as written in the first `Entry`. First, we calculate the path for the key `a/b`, which will be the same as the first `Entry`. Then we take the "latest" (most-recent) Entry, with entry `index 3`. We compare the `path hash arrays`, starting at the first element, and find the `diff index` at `index 1` of the array `(1 == 1, then 1 != 2)`.

We look at `index 1` in the current `Entry's` trie, and find a pointer to the entry at `index 2` of the underlying dDatabase itself, so we fetch the `Entry` and recurse. Comparing `path hash arrays`, we now get all the way to `index 34` of the array before there is a difference (`diff index`). We again look at the trie, find a pointer to the entry at `index 1` of the underlying dDatabase and fetch the first `Entry` and recurse. The path elements of the entry at `index 1` of the feed (dDatabase feed) match exactly and therefore, we have found the entry we are looking for and it has an existing value, so we return the value, which is `24`.

Lastly, consider a lookup for `db.get('/a/z')`. This key does not exist, so we expect dWebTrie to return with `key not found`. We calculate the `hash path array` for this key:
`[1,2,0,1,2,0,2,2,3,0,1,2,1,3,0,3,0,0,2,1,0,2,0,0,2,0,0,36,2,1,1,2,1,2,3,0,1,0,1,1,1,1,1,2,1,1,1,0,1,0,3,3,2,0,3,3,1,1,0,23,1,0,1,1,2,4]`

Similar to the first lookup, we start with the `entry index` 3, and follow the pointer to `entry index` 2. This time, when we compare `path hash arrays`, the first `diff index` is at `array index` 32. There is not a trie entry at this index, which tells us that the key does not exist in the database.

###### Listing A Prefix
Continuing on with the current state of the dDatabase below dWebTrie abstraction layer, we call `db.list('/a')`, to list all keys with the prefix `/a`.

We generate a `path hash array` for the key `/a` without the terminating symbol (4):
`[1,2,0,1,2,0,2,2,3,0,1,2,1,3,0,3,0,0,2,1,0,2,0,0,2,0,0,3,2,1,1,2]`

Using the same process as the `get()` lookup, we find the first `Entry` that entirely matches the prefix, which is `entry index` 2. If we had failed to locate any `Entry` with a complete prefix match, then we would return an empty list of matching keys. If we start with the first prefix-match node, we save that key as a match (unless `deleted` is `true` in the `Entry`). Then, select all trie pointers with a higher index than the length of the prefix and recursively inspect all pointer-to `Entries` and the `Entries` they point to and so on, until a complete list tree is built.

#### The Baseline Foundation For A Distributed File System
For a server-less web like the dWeb to work, it would have to find a way to not only transfer data between peers, but to transfer entire files and therefore, entire file systems between peers, just as servers do when teamed with the HTTP protocol on a server-to-client basis. Doing this on a peer-to-peer basis, as the dWeb has accomplished, not only requires a protocol for exchanging indexed abstract data blobs between peers (dDatabase), but another protocol (dWebTrie) layered above this for creating a file system-like logical and persistent database structure, which utilizes a key-value format, so that file-paths (keys) can be appended to a blob, with file data (values) and efficiently traversed by receiving peers performing specific lookups.

This way, an entire file system can be stored within a single file (a single dDatabase feed) and any peer in the world who is able to locate the feed via dWeb's DHT, can then communicate with the peers within the underlying dDatabase's swarm, request the dDatabase and subsequently download the dDatabase to their machine and easily consume the data via higher-level abstractions (like dWebTrie, or even higher levels like dDrive's actual file system layer).

dWebTrie was designed to store the data that derives from an entire file system within a dDatabase and allow higher-level abstractions like [dDrive](#ddrive) to consume that data and reproduce the entire file system on a remote peer's device. dWebTrie can traverse millions of records within a dDatabase and relate specific path segments via `list()`, without any bottlenecks. This makes dWebTrie the perfect file system database for a distributed file system, considering it is built on top of a distributed, append-only data feed that can be exchanged between peers in real-time. I can't make the rationale for dWebTrie as a file system for distributed file systems any clearer, other than to say that our current dWebTrie implementation scales with `O(N log(N))`.

###dDrive
dDrive is a secure, real-time distributed file system abstraction layer on top of dWebTrie, designed for the real-time exchange of file systems on a peer-to-peer basis. When comparing the dWeb to the World Wide Web, one could easily arrive at the conclusion that a dDrive is the equivalent to a server on the World Wide Web for some use-cases, like handling a client's request for a specific file. This is somewhat true, since a dDrive can be discovered on dWeb's DHT and a request can be sent to the peer(s) in its underlying swarm for specific files, or the entire file system for that matter - although the process of doing this on the dWeb is more secure and far more efficient.

**NOTE:** a dDrive cannot replace a server and its ability to act as a backend for both user authentication and remote application execution (currently, at the time of this writing). Web applications within a dDrive can utilize blockchain-based protocol suites like dWeb's [ARISEN](#arisen) to build truly serverless and decentralized applications that do require a backend and/or user authentication.

Just as I noted within my explanation of dWebTrie, dDrive is also completely logical; in other words, it doesn't technically exist. A dDrive is an abstraction for storing files from a file system as dWebTrie-based `Entries` within a dDatabase and reproducing the actual file system and its files from those same `Entries`.

Consider the following pseudo-representation of how files in a dDrive are logically referenced via a dWebTrie and actually stored within a dDatabase:

-Alice creates a dDrive with her website files:
```
index.html
img/logo.png
```

-dDrive creates the following key-value entries into its underlying dWebTrie (put):
```
Key                              Value
= = = = = = = = = = = = = = = = = = = = = = = = = =
/index.html                 <html><head>...
/img/logo.png             8fe1e4... (image file converted to hexadecimal notation)
```

-dWebTrie then creates a dDatabase and the following entries:
```
Index                           Entry
= = = = = = = = = = = = = = = = = = = = = = = = = =
1                                 {
                                     type: dwebtrie,
                                     message: Entry,
                                     key: /index.html,
                                     value: <byte array of file data>
                                     deleted: null,
                                     ...
                                   }

2                                 {
                                     type: dwebtrie,
                                     message: Entry,
                                     key: /img/logo.png,
                                     value: <byte array of file data>,
                                     deleted: null,
                                     ...
                                   }
```

**NOTE:** The first entry in the feed (index 0), as noted in [dWebTrie Schema](#dwebtrie-schema) is a special protocol header entry, which is not shown in the above example in order to keep all pseudo-representations simple.

-dDatabase then encodes both entries in binary notation and stores them in the feed:
```
Index                           Entry
= = = = = = = = = = = = = = = = = = = = = = = = = =
0                                 01011101101010...
1                                 00101001110101110...
2                                 0101101011011001...
```

#### dWeb Network Address
Since a dDrive is simply a dDatabase, it uses the dDatabase's public/private keypair and therefore its dWeb network address.

Remember, a dWeb network address is simply a 32 byte hexadecimal hash and in the case of a dDrive, its a 32 byte hexadecimal hash of its underlying dDatabase's public key.

The following is an example of a dWeb network address:
`40a7f6b6147ae695bcbcff432f684c7bb529lea339c28c1755896cdeb80bd2f9`

#### dDatabase Protocol Headers
You will recall in [dWebTrie Schema](#dwebtrie-schema) that the first entry in its underlying dDatabase, contains a `protocol header`. Protocol headers at `index 0` are used to decipher what the proceeding entries in the dDatabase are related to.

Below are the fields included in a dDatabase's `protocol header`:

| Field No. | Name | Type | Description |
| --- | --- | ---- | --- |
| 1 | Type | Length-prefixed | The type of data in the dDatabase |
| 2 | Content | Length-prefixed | 32-byte public key of content feed (if type is `ddrive`) |

If the `type` is `dwebtrie`, then we know there is a single dDatabase feed and that the overlaying dWebTrie abstraction layer has formatted the data as a key-value store. If the `type` is `ddrive`, then we know there is a separate `content` feed (as explained in the next section [Content and Metadata Feeds](#content-and-metadata-feeds) ), whose 32-byte public key can be found in the `Content` field of the feed's header.

The `Content` field allows two or more feeds to be coupled together, while the overall `protocol header` is used to distinguish the data set type within an underlying dDatabase. This simple header, as you will see later in this paper, is giving way to the creation of many decentralized technologies, outside of just a decentralized web, like the dWeb.

That brings me to dDrive's coupling of dDatabase feeds.

#### Content and Metadata Feeds
Contrary to the pseudo-representation that was shown in the previous section, a dDrive doesn't use a single dDatabase, it uses two coupled feeds to represent files and folders. One feed is for metadata and the other represents files and folders. Put another way, one feed is for file metadata and the other feed is for the data related to files. The `metadata` dDatabase feed contains the names, sizes and other metadata for each file, and is typically sparse, even when the overlaying dDrive contains a large amount of files and folders. The `content` dDatabase feed contains the actual file contents and is of the plain "ddatabase" type, instead of the `dwebtrie` or overlaying `ddrive` type.

When a dDrive is created, dWebTrie is initiated and creates two underlying dDatabase feeds. In the protocol header of the `metadata` feed, the `Type` field is sent to `ddrive` and the `Content` field is set to the 32-byte public key of the `content` feed.

When a file is added to a dDrive, a dWebTrie `Inflated Entry` is created for the file metadata in the `metadata` feed and the actual file data is stored as an arbitrary data blob within the `content` feed. If you recall in the [dWebTrie Schema](#dwebtrie-schema), an `Inflated Entry` contains the `contentLog` field, to point to the location in the `content` dDatabase feed for a given dDrive, where the file contents, related to the `metadata` `Inflated Entry`, are located. Put another way, each `metadata` feed array points to where in the `content` feed the file data is located, so you only need to fetch the contents of files you are interested in.

Here is an updated pseudo-representation of a dDrive with a simple `index.html` file, so you can update your mental model of how a dDrive is actually constructed:

-A blank dDrive is created, where dWebTrie initiates the creation of a blank dDatabase for the `content` feed, and a blank dDatabase feed for the `metadata` feed.

-The following `protocol header` is added as the first entry in the `metadata` feed:
```
Index                            Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                  {
                                     Type: ddrive,
                                      Content: <public-key-of-content-dDatabase>,
                                    }
```

-A file called `index.html` is added to the dDrive and the following entries are added to the `content` and `metadata` feeds:
```
Metadata Feed:
--
Index                            Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                  { // Protocol Header
                                      Type: ddrive,
                                      Content: <public-key-of-content-database>
                                    }
-- newly added entry:
1                                  { // Metadata Inflated Entry
                                      key: /index.html,
                                      value: metadata-for-file,
                                      deleted: null,
                                      trie: <pointers to other files or folders where the path is created> (none in this case),
                                      contentLog: <index number in content feed, where file data is> (1)
                                    }
```
```
Content Feed
--
Index                            Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                  { // Protocol header
                                      Type: ddatabase
                                    }

1                                  010110... (Binary blob of index.html file content)
```

-In actuality, both dDatabases are truly in binary when it's all said and done:

```
Content Feed:
--
Index                             Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                   010110111...
1                                   1101110001...
```
```
Metadata Feed:
--
Index                             Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                   01011101110...
1                                   10110111011...
```

There are many reasons the `content` feed doesn't use the dWebTrie abstraction layer/format for appending data, one of which is the limited value constraint of the `value` field in a dWebTrie's `InflatedEntry`. Other reasons include the fact that there is no need for pointers via the `trie array`, since related path segments are sorted via the `metadata` feed where a `trie array`, via each `Inflated Entry's` `trie` field, can be found.

It's actually quite simple to build a tree of files and folders via the `metadata` feed by relating `path segments`. After a tree is calculated, thanks to the location of the `content` feed in the protocol header and the `contentLog` field in each `Inflated Entry`, the contents of a file can easily be located.

#### Metadata Value Field Schema
In the previous pseudo-representation of how a dDrive is created, you will notice that in the `Inflated Entry`, within the `Metadata` feed of the `index.html` file, the `value` field is where the metadata is located. This field carries its own schema, as follows:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Mode | uint32 (varint) | Unix permissions (setuid or setgid) |
| 2 | UID | uint32 (varint) | User ID (Unix) |
| 3 | GID | uint32 (varint) | Group ID (Unix) |
| 4 | Size | uint64 (varint) | File size in bytes |
| 5 | Blocks | uint64 (varint) | The number of object blocks |
| 6 | Offset | uint64 (varint) | Similar to Node's FS module |
| 7 | ByteOffset | uint64 (varint) | Similar to Node's FS module |
| 8 | MTIME | uint64 (varint) | Modified time |
| 9 | CTIME | uint64 (varint) | Created time |

This is packaged within a `protocol Buffers` encoded message and used as the value `field` within a dWebTrie-based `InflatedEntry` message, which is also encoded with `Protocol Buffers` and appended as an entry in the `metadata' feed.

#### Version Controlled
As mentioned in [dDatabase](#ddatabase), files transferred over `HTTP` are not versioned, which means the historical state of a file or files is not available. `HTTP` is used to transfer a file's or files' current state from the server it/they exist on, to the client they're being requested from. When it comes to the `DWEB` protocol and its underlying protocols, the underlying data is [immutable](#the-case-for-immutability), which means that any rendition of the underlying data's state can be requested and constructed. In the case of a dDrive, each time a file is added to a dDrive or the metadata/content of a file is modified, entries are made to its underlying dDatabase feeds, where each entry represents an instance of the dDrive's state.

Each modification to a dDrive represents a new version of the dDrive. String names can also be assigned to dDrive versions and these can be stored within the drive itself, creating a straightforward way to switch between semantically meaningful versions. Simply put, a dDrive is built on top of append-only logs where old versions of files are preserved by default. You can get a read-only snapshot of a specific version of a dDrive at any time. Additionally, as mentioned above, you can tag versions with string names, making them more parseable.

Tags are stored inside the dDrive's `hidden trie`, meaning they are not enumerable using dDrive's [standard filesystem methods](#standard-filesystem-methods). They will replicate with all other data in the drive, though.

For more information on version control method's, see dDrive's JavaScript implementation and its official documentation [here](https://github.com/distributedweb/ddrive#version-control).

#### Sparse Downloading
Now that it has been explained how a dDrive's file system is stored and versioned within two coupled dDatabase feeds, it becomes easier to understand how one peer can download a specific version of specific files from a specific dDrive, otherwise referred to as `Sparse Downloading`. Simply put, since all of the layers that overlay a dDatabase feed utilize its underlying [dDatabase Protocol](#ddatabase-protocol) for the request and fulfillment of data between peers, any of these abstraction layers can make a request for a specific index range, for whatever that equates to at a specific abstraction layer. For example, when a specific file or folder is requested at the dDrive layer, it's common for the entire `metadata` feed to be requested from the dDrive's swarm. Once received, the file or folder path is passed to the dWebTrie layer, which searches the `metadata` feed for the matching path. If the current version of the file or folder is wanted, rather than older versions, the `most-recent` `Inflated Entry` that matches the path is chosen. This `Inflated Entry` should contain a `contentLog` pointed to the `index` where the file's data is stored in the `content` feed. From there, a `request` message can be sent to the dDrive's swarm, using an underlying dDatabase Protocol-based duplex stream, where the specific `index` from the `contentLog` can be requested.

For example, if the link `dweb://<dweb-key>/index.html` is requested via a ` DWEB`-ready client, the entire dDrive in this instance is not downloaded; as a matter of fact, only the `index.html` file and any files linked within it are downloaded by the requesting peer.

This is how the process would take place:
-1. A remote peer requests `dweb://<dweb-key>/index.html` via a DWEB-ready client.
-2. The client looks up the dWeb key via dWeb's DHT and a list of peers who are announcing this dDrive (its swarm) are returned.
-3. The client chooses a peer from the returned list and requests to open a dDatabase protocol-based duplex channel on channel 1 with the peer (or even multiple peers).
-4. The client asks for only the `metadata` feed, once the peers accept the connection request, by sending a `request` message on the channel, requesting the entire `metadata` feed's index range.
-5. The peer(s) sends a series of `data` messages back over the duplex channel containing the `metadata` feed.
-6. The client passes the `/index.html` path segment to the dWebTrie abstraction layer, so it can perform a lookup in the `metadata` feed.
 -6. An `InflatedEntry` is found, with a `contentLog` field within the entry, pointing to `index 6` of the `content` feed.
-8. The client locates the `content` feed's public key in the `protocol header` of the `metadata` feed and opens another duplex channel on channel 2, with the `content` feed's public key, with the same peers from step #3 and sends a `request` message for `index 6`.
-9. The `content` feed is sent in a `data` message to the client, containing **ONLY** `index 6`.

It should be noted, that at step #8, one could request the entire `content` feed, which would eliminate future remote requests, as long as both feeds are replicated live.

#### Drive Nesting
dDrive has a built-in "mounting" system, which allows one or more dDrive(s) to be mounted or "nested" within a parent dDrive. This allows Bob, to nest Alice's dDrive full of her published photos within his dDrive full of published photos. What is great about this, is that this enables true peer-to-peer collaboration, since when Alice adds a photo to her dDrive of published photos, they are automatically replicated within Bob's dDrive, since Alice's drive is nested within Bob's.

**NOTE:** It's important to note that Alice's dDrive is a totally separate dDrive from Bob's and can be accessed directly. Bob is simply a peer (seeder) of Alice's dDrive and mounts it within one his own dDrives, so that when a peer downloads Bob's dDrive, they also receive the most recent version of Alice's. In truth, this means that a portion of Bob's dDrive is downloaded from his dDrive's swarm and another portion from the swarm of Alice's dDrive.

For more information on dDrive mounting, please see dDrive's reference JavaScript implementation and its [mounting docs](https://github.com/distributedweb/ddrive#ddrive-mounting).

#### Standard Filesystem Methods
What one will find is that a dDrive implementation, like our JavaScript implementation, truly mirrors the standard filesystem methods of the Node.js `fs` module. Below is a quick explanation of those methods.

##### `createReadStream`
Read a file out of the dDrive, as a stream. Similar to `fs.createReadStream`.

##### `readFile`
Read an entire file into memory. Similar to `fs.readFile`.

##### `createWriteStream`
Write a file as a stream. Similar to `fs.createWriteStream`.

##### `writeFile`
Write a file from a single buffer. Similar to `fs.writeFile`.

##### `unlink`
Unlinks (deletes) a file. Similar to `fs.unlink`.

##### `mkdir`
Creates a directory. Similar to `fs.mkdir`.

##### `rmdir`
Deletes an empty directory. Similar to `fs.rmdir`.

##### `readdir`
Lists a directory. Similar to `fs.readdir`.

##### `stat`
Stat an entry. Similar to `fs.stat`.

##### `lstat`
Stat an entry but do not follow symlinks. Similar to `fs.lstat`.

##### `info`
Get mount information about an entry.

##### `access`
Similar to `fs.access`.

##### `open`
Open a file and get a file descriptor back. Similar to `fs.open`.

##### `read`
Read from a file descriptor into a buffer. Similar to `fs.read`.

##### `write`
Write from a buffer into a file descriptor. Similar to `fs.write`.

##### `symlink`
Create a symlink from a link name to a target name.

##### `mount`
Mounts another dDrive at the specified mountpoint. A mount can be a specific version of a dDrive.

##### `unmount`
Unmount a previously-mounted dDrive.

#####  `createMountStream`
Create a stream containing content/metadata feeds for all mounted dDrives.

##### `getAllMounts`
Returns a `Map` of the content/metadata feeds for all mounted dDrives, keyed by their mountpoints.

##### `close`
Close a file. Similar to `fs.close`.

##### `version`
Returns the current version of the drive.

##### `key`
Returns public key indentifying the drive.

##### `discoveryKey`
Returns a key derived from the public key (the dWeb network address).

##### `writable`
Returns a boolean indicating whether the drive is writable.

##### `peers`
Returns a list of peers currently replicating with a dDrive (via dWeb's DHT).

##### `checkout`
Checkout a read-only copy of the dDrive at an old version.

##### `createTag`
Create a tag that maps to a given version. If a version is not provided, the current version will be used.

##### `getTaggedVersion`
Returns a version, corresponding to a tag.

##### `deleteTag`
Delete a tag. If the tag doesn't exist, this will be a no-op.

##### `getAllTags`
Return a `Map` of all tags.

##### `download`
Download all files, in path of current version. If no path is specified, this will download all files.

For more detailed documentation, please see dDrive's JavaScript implementation [here](https://github.com/distributedweb/dDrive).

#### Live Replication
A dDrive can be live-replicated, by keeping two duplex streams open with online peer(s) of a dDrive's swarm, while sending a constant `request` message to said peers, for the entire index range of both the `content` and `metadata` feeds. This is easily done via the dDrive layer by simply using the [`replicate`](https://github.com/distributedweb/ddrive#replication) method in a dDrive implementation.

Replication can also take place on a per-file or per-path basis, rather than downloading updates for an entire dDrive's underlying files/paths within its filesystem.

##### dDrive Auditing
Since a dDrive is technically two dDatabase feeds, the data is easily  audited and validated as to having derived from the holder of the public-private keypair, and the dweb network address that mathematically derived from the public key.

Also, since a dWebTrie and a dDatabase are both single-writer, it's an easily proven fact that only the creator of a dDrive can write to it, preventing outside forces from, for example, manipulating a website's content. It is also easily provable that data within a dDrive has not been altered, since it's an append-only log. For more information on how data within a dDatabase is validated, please read about dDatabase's [Merkle Trees & FIO Trees](#merkle-tree-and-fio-trees).

### ARISEN
ARISEN is a suite of blockchain protocols, powered via smart contracts, that also acts as a development framework for dWeb-based applications, by providing a distributed virtual machine (global computer), a smart contract engine, a universal authentication layer, decentralized network consensus, decentralized payments, a decentralized domain name system and a decentralized reporting system.

#### On-Chain Vs. Off-Chain Protocols
While dWeb's DHT (DWDHT), dDatabase (DDB), dWebTrie (DWT) and dDrive (DDRIVE) form dWeb's "off-chain" protocol suite, ARISEN forms the dWeb's "on-chain" protocol suite. It's important to explain the difference between `off-chain` protocols and `on-chain` protocols, other than what is obvious. While blockchain technology is great for many use-cases, as are peer-to-peer networks like the one formed by dWeb's `off-chain` protocol suite, both of these have obvious deficiencies and crystal-clear bottlenecks when either is used independently in the formation of a decentralized web. Although, when both are combined, a decentralized web, with its own distributed and decentralized application development suite, is brought to life. Here were some of the issues we were able to solve through the combination of ARISEN and dWeb's `off-chain protocols`:

-1. Since an application and its files that are distributed to users via a dDrive are completely open source, a distributed database of some sort would have to be distributed in the dDrive so that users of the web application and their actions within the app could be written to this nested database. (**NOTE:** this database would be located within a separate dDrive, which would be nested with the app's dDrive. The database would have to be a multi-writer distributed database, like [dAppDB](https://github.com/distributedweb/dappdb), that allows multiple users to write to the database, since a dAppDB is made up of a constantly growing web of dDatabases, each of which is written to by a unique user (more on this in [Multi-Writer Databases](#multi-writer-databases)). This requires the initial creator of the dAppDB to constantly authorize new keys and therefore introduces a slight bit of centralization, since the dDrive creator would have to approve the authorization from their end; a change that would update the state of the nested dDrive, which would replicate to all of the app's users in real-time as well. This creates an issue where an app's creator could decide which users can use their application, through their ability to choose who they authorize to write to the app's database.

As you will see in subsequent issues within this section, ARISEN easily provides a decentralized, on-chain data persistence layer for applications which CAN allow for the use of off-chain distributed databases, that which the application itself would be unable to control.

-2. Applications have users and need a decentralized user management system of some sort, where users are in control of their authentication details. Centralized applications store user credentials within a centralized database, typically have their own integrated user and user permissions system and therefore control who can and who cannot use their applications. In the world of decentralized applications, a user management system cannot be managed by the app developer, even if it's open and based around public key cryptography, due to the fact an application could still manage to choose who can and cannot use their application. dDrives are controlled by their creator, while a blockchain provides a decentralized and trusted third party for user authentication. A blockchain protocol suite like ARISEN, that has an onboard smart contract engine, allows an application developer to isolate actions that require user input and therefore user authentication within a smart contract that's stored on ARISEN's blockchain. Actions within a smart contract (i.e., creating a post on a social network) can be executed from the dDrive's application files (outside of the blockchain), where the action is initiated by a blockchain-based account, signed with the private key of the account and broadcasted to the blockchain, where it can be validated by trusted (in the case of ARISEN, elected) members of the blockchain network. Where, if verified, the data that derives from the action is stored in a database that is related to the smart contract on ARISEN. This clearly solves the database issue faced in #1, where multiple users can write to an application's database without the application developer preventing certain users from doing so, since the application developer is not in control of the blockchain, like he is in control of the app's dDrive. The on-chain database could be used to point to off-chain distributed databases that only users are in control of for data storage. This also insures that an application doesn't have to rely solely on a blockchain, instead using it as a pointer to off-chain databases. Beyond ARISEN providing a user management system, it provides `human-readable` usernames and an access-control system (permission system), which applications can easily be designed around.

-3. The dWeb, for it to be used by non-technical users, needs a decentralized domain name system (dDNS), since 64-character hexadecimal addresses are too difficult, if not impossible, to memorize. Centralized domain names would not work for several reasons, one being that the standard DNS system is not designed to work with dWeb addresses and two, as I noted in the [Preface](#preface) of this paper, centralized domains can be seized by tyrannical entities. dWeb's off-chain protocols like DWDHT could be used for a domain name system, but I have determined that actions of domains, like the actions of accounts, need to be cryptographically validated. Also, with DHT, someone would have to control the "issuance" of dTLDs. ARISEN allows for the creation of premium accounts that can be won via auction, which can be used to create a sub-account that contains a period (.). For example, the account "dcom" can be won, and used to create a subaccount like "website.dcom", which resembles a domain and has its own permission-levels and associated keypairs, since it can authenticate with ARISEN (like any other account). The won name clearly acts as a dTLD and auctions clearly decentralize the issuance. A smart contract can then be developed for creating dDNS records for a domain, where the domain itself has to sign for the creation of a record. For example, this contract would have an `add` action that accepts the following parameters:
-`domain` (account used for validation)
-`record_name`
-`ttl`
-`class`
-`type`
-`rdata` (dWeb Network Address)

Like I mentioned in #2, each action would require an account (in this case a domain) to sign for the action. Once signed and validated, the above record would be stored in a database on ARISEN, logically associated with the contract and the domains as well, so that record lookups via ARISEN's API can be unique to a domain - more on this in [On-Chain Data Persistance](#on-chain-data-persistance).

-4. Then comes the issue of payments. Since applications on the dWeb are distributed openly within a dDrive, integrating centralized forms of payment simply isn't possible, since the integration with these types of systems would reveal API keys (secret keys) used to integrate and authenticate with these services (e.g., PayPal). This forces applications to utilize decentralized forms of payments via networks like ARISEN (RIX), Bitcoin (BTC), EOS (EOS) and others. Obviously, the dWeb is blockchain agnostic when it comes to application development, but the dWeb protocol, as explained in (DWEB)(#dweb), bridges both the off-chain and on-chain protocols discussed in this paper, for reasons discussed later. While off-chain currencies like CloudCoin could certainly be integrated, there are still questions regarding the centralization of the DNS servers CloudCoin uses as a way of validating the authenticity of CloudCoins; although, this doesn't create an issue where the apps developers or an outside entity can seize CloudCoins. Technically, anyone could launch their own validation system and, since CloudCoin is distributed within image files, there is no ledger or transaction history, introducing anonymity, which might be avoided by some developers.

While a similar system like CloudCoin could be developed using dWeb's DHT, we have not embarked on such a project but we certainly will in the future.

-5. The downsides of a decentralized web with no checks in place would allow for the illegal distribution of narcotics and child pornography. I will certainly mention terrorist activity, but I won't over-emphasize since it is consistently over-exaggerated by various government entities to circumvent the Bill of Rights and discourage the use of certain privacy-enabling technologies. If the dWeb only consisted of off-chain protocols, the dWeb would act as a Torrent network for websites and web applications and would certainly be a free-for-all for criminals. Again, ARISEN's blockchain protocol became the solution to this issue, through its builtin election/voting system and 21-member elected governance, which has the power through a 15/21 majority vote to reverse transactions (actions). Since dWeb network addresses are registered on ARISEN (see [DWEB Protocol](#DWEB) and domains, as well as dDNS records are controlled and stored on ARISEN (see [dDNS](#ddns)), the governance could halt the activity taking place on specific dWeb network addresses and domains, further protecting the network and dWeb's users from illegal and nefarious activity.

It became important, in our eyes, to create a reporting system using an ARISEN smart contract and integrate it with other contracts, like `dweb` and `arisen.wrap`, so that community members could report illegal activity and vote on its submissions to the governance for a `removal vote`. This gave way to the creation of dWeb's ratified [Constitution](#dweb-constitution), which governance members must abide by in the removal of illegal content. The Constitution protects free speech and many other human rights.

dWeb's off-chain protocols are incapable of providing this sort of solution, since it requires all of the solutions explained in #1 through #4.

It was essential that I started off the ARISEN section explaining why a blockchain protocol suite was needed, along with the differences between `on-chain protocols` and `off-chain protocols`. `On-chain protocols` provide a decentralized and trusted network that help bring to life the necessary facilities needed to allow for the development of web applications that provide end-to-end decentralization for their users, while also placing checks on the content that is distributed between peers via `off-chain protocols`. This bridge between both protocol types is what forms what we call the `dWeb`, hence the reason why we named the protocol that bridges the `off-chain` and `on-chain` networks the [DWEB Protocol](#dweb).

The sub-sections that follow explain the ins-and-outs of ARISEN and its underlying protocols, algorithms and features.

**NOTE:** If it is your view that #1 through #5 introduced too much too quickly, it is my view that many developers discourage the inclusion of blockchain in to many projects. I felt like it was appropriate to explain first, why the dWeb combined both  `on-chain` and `off-chain` protocols and second, why `off-chain` protocols were incapable of accomplishing what `on-chain` protocols can with exceptional simplicity and efficiency. If any of the previous section confused you, due to a lack of understanding of a `singleton computer` like ARISEN, all of what was explained in these sections is thoroughly detailed in subsequent sections.

#### The Global Computer
ARISEN can be described as a global computer, in other words, it's much more than just a blockchain. You can think of a blockchain as a bunch of protocols that form decentralized information exchange platforms. From a computer science perspective, ARISEN is a deterministic but practically unbounded state machine, consisting of a globally accessible singleton state, along with a globally distributed virtual machine (see [RSN VM](#rsn-vm)) which is designed to programmatically apply changes to ARISEN's overall state.

From a practical perspective, you could think of ARISEN as a massive, multiwriter dDatabase that forms a ledger that is written to by a trusted and elected network of computers that have the capability of executing application code which is ultimately stored within the ledger itself and subsequently used to update the ledger (the ARISEN state). This ledger is referred to as a "blockchain" in scientific terminology and is used to synchronize and store the system's state changes. A cryptocurrency known as RISE (RIX) is used to meter and charge for the cost of storing computed data on the ledger, to insure that ARISEN's global computer resources cannot be abused by bad actors.

#### Turing Completeness
The term "Turing Completeness" refers to the father of computer science, Alan Turing. When working with ARISEN, you will be lucky if you don't stumble across the terms "Turing Completeness" or "Turing Complete." In 1936, Turing created a mathematical model of a state machine that manipulated symbols by reading and writing them on sequential memory (meant to resemble an infinite-length paper tape). With this construct, Turing would then release a mathematical foundation to answer questions about universal computability; meaning, all problems are solvable. Turing ultimately proved that there are classes of problems that are incomputable.

In this model, Turing specifically provided the "halting problem," whether it was possible, given an arbitrary program and its inputs, to determine whether the program will ever stop running. Turing further defined a system to be "Turing Complete" if it can be used to simulate any Turing Machine. Such a system is called a "Universal Turing Machine" (UTM). ARISEN's ability to execute a stored program (smart contract) in a state machine (ARISEN's global computer), while reading and writing data to memory (ledger), makes it a Turing Complete system and therefore a UTM. ARISEN can compute algorithms that can be computed by an Turing Machine, given the limitations of finite memory.

Turing proved that you cannot predict whether a program will terminate by simulating it on a computer. In other words, we cannot predict what the true outcome of a program will be without running it. Turing Complete systems can run in infinite loops or in simpler terms, without a termination point. They say it's trivial to develop a program that runs in a loop that never ends (which I disagree with), but intended never-ending loops can arise without warning, due to the complex interactions between the starting conditions and the code. ARISEN cannot predict if a smart contract will terminate or how long it will run without actually running it. This means that smart contracts can be designed to purposely run forever, after a node attempts to validate them, so that they consume all of ARISEN's available network resources.

To attack this issue, `Singleton Computers` (such as Ethereum) introduced the `gas` metering mechanism, whereas the ARISEN network uses the `NET`, `CPU` and `RAM` metering mechanisms, discussed further in [Distributing Computing Resources](#distributed-computing-resources). ARISEN can account for every instruction (computation, data access). In order to execute contract-based transactions on the network, a user is required to have `NET`, `CPU` and `RAM` resources, so that infinitely-looping contracts can eventually be halted. `NET`, `CPU` and `RAM` can be acquired through the `staking` of RIX on the network.

#### Smart Contract Engine
When it is said that ARISEN is also a smart contract engine, it must be pointed out that much of ARISEN, like any other computer - distributed or not - gains its functionality from an underlying set of programs that are stored within it. I like to say that Bitcoin is also a distributed computer like ARISEN, but Bitcoin is limited to a single program that centers around the minting and validated transacting of Bitcoins. The entire state of the Bitcoin computer contains a current historical record of every transaction that has ever occurred from the moment it was started. ARISEN is capable of running an infinite number of programs (smart contracts) which can be compiled and published into ARISEN's memory (ledger). All of the functionality of Bitcoin (minting and validated transacting) of Bitcoins, is combined into a single program on ARISEN known as `arisen.token`, the only difference being that `arisen.token` allows the minting and issuance of an infinite number of currency types, while Bitcoin is limited to a single currency type.

Aside from the minting and validation of various token-types, it is important to note that all of ARISEN's core system features store all smart contracts that were used to boot up ARISEN and maintain its complex methods of constant operation. In the sub-sections that follow, I will explain the purpose of each of these core programs.

##### `arisen.bios`
-The `arisen.bios` contract is a minimalistic system contract that simply supplies the actions that are absolutely critical in the bootstrapping of an ARISEN-based blockchain.
-Introduces the data structure of ARISEN's [Block Header](#block-header), weighted permissions, weighted keys, blockchain authorities and the ABI (application binary interface) hash structure.
-Enables the following actions:
--Account creation using [Human-Readable Names](#human-readable-names).
--Update permissions and associated keys for a specific account.
--Delete account permissions.
--Assign specific actions from a specified contract to a specific account permission.
--Publish a smart contract on the ledger.
--Sets the ABI for a contract, identified by account name.
--Cancel a deferred transaction.
--Send error notification when an error occurs while executing a deferred transaction.
--Set privileged status for an account.
--Set resource limits of an account (RAM, NET and CPU).
--Set a new list of active block producers, by proposing a schedule change.
--Set blockchain's parameters.
--Check if an account has authorization to access a current action.
--Activate a protocol feature.
--Assert whether a protocol feature was activated.

##### `arisen.system`
-This contract defines the structure and actions needed for an ARISEN-based blockchain's core functionality. Some of the BIOS functionality can be found here, since the system contract largely takes over for the BIOS contract, after the chain has been bootstrapped.
-Introduces the constants for blocks per day, minimum amount of RAM to activate block production, annual rate of inflation (for native currency), the inflation pay factor, producer pay (percentage of inflation) and the time delay for refunds.
-Introduces the data structure for name bids, bid refunds, bids, global blockchain parameters, product details, voter info, delegated bandwidth and refund request.
-Enables the following actions:
--Init actions for initializing the system contract for a specific version and a symbol.
--`On Block` action for paying producers and calculating the missed blocks of other producers.
--Set RAM limits for an account.
--Set NET limits for an account.
--Set CPU limits for an account.
--Activate a protocol feature.
--Delegate bandwidth (NET) or CPU.
--Undelegate bandwidth.
--Buy RAM.
--Sell RAM.
--Refund pending, unstaked RIX after delegation period.
--Register block producer.
--Unregister block producer.
--Set RAM supply.
--Set RAM rate.
--Vote for a block producer.
--Register voting proxy.
--Set blockchain parameters.
--Claim block production rewards.
--Set privilege status for an account.
--Create name bid (auction).
--Bid name refund.

##### `arisen.token`
-The `arisen.token` contract defines the structure and actions that allow users to create, issue and manage cryptocurrencies on ARISEN-based blockchains.
-Introduces the data structure for account balances and currency statistics.
-Enables the following actions:
--Create a cryptocurrency.
--Issue a specific amount of coins.
--Transfer a cryptocurrency from one account to another.
--Get supply of a cryptocurrency.
--Get the balance of an account (for a specified cryptocurrency).

##### `arisen.wrap`
-Allows elected block producers to change an account (and domain's) keys, modify a contract, an account's owner and reverse any transaction, through the execution of an action that bypasses regular authorization checks. This can only be successfully executed by a majority (15/21) of the 21 elected block producers.

#### The Blockchain
It is the programs, aka smart contracts, that when executed, output data which is packaged into transactions, subsequently packaged into blocks and ultimately validated by nodes that are controlled by the network's elected block producers. This section will attempt to explain ARISEN's built-in blockchain from block headers and block production, to the consensus algorithms used to validate blocks, while still meeting ARISEN's rigorous performance requirements.

##### An Example Smart Contract

```c++
#include <arisen/arisen.hpp>

using namespace arisen;

class [[arisen::contract]] hello : public contract {
  public:
    using contract::contract;

    [[arisen::action]]
    void hi(name user) {
      require_auth(user);
      print("Hello, ", name{user});
    }
};
```

The execution of the above `hi` action is quite simple to understand. One would simply provide the `hi` action with a `user` of the `name` type (an ARISEN account). As long as the `user` can sign the action with a key that matches one of the public keys associated with their stored account on ARISEN, this action will output "hi, name" with the name of the executing user, will generate a transaction, and will eventually be validated within a block. As discussed in the previous section, ARISEN's core functionality is handled most by the `arisen.bios` and `arisen.system` contracts. As the computer runs, regardless of whether users are interacting with it or not, blocks are being produced by ARISEN's 21 block producers, every half second, in an attempt to capture the blockchain's state on a per half second basis.

What is not shown in the smart contract is how data is saved to ARISEN using contract-supplied data structure(s). Contracts can supply specific data structures and can initiate the creation of a database on ARISEN where the data that derives from the execution of an action is ultimately stored. For example, when executing the `newacct` action within the `arisen.system` contract, a valid account name and two public keys are submitted, and if accepted as valid, are stored in the `accounts` table. Other actions within other contracts can now lookup users via the `accounts` table and validate the digital signature accompanying the execution of actions, to see if it derived from the user executing the action.

Simply put, everything that happens on ARISEN, whether it's the creation of an account, a vote for a block producer, or even the uploading/activation of a new smart contract, are actions that are handled by various smart contracts that are stored within ARISEN's low-level block-based database structure according to their own data structure and logical database names. As confusing as that may sound, a smart contract does indeed enable the uploading and activation of other smart contracts that can be created by anyone on the network to extend the capabilities of the ARISEN computer itself. Just like an account or a vote for a block producer, newly published contracts have their own data structure and are stored in their own database as well.

Before any action or its outputted data can be stored in their specified formats, the data must be validated as having derived from the specified user, and is then packaged in a transaction and transmitted by the executing user to the network. A block producer then confirms this data, places it in a block and then cryptographically verifies the legitimacy of the block. Once a block has been validated, it is then stored in the blockchain. For ARISEN, the blockchain represents a gigantic database of transactions but if you dive a little deeper, each of these transactions represents a data entry that derived from a contract formatted in a specific data structure, each of which are intended to be organized in a sub-database with other entry types. This creates what amounts to a massive distributed database management system, with an infinite number of possible data collections, all of which derive their data from the executions of programs which are stored in that very same system.

Actions, in most instances, depend on data that derived from a past execution of the same action or a different action. Take the `hi` action for example. The `hi` action requires the `user` parameter, which is an account. Accounts only exist on ARISEN, because they can be created via the `newacct` actions in the `arisen.system` contract. When created, they use the `accounts` structure and are stored in the `accounts` database. The `hi` action uses another action from the `arisen.system` contract called `require_auth()`, which performs a lookup on the `accounts` table and authenticates a given user. All rocket science aside, one could make the argument that ARISEN is a dumb robot and only knows what a given action (or actions) allows it to know. Every single action builds upon the data that derives from other actions. As developers add programs to ARISEN, there are more actions and more data to manipulate. Like a [dDatabse](#ddatabase)), all of this data is immutable, unless of course a block producer majority chooses to remove specific portions of it.

##### The Block Header
The blockchain is a synchronized collection of blocks, each of which consist of a collection of transactions, with each transaction consisting of a collection of actions, from the period of time a block was produced. Each block has the following header:

| Field No. | Type | Description |
| --- | --- | --- |
| timestamp | uint32_t | Block timestamp of when block was produced |
| producer | name | Name of producer who validated block |
| confirmed | uint16_t | The amount of block confirmations |
| previous | checksum256 | Link to the previous block |
| transaction_mroot | checksum 256 | Link to the transaction's Merkle root |
| action_mroot | checksum 256 | Link to the action's Merkle root |
| scheduled_version | uint32_t | A schedule version |
| new_producers | producer_schedule | Producer's schedule |

Like any other blockchain, the block header allows one block to be linked to another, whereas within the block, a Merkle Tree is created of all transactions, another is created of all actions, and the block pointer (block hash) is a hash of the block header. Like with a dDatabase, the Merkle Tree hash and transactions within an ARISEN-based block efficiently summarize and verify the integrity of the transactions and actions that derive them. The `new_producers` field sets the elected producers and their schedule for the next block. Since ARISEN's election is conducted on a per-minute basis, and blocks are produced every 0.5 second, the producer roll and the producer schedule can change often. The ARISEN software, and therefore the producers themselves, follow the producer schedule on the previous block for the current block. In any blockchain, synchronization and scheduling is key, which brings me to consensus.

##### Consensus
ARISEN is based around the Delegated Proof of Stake (DPOS) algorithm, which dictates who is allowed to validate blocks on the network and who receives the delegated authority to make governance-based decisions on behalf of the network. DPOS, when compared to other consensus algorithms, is by far the best choice when it comes to meeting the performance requirements of a completely decentralized web.

A few facts about ARISEN'S rendition of the DPOS algorithm:
-Those who hold RIX coins may elect "block producers" (governance members) through a "continuous approval voting system."
-Any member (account) on ARISEN can become a "block producer" candidate and can gain the vested power of a governance member, as long as they're in the top 21 candidates.
-ARISEN's blockchain produces blocks every 0.5 second and only one block producer has the authority to produce a single block.
-If a particular block is not produced at a scheduled time, then the block for that time slot is skipped.
-When one or more blocks are skipped, there is a 0.5 second or more gap in the blockchain.
-Blocks on ARISEN are produced in rounds of 126 (6 blocks each, multiplied by 21 block producers).
-21 block producers are chosen through the votes casted by members of the network at the start of each round.
-The 21 selected block producers are then scheduled to verify blocks within that specific round, in an order agreed to by 15 of the 21 block producers.
-If a particular block producer "misses" a block, or has not produced a block within the previous 24 hour period, they are removed from the governance and must inform the blockchain that they're ready to be a candidate once again. The performance and reliability of the blockchain depends on perfected network operations, where only provably reliable blocker producers are in position to keep the network running properly.
-Block producers do not compete for blocks, rather they work collectively to validate a round of blocks. Typically, this keeps the blockchain from experiencing forks.
-If the blockchain does experience a fork, the algorithm automatically switches to the "longest chain." In other words, the longest chain relates to the chain with the most blocks, which means it has produced the most blocks, which also means it has a higher percentage of block producers who prefer that particular chain and have ultimately reached consensus around it. This works because a fork of ARISEN with more block producers will be longer than other forks and will grow in length much faster, due to it missing fewer blocks.
-Block producers caught producing blocks on two forks at the same time will likely be voted out, which can be backed by cryptographic evidence as well.

###### Byzantine Fault Tolerance (BFT)
BFT stands for Byzantine Fault Tolerance, which prevents a block producer from signing two blocks with the same timestamp or the same block height. With BFT, once 15 block producers have signed a block, that block is deemed irreversible. Any block producer under this model could in fact produce two blocks with the same timestamp or block height, but will create cryptographic evidence of their "treason." By using BFT, an irreversible consensus is typically reached within 1 second.

###### Transaction Confirmation
DPOS blockchains like ARISEN, typically have 100% block producer participation. Within a quarter of a second, transactions are considered confirmed 99.99% of the time. All EOS.IO-based blockchains like ARISEN, have the added luxury of utilizing an aBFT (asynchronous Byzantine Fault Tolerance) for faster achievement of irreversibility. The aBFT algorithm provides 100% irreversibility within a single second.

###### Transaction as Proof of Stake (TaPOS)
ARISEN requires every transaction to include part of the hash of a recent block header. This hash serves two purposes:
-1. Prevents a replay of a transaction on forks that do not include the reference block.
-2. Signals the network that a particular user and their stake are on a specific fork.

Over time, all users end up directly confirming the blockchain which makes it difficult to forge counterfeit chains, as the counterfeit chains would not be able to migrate transactions from the legitimate chain.

#### Accounts, Authentication and User Permissions
In previous sections, I explained how ARISEN's account and user management systems were formed via the actions and data structures within the `arisen.system` contract. Without accounts, no other action within ARISEN contracts, including those within the system contract, would function since they all require account-based authentication. With that in mind, accounts and ARISEN's authentication protocols are single-handedly the most important feature because without them, nothing you're reading in this paper would be possible.

A few facts about accounts:
-Accounts on ARISEN, unlike many other blockchains, are unique "human-readable names" that must be 12 characters in length.
-Users choose their username when creating an account.
-A new account can only be created by an existing account, due to the fact that any action-generated data stored on ARISEN requires a certain amount of RAM to be staked by the user initiating the action. RAM is like disk storage on ARISEN and is needed to write to the blockchain's memory database for any action that results in stored data in memory, including the create account (newacct) action.
-This gives way to account creation services that charge a service fee, or ones that simply sponsor account creation, like [PeepsID](https://signup.peepsid.com).

When it comes to decentralized applications, application developers will pay a small cost for account creation to signup new users but the cost is nominal when compared to the costs needed to acquire new users via advertising or free services, etc. The greatest feature in relation to this model is that once a user has created an account on an ARISEN-integrated dApp, that same account can be used to log in to other ARISEN-integrated dApps. This means that only one app covers the cost and therefore application developers don't have to pay for the creation of every account of their users.

Names less than 12 characters are considered premium names and must be won at auction.

A few facts about premium names:
-Can be any number of characters (a-z, A-Z, 0-5).
-Must be won at auction. Any user can start an auction for a name and must be the highest bidder after 72 hours.
-Once won, other accounts can be created as sub accounts of the premium name, which must include a period (.). For example: `jared.rice`, in this case `rice` is the premium name.

##### Public Key Cryptography & ECDSA
While cryptography has been around for ages and public key cryptography since the 1960s, its use in blockchains has revolutionized how users are able to authenticate with blockchain-integrated web apps. The blockchain acts as a `Public Key Authority`, where any account's public key can be openly found on the blockchain itself, by anyone in the general public. This way, a user can `sign` anything (e.g., an action transaction) and the blockchain (or anyone for that matter) using `Elliptical Curve Digital Signature Algorithm` (ECDSA) can decipher that the owner of the private key, related to to the public key, was the creator of the digital signature.

ARISEN accounts, when created, require two permission levels, both of which have unique public keys associated with them. These permission levels are known as `owner` and `active`. Actions within ARISEN contracts each require the executing user to authenticate themselves using a specific permission level; in other words, the action must be signed with the private key that was used to derive the public key associated with the permission level selected for action-based authentication.

A few facts about ARISEN keypairs:
-Public keys start with the `RSN` prefix.
-Private keys are seeded with 128 random bits.
-Private keys start with the number `5`.

##### Role-Based Permissions Management
ARISEN uses a "role-based permissions management" system to determine if account-based actions are authorized. ARISEN's software provides a "declarative permissions management system" that gives accounts high-level control over who can do what and when. It is critical that authentication and permission management be standardized and separated from the business logic of an application, which enables the development of tools that manage permissions, generally, while also giving way to a significant boost in performance. An account can be controlled by multiple other accounts at once. Each one of these "controlling accounts" can have different weighted permissions, so that accounts have "multi-user control."

Every account can be controlled by any weighted combination of other accounts and private keys. This ultimately forms a hierarchical authority structure that reflects how permissions are organized and makes multi-user control easier than ever. Multi-user control is the single biggest contributor to security and when used properly, can greatly reduce the risk of theft due to hacking. ARISEN allows accounts to decide which keys and other accounts are allowed to send a particular action type to another account. For example, it's possible to have one key for a user's social media account and another for access to a ride sharing account. You could also give one account the ability to act on behalf of another account, without assigning a single key.

##### Vanity Permission Levels
ARISEN accounts are capable of creating "vanity permissions levels" whose origins are that of higher-level vanity permissions. Each vanity permission, or VPL, must define an "authority." Authorities on ARISEN are essentially "multi-signature threshold checks" that are comprised of keys and/or VPLs of other ARISEN accounts. For example, as ARISEN account could have a VPL known as "friend" that could be correlated to a specific "Action" on the account, and could be mutually controlled by any of the account's friends. The Steem blockchain (home of the Steemit social network) has three hard-coded VPLs known as `owner`, `active` and `posting`, where the `active` VPL can do anything but change the `owner` permission's key, which is the most powerful authority on the account and the `posting` permission can only perform social actions like voting and posting. ARISEN allows accounts to create their own custom-named (vanity) permission levels, by allowing account holders to define their own hierarchy of permissions, as well as a customized group of actions.

##### Permission Mapping
With ARISEN, each account is able to outline a specific "mapping" between a contract/action or contract of any other ARISEN account and their own VPL. A user, using a social media app, could "Map" his/her social media app to the "friend" VPL. This would allow his/her friends to post on behalf of this user to his/her social media page. While the friend's post still appears as the account holder, they would still be identified by their keys that were used to sign the "Action," therefore you could easily identify users who are abusing their permissions.

##### Evaluating Permission Levels
If a VPL is used, which doesn't exist, the "active" DPL is automatically used to authorize an "Action." Once a mapping is determined, the signing authority is validated using the threshold multi-signature process as well as the authority associated with the VPL. If this is unsuccessful, it traverses up to the `active` DPL and ultimately the `owner` permission.

##### Parallel Evaluation of Permissions
The process behind the permission evaluation process is immutable, where transactions based around the change of permissions are not completed until the end of a block, so that all key/permission evaluations for every transaction can be completed in parallel. The quick validation of permissions is possible without costly contract executions that would ultimately be rolled back, and transaction permissions can be evaluated as pending transactions are received so they don't need to be re-evaluated when they're confirmed. Parallelizing the permissions process, while making it "read-only," dramatically increased performance on the network, because of the significant percentage of computation that is required to validate permission-based actions. To add to this, when "replaying" the blockchain from a log of actions, the permissions do not need to be reevaluated. This also significantly lowers the computational load brought by the replaying of a constantly growing blockchain.

##### Stolen Key Recovery
ARISEN allows users to restore control of their account when their keys are stolen. An account owner can use the stolen `owner` key along with approval of a designated `account recovery partner`. It is important to note, that the account recovery partner cannot reset control of the account without the help of the user that possesses the `owner` key for the stolen account. Even if a hacker went through this process, it would be pointless due to the fact they they already control the account; although, even if the hacker did go through this process, the `account recovery partner` would probably demand some sort of multi-factor authentication; that is, if the account recovery partner is a dApp itself. A recovery partner has no authority over day-to-day transactions; rather, the recovery partner is only a "party" to the recovery process. This will ultimately reduce legal liabilities, costs, and lost currency that would otherwise be unrecoverable on blockchains that lack this feature.

#### Universal Authentication Layer & Transport
While public key cryptography is by far one of the most secure forms of authentication in the world, the process of securely managing one's private keys can be cumbersome and can introduce many security risks. As was mentioned in [Role-Based Permission Management](##role-based-permission-management), ARISEN gives way to the development of tools for the management of accounts and permissions, and if developed correctly, can greatly simplify the management of private keys and greatly reduce, if not eliminate, the security threats surrounding the management of private keys. Although, this is only half the problem.

dWeb-based apps that are integrated with ARISEN would require their users to copy and paste a 64-character private key when logging in, and each and every time they execute a specific action within an application (e.g., the `post` action within a social networking app). Just so one can build a mental model surrounding how this process works on the dWeb, try to follow the following description:

-1. A developer writes a simple application that allows someone to post a message to the page. The app itself is written in JavaScript and HTML and utilizes ARISEN's [JavaScript Library](https://github.com/arisenio/arisenjsv1) for executing contract-based actions via ARISEN's API.

-2. The developer writes a simple smart contract called "postit" with a `post` action, that requires an ARISEN account to authenticate using their `active` permission when submitting the action's `message` parameters. The developer deploys the "postit" contract to ARISEN and publishes his app files in a dDrive and announces on the dWeb.

-3. There is a "Post" button in the application, that when clicked brings up a popup that asks for the account username, the `active` private key and the message being posted. When the "send" button is pushed, the app then packages the `post` action with the entered message, along with a digital signature created with the entered private key, and transmits this action to ARISEN so that the `post` action can be executed.

-4. Once executed and validated, it is stored on ARISEN, where the app can now retrieve this data and display on the page.

This is a horrible user experience, even though the user was using a secure account/permission management software, the eventual exposing of the private key in the browser eliminated all of the security advantages provided by the key management software in the first place. This problem was solved by authentication apps like MetaMask and Scatter, which apps can use to communicate with the Ethereum and EOS blockchains. Using MetaMask and Scatter, the process goes like this:

-1. Instead of a "Post" button, there is a "Login" button. Bob clicks the "Login" button and the MetaMask app comes up, where Bob is asked to choose an account (and permission level) and enter a password.

-2. Bob is brought back to the app where he now see a "Post" button.

-3. Bob clicks "Post" and a popup appears with only a "Message" box.

-4. Bob enters the message and clicks "Send."

-5. The app then packages the `post` action with the entered message and the "logged-in" user via step 1 (note: there isn't an actual session, the app simply remembers which account/permission was chosen), along with the corresponding permission to MetaMask.

-6. The MetaMask app pops up, showing the packaged action and which user/permission is packaged with the action as the executor.

7. Once Bob clicks "Execute," Bob enters a password (for MetaMask) and MetaMask signs the action with the private key matching Bob's chosen account/permission level and the transaction is sent to ARISEN for confirmation and eventual inclusion into a future block.

**NOTE:** Authenticators like MetaMask use a password to encrypt the private keys it stores, so that when a user wishes to sign an action, or complete a transaction for that matter, the private key is decrypted with the entered password, used to sign the transaction and MetaMask subsequently removes the decrypted key from the temporary memory or off-line storage. The password is never saved.

This definitely made the process far more secure since Bob's private key was never revealed, but the user experience went from 4 very unsecure steps to 7 very secure steps and requires way too many back and forth interactions with the authenticator application (MetaMask). It was a major reason for the creation of ARISEN's Authentication Transport Protocol and what we refer to as the "Universal Authentication Layer."

##### Simplifying Authentication & Authenticators
As was seen in the previous section, there are many authenticator apps, each of which use their own protocol for transporting action data between apps and the authenticator itself. A simplified transport protocol would eliminate many steps in the authentication process, while also making the authentication process with decentralized applications even simpler than the authentication process with centralized web applications. The authenticator discussed in the previous section was for other blockchains, but hopefully helped expose the issues most decentralized applications face when it comes to user experience. The following sub-sections will discuss how various ARISEN protocols, tools and features have helped simplify the authentication process for dWeb-based applications.

###### Human-Readable Names
Unlike most blockchains that use 64-character (or more) hexadecimal addresses for "account addresses," ARISEN associates a username that is `human-readable` with a set of permissions, each of which is associated with a unique public key.

The use of `human-readable` usernames - rather than computer-generated hexadecimal or byte-formatted addresses - means that we're able to further simplify the authentication onboarding, and app-to-authenticator / authenticator-to-app request and response transport lifecycle, and the overall authentication transport process.

###### Universal Authentication Layer
The goal of ARISEN's `Universal Authentication Layer` is to bring to life a universal means for authenticating users, by standardizing the transport between dWeb-based applications and ARISEN-integrated authenticators via a universal bridge, which makes the authentication process between authenticators seem identical, regardless of how the underlying authenticator is designed. Put another way, it provides a UI layer for giving users a consistent UI/UX flow, independent of the authenticator they are using or the website they're on.

The Universal Authentication Layer brings to life support for Biometric Hardware Secured Keys (e.g., Apple Secure Enclave), and a way for insuring a user is completely aware of the app they're using, as well as the related smart contract they're executing an action within.

The Universal Authentication Layer involves the following components:
-[Chain Manifest Specification](#chain-manifest-specification)
-[Ricardian Specification](#ricardian-specification)
-[Authentication Transport Protocol](#authentication-transport-protocol)
-[Universal Authentication Library](#universal-authentication-library)

###### Chain Manifest Specification
An `off-chain manifest`, or manifests, is metadata about an application that is displayed in the root of a [dDrive](#ddrive) (e.g., dweb://<dweb-key>/chain-manifest.json). The location of these files, along with their checkout hash, is referenced in the smart contract that is utilized by the app in what is referred to as an `on-chain manifest`.

Both `off-chain manifest` files are contained in the `chain_manifests.json` and `app-metadata.json` files within the root of a dDrive. The `app-metadata.json` file contains the following fields:

-`spec_version` - The specification version of the `chain manifest`.
-`name` - The full name of the application. This will be user-facing in app listings, history, etc.
-`shortname` - A shorter name for the application.
-`scope` - An absolute path relative to the application's root. (`/` or `/app`, but not `../`).
- `apphome` - Tells the browser where your application should start when it is launched. This must fall within the `scope`.
-`icon` - An HTTPS url, DWEB url or absolute path relative to the application's root, followed by a SHA-256 checksum hash. May be displayed in app listings, history, favorites, etc. (.e.g, `dweb://dsocial.dcom/icon.png#SHA256HASH` or `/icon.png#SHA256HASH`, but not `../icon.png#SHA256HASH`).
-`appIdentifiers` (optional) - For native applications, an array of whitelisted app identifiers (e.g., bundled identifiers for iOS apps or package names for Android apps).
-`description` (optional) - A paragraph about your application. May be displayed in app listings, etc.
-`sslfingerprint` (optional) - Your app domain's SSL SHA-256 fingerprint as a hex string. If present, the user agent may check that the SSL fingerprint of the domain submitting the transaction matches that in the provided manifest.
-`chains` - An array containing the `chainID`, `chainName` and `icon` for any ARISEN chain for which your application's smart contract is located on and therefore, where your application plans to require signing. User agents (like an authenticator) will use this for presenting a friendly chain name and icon to the user when prompted for signing.

An `on-chain manifest` must then be registered using the [arisen.assert::add.manifest](https://github.com/ARISENIO/arisen.assert) action on every ARISEN chain where smart contracts for an app exist and on which an app will transact. Furthermore, an array of `chain manifests` must be declared in a publicly available JSON file at the root of the application's `declaredDomain`. See [Authentication Transport](#authentication-transport-protocol) for the `declaredDomain` parameter, which must match the domain referenced in the `off-chain manifest`.

The `chain-manifests.json` file has the following fields:
-`account` - The chain account name. This is the account that published this `on-chain manifest`.
-`domain` - The uri or bundle identifier.
-`appmeta` - The DWEB or HTTP address of the application's metadata JSON file and its hash. Must be an absolute path.
-`whitelist` - An array containing the `contract` and `action` for each allowed contract action(s) that your app can propose.
--The `contract` and/or `action` fields within the `whitelist` on the manifest may contain a wildcard. Wildcards are denoted by " " (empty string).

For a more detailed description of Chain Manifests, please read the full specification [here](https://github.com/ARISENIO/manifest-spec).

###### Ricardian Specification
The Ricardian Specification gives a set of the human-readable requirements (that a smart contract should follow in order to be considered valid) and easily consumable metadata about the actions a contract contains (action title, description and a URL of an icon that describes an action; for example, a pencil icon for the `post` action described earlier in this section). Ricardian Metadata is written in HTML (in .html file(s) separate from the contract code itself) and combined with the contract code in a WASM compiled output format by ARISEN's smart contract compiler. Once deployed to an ARISEN blockchain, it can easily be viewed in an ABI (application binary interface) format (similar to JSON).

For a deeper drive into ARISEN's Ricardian Specification, please refer to the latest version of the spec [here](https://github.com/ARISENIO/ricardian-spec).

###### Authentication Transport Protocol
ARISEN Authentication Transport Protocol (AATP) provides a protocol that ARISEN-based authenticators, like [dWebID](#dwebid), can utilize to respond to requests from dWeb-based apps, with consistent handling logic in a request-response lifecycle. It aims to improve the user's overall experience and security when authenticating and signing actions that derive from dWeb-based websites and apps.

Below is an explanation of how an authenticator like [dWebID](#dwebid) uses AATP, Chain Manifests and Ricardian-compliant contracts in the process of authenticating a user.

###### `Request Transports`
A dWeb application can use either a dWebID Link, dWeb Link, Apple Universal Link or a Deep Link to invoke an authenticator application, including the transaction payload in the query string as a hex-encoded value, and the recipient public key, if the payload is encrypted.

-`dwebid://request?payload={hexPayload}&Key={publicKey}`
-`{customProtocol}://request?payload={hexPayload}&Key={publicKey}`
-`dweb://{siteUrl}/auth?payload={hexPayload}&Key={publicKey}`
-`https://{siteUrl}/auth?payload={hexPayload}&Key={publicKey}`

###### `Response Transports`
Authenticator applications will return a response to the request's `returnUrl` with the payload appended as a hex-encoded URL hash fragment identifier. If the payload is encrypted, the public key is provided at the end, preceded by `-`.

-`dweb://{siteUrl}/some-resource/resource-id#{hexPayload}-{publicKey}`
-`https://{siteUrl}/some-resource/resource-id#{hexPayload}-{publicKey}`
-`{customProtocol}://transaction-response#{hexPayload}-{publicKey}`

###### `Request Envelope`
The top-level properties of each request payload make up the `request envelope`. Envelopes may contain several keys:

-`version` (required) - The protocol version.

-`id` (required) - The unique UUIDv4 of the request, which insures that the requesting application is able to connect a response to the initial request.

-`declaredDomain` (required) - Integrating applications (both web and native) must self-report a `declaredDomain`. Authenticator applications should not blindly trust this URL.

-`returnURL` (required) - The URL to which the authenticator application will return the user after the request has been processed and the user had taken any necessary action.

-`responseKey` (optional) - An elliptic curve 65 byte public key. If provided, the response will be encrypted with this key using the agreed upon algorithm.

-`request` (required) - The request payload, consisting of one or more request types.

###### `Request/Response Types & Authenticator Behavior`
There are four different types of request, each of which have different payloads. Those requests, the behavior of an authenticator app and each response type are explained below:

-`Transport Authorization Response` - This request must include the preferred response transport (e.g., dWeb Link, Custom Protocol, etc.) as well as an array of contracts and their associated actions involved in the request.

This request type carries out two functions:
--1. It negotiates and establishes communication with an authenticator application over one or more transports.
--2. It requests user authorization for the transaction actions that may be requested through each transport. The idea here is that integrating applications or users may restrict which actions are authorized over less secure transports.

**This is an example Request/Response lifecycle for a `Transport Authorization Request`:**
-1. The request must include a prioritized list of app-supported response transports and a list of requested action permissions for each response transport, which must be white-listed in the `on-chain manifest`. The request may be sent with one or more `Selective Disclosure` or `Authentication` request types.
-2. Upon the authenticator app receiving the request, it will prompt a user for approval if this is the first time encountering this `Transport Authorization Request` for the given `chain manifest`. The authenticator may respond automatically if the user has previously approved an identical `Transport Authorization Request` for the given `chain manifest`.
-3. A `Transport Authorization Response` sent from the authenticator app back to an application must include a prioritized list of vault-supported request transports and vault-supported response transports, and MAY be sent through one of the app-supported response transports.

-`Authentication Request` - This request type allows an integrated dWeb-based application to request proof of a user's possession of one or more private keys corresponding to any public keys that they have disclosed. This enabled passwordless authentication flows so that integrating applications can display private data to the authenticated user.

-`Selective Disclosure Request` - Allows an app to request private user data (e.g., availableKeys, authorizers).

**This is an example Request/Response lifecycle for a `Selective Disclosure Request`:**
-1. The request must include one or more requested attributes (e.g., availableKeys).
-2. The authenticator app must prompt the user to approve any disclosures they have not previously approved for an identical `Selective Disclosure Request`. It may respond automatically if the user has previously approved an identical `Selective Disclosure Request` for the `chainmanifest` `scope`.

-`Transport Request` - Allows a dWeb-based application to request a user signature for a transaction. In this case, the authenticator application must:
-1. REJECT the transaction request automatically if the transaction contains any actions not whitelisted in the app's `chainmanifest` for a given ARISEN chain.
-2. REJECT the transaction request automatically if the transaction contains any actions that have not been allowed by a previous `Transport Authorization Request` for the given transport.
-3. PROMPT the user for permission to sign if the transaction contains any actions that have not been allowed by a previous Action Permission Request, without autosign privileges. This prompt may be ignored and a `Transaction Request` may be approved automatically if the transaction contains only actions that have been allowed by a previous `Action Permission Request` with autosign privileges.

##### The ARISEN Authentication Process
Below is a description of the simplified authentication process for dWeb-based apps that utilize an AATP-ready authenticator:

-1. Bob opens the post app again at `dweb://postit.dcom` in his mobile Safari browser.
-2. Bob downloads dWebID, an AATP-ready authenticator for iOS.
-3. The PostIt app asks Bob to create an account. He chooses a username and his `owner` and `active` keys are automatically imported into [dWebID](#dwebid) using its builtin import API, and both keys are securely stored on the device's `Secure Enclave` chip.
-4. Bob sees a "Login" button and clicks it. Upon clicking the login button, a `Transport Request`, along with a `Selective Disclosure Request` is sent to the dWebID mobile application using the `dwebid://` protocol.
-5. dWebID opens and performs the following checks immediately in the background:
--assert that the `referrerUrl` and `reuturnUrl` are all paths of the `declaredDomain`.
--fetches the `chain-manifests.json` file from the root of the `declaredDomain`.
--asserts that the values for `domain` declared therein all match one another and the `declaredDomain`.
--fetches the `app-metadata.json` from the `appMeta` URL in the `chain-manifests.json` file and asserts that the file's hash matches the hash declared in both manifests.
--asserts that the file's hash matches the hash declared in the `on-chain manifest`.
-6.  If all checks pass, dWebID shows a screen that takes on the following format:

```
Allow <appnames> to log in
using dWebID?

<app-icon>

Domain Requesting:
<app-url> (request URL)

<Deny Button> <Allow Button>
```
The above data is retrieved from the `chain-manifests.json` and `app-metadata.json` chain manifest files in the root of the app's dDrive.

-7. dWebID asks Bob which account he would like to disclose with PostIt. Bob selects the @bob account and dWebID sends a `Selective Disclosure Request` with the public key for @bob back to PostIt.
-8. Bob is returned back to the PostIt app in Safari, where he now sees a "Post" button.
-9. Bob clicks "Post," which brings up a popup where he can type a message.
-10. Upon clicking "send," the `post` action related to PostIt app's smart contract on ARISEN is packaged in a `Transaction Request` and sent to dWebID.
-11. The dWebID is opened automatically and a `Transaction Approval Request` screen is shown that takes on the following format:
```
<app icon> <app domain>
                 <app url>
--
<action item> <action name>
<action description>
<action contract>

Signing as <username> on <chain name>
--
<Cancel Button> <Approve Button>
```
The above app icon, app domain and app url derive from the website's `chain-manifests.json` and the `app-manifests.json` file with the app's dDrive, while the action icon, action name, action description and action contract clause derive from the Ricardian metadata stored with the app's smart contract on a specified ARISEN chain.

-12. Bob clicks "Allow" and is prompted for biometric confirmation (face or fingerprint).
-13. Once Bob is biometrically authenticated, the transaction is signed with the keys from the Secure Enclave.
-14. The transaction and signatures will be returned to the requesting app, which may, in turn, broadcast the transaction to the chain (NOTE: dWebID broadcasts by default).
-15. Bob is returned to the PostIt app, which can now retrieve Bob's post remotely from the `postit` database on ARISEN and displays the posted message to Bob.

In the future, any time Bob accesses the PostIt app and initiates the `post` action, Bob is only asked to verify himself biometrically. Bob's entire experience with PostIt is now passwordless and extremely secure.

Keep in mind, Bob probably has no idea he's using a blockchain, nor does he ever see his cryptographic keys. All he ever sees are user-friendly, human-readable prompts and is only prompted twice. Once when he first logs into the app and second, the first time he initiates an action. He is only prompted again when an action is initiated and a `Transaction Request` is sent and he needs to verify his biometrics. ARISEN's Universal Authentication Layer allows dWeb-based apps to simplify their authentication process with ARISEN and allows everyday users of the Internet to experience the power of decentralized apps for the first time.

##### dWebID
At PeepsLabs, we're developing the dWeb's first authenticator using AATP which can be used as a means for decentralized and universal authentication on the dWeb.

dWebID has the following features:
-Enables seamless, multi-network support. In fact, the app itself does not even communicate with chains or nodes directly.
-Securely stores private keys and employs the use of biometrics to sign transactions.
-Displays richly formatted Ricardian Contracts, which provide users with a human-readable explanation of the actions the app is proposing and allows users to approve or reject the terms of the contract(s).
-By following the `Manifest Specification`, it displays metadata about the requesting application to end users any time they are prompted to trust an app or sign a transaction from one. This provides users with an improved sense of trust in the requesting application and the signing ceremony itself. It also runs pre-flight security checks, comparing the contents of a transaction request with what integrating apps have declared about themselves.

#### Smart Contracts & RSN VM
In previous sections, I described out how Smart Contracts power the entire ARISEN computer, but it is the computer itself I have yet to explain. Blockchain-based programs must be compiled, validated and computed; and are entirely dependent upon single-threaded performance, fast compilation and validation of assembly code, as well as the low-overhead calls to native bytecode. ARISEN contracts are for the most part, written in C++, a low-level programming language also known as a systems programming language, whose most pervasive uses are deep in the infrastructure [STRO13]. The language's focus on static types and compile-time type checking help give way to the secure execution of programs (contracts) and help to eliminate the threat of stack overflow attacks, discussed subsequently. ARISEN ships with its own compilers, like [ARISEN CDT](https://github.com/arisenio/arisen.cdt), which compile C++-based contracts into the WebAssembly (WASM) assembly language, a programming language that is one step away from machine language (bytecode), allowing each instruction to be translated into one machine instruction by an assembler [STALL16].

RSN VM, a fork of EOS VM, is a high performance WASM engine with predictable compile times, where contracts do no have to be compiled every time the process restarts. Unlike more popular WASM engines, RSN VM is designed from the ground up for the rigorous demands of blockchain applications, which require far more from a WASM engine than those that were designed for web browsers or standards development.

When it comes to smart contracts, it's a blockchain's lifeline to insure that any non-deterministic behavior, unbounded computations or unbounded use of RAM be prevented, as it can bring down an entire blockchain in seconds. As discussed in [Turing Completeness](#turing-completeness) and [Distributed Computing Resources](#distributed-computing-resources), RSN VM uses a builtin metering mechanism to avoid some of these issues, but at even lower-levels, other issues remain.

The RSN VM has no physical existence. It is a distributed and virtual computation engine, and is not hugely dissimilar to the virtual machines of Microsoft's .NET framework, or interpreters of other bytecode-compiled programming languages such as Java. RSN VM, at a basic level, is in charge of both smart contract deployment and smart contract execution. At a high-level, it is essentially a global, decentralized computer, containing millions of executable objects, each of which utilizes its own data store. Furthermore, RSN VM, in one way or another, is a Turing-complete state machine because every single execution process is limited to a certain amount of computations and completely dependent on the CPU, NET and RAM that's available to the user who is initiating the execution.

RSN VM was designed with the goal of creating a highly deterministic and secure environment for the parallel execution of contracts. Given what derives from the execution of a contract must be deterministic - and the non-deterministic nature of denomals, NaNs and rounding modes as it relates to floating-point arithmetic, in addition to the underlying physical computer's ALU - RSN VM relies on the `softfloat` implementation of IEEE-754 float-point arithmetic, which is even further constrained to insure determinism. To add to that, secondary limits and constraints such as stack size and call depth can cause consensus failures if they differ from a previous backend, which is solved through RSN VM's user-definable constraints at either compiler-time or run-time. User-definable constraints are defined based on use-case and the data-type involved.

While deterministic execution is crucial, time-bounded execution is just as important when it comes to a blockchain-based computer, in order to insure that the deterministic execution of a contract doesn't "over run" the CPU time that is allotted for a given contract. Since blockchains are limited in resources, an `instruction counter`, at a very minimum, is needed to insure the time-bounded execution of a contract. RSN VM allows users to use a simple instruction counter for counter-based time-bounding in a single-threaded environment, but can be avoided by using a `watchdog timer` that doesn't introduce any performance overhead like the latter option.

Even more important is the secure execution of contracts. RSN VM was designed to avoid unbounded memory allocation, extremely long load times, and stack overflows deriving from a syntax analysis (like recursive descent parsing or execution), thanks to a type system that was prebuilt from the onset to insure RSN VM's foundational data types were invariant (exact type matching, where `T = T`). This insures developers don't have to worry about explicit type checks and validations since RSN VM's underlying type system insures that each data type maintains these invariants and kills the execution of a contract that violates this integrated invariance. Given the problems that can occur in C++ with unsafe arrays and pointer references, there have been a number of proposals to augment compilers to automatically insert range checks on such references [STALL18].

RSN VM has special purpose allocators that utilize the security of the physical CPU and the underlying operating system that RSN VM exists upon, to insure that a contract is not able to store data beyond the limits of a fixed-size buffer or attempt to overwrite adjacent memory locations that may hold other variables, parameters or control flow data from the contract itself, which could contain return addresses and pointers to previous stack frames. Buffer overflow attacks are one of the most dangerous and popular forms of security attacks and a guard paging mechanism, via the CoreOS, is used so that memory is properly sandboxed.

There is never a point where, during either the parsing or evaluation phases, RSN VM uses unbounded recursion or loops. RSN VM is constrained to limit or eliminate the ability for a bad or corrupt contract to cause a crash or infinitely hang the machine. It's custom allocators and memory management facilities allow for different access patterns and allocation requirements. RSN VM's allocators are used to back the core data types (fast vector, WASM stack, fast variant and WASM module) and, as such, do not "own" the memory that they use for operations. This gives way to maximizing the performance of the interpreter implementation.

These non-owning data structures allow for the ability to use the memory cleanly, while not having to concern the data type with destructing when going out of scope, which creates a performance increase for portions of RSN VM, without loss of generality for the developer. Since the data is held by these allocators and has lifetimes that match that of a WASM module, no copies of these heavy weight data types are ever needed. Once an element in an RSN VM is constructed, that is its home and final resting place for the lifetime of the WASM module.

##### Contract Storage & On-Chain Databases
As you may recall, a smart contract itself (`arisen.system`) and its `set` action are used to deploy other contracts to the network and subsequently store the WASM-derived bytecode in an on-chain database associated with the uploading user. The contract itself has its own database associated with it, where the data that derives from the execution of its actions is stored. Although, each account on ARISEN has its own private on-chain database associated with it, for which contracts can optionally choose to store action-derived data via a contract-specific scope.

On-chain databases associated with accounts can only be accessed by its own action handlers. Action handlers are scripts that send actions form one account to another. ARISEN is able to define smart contracts through the combination of action handlers and automated action handlers. Each account can ultimately send structured actions to other accounts and may define scripts to handle actions when they're received. To support parallel execution, each ARISEN account can also define any number of scopes within their database. The block producer will schedule transactions in such a way that there is no conflict over memory access to scope and therefore can be executed in parallel.

As an example, dWeb's [dDNS](#ddns) contract stores domain records within a scope named `ddnsrecords` within each domain's on-chain private database (considering domains in the context of ARISEN are actually accounts), so that records serve as a unique, searchable index for each domain.

##### Deterministic Parallel Execution of Appliances [BLOC18]
RSN VM has no scheduling capabilities, because execution ordering is handled externally by ARISEN clients. ARISEN has to execute transactions (transactions derive from the execution of an action), considering its consensus algorithm is dependent upon deterministic (reproducible) behavior, which means all parallel execution must remain without mutexes or locking primitives. When transactions are executed in parallel, it's important that these transactions don't create non-deterministic results, considering the absence of locks.

When ARISEN's mainnet begins producing in parallel, block producers will organize action delivery into individual `shards` in order to evaluate transactions in parallel. The scheduling of transactions will be deterministically executed according to the output of a block producer, although protocol for the generation of transaction scheduling will not be deterministic. Because of this, block producers can take advantage of parallel algorithms to schedule transactions.

With parallel execution, it's important to note that when a new action is generated by a "script" or "contract," it does not get delivered instantly; rather, it's scheduled to be delivered in the next cycle because the receiver may be actively modifying its own state within a completely different shard.

##### Mining Communication Latency [BLOC18]
Latency is the time it takes for one account to send an action to another account, and then receive a response. The goal is to enable two accounts to exchange actions back and forth within a single block, without having to wait 0.5 second between each action (ARISEN's block production turnaround time). To enable this, ARISEN divides each block into cycles. Each cycle is divided into shards and each shard contains a list of transactions. Each transaction contains a set of actions to be delivered.

Below is a pseudo-representation of a block's layers and how each layer is processed:
```
Block
    Region
        Cycles (sequential)
            Shards (parallel)
                Transactions (sequential)
                    Receiver and Notified Accounts (parallel)
```

Transactions generated in one cycle can be divided in any subsequent cycle or block. Block producers will keep adding cycles to a block until the maximum wall clock time has passed, or there are no new generated transactions to deliver.

It is possible to use static analysis of a block to verify that within a given cycle, no two shards contain transactions that modify the same account. So long as that invariant is maintained, a block can be processed by running all shards in parallel.

##### Read-Only Actions Handlers [BLOC18]
Some accounts may be able to process an action on a pass/fail basis without modifying their internal state. If this is the case, then these handlers can be executed in parallel, so long as the read-only action handlers for a particular ARISEN account are included in one or more shards within a particular cycle.

##### Atomic Transactions with Multiple Accounts [BLOC18]
Sometimes it is desirable that actions are delivered to and accepted by multiple accounts atomically. In this case, both actions are placed in one transaction and both accounts will be assigned the same shard and the actions applied sequentially.

##### Partial Evaluation of Blockchain State [BLOC18]
Scaling blockchain technology necessitates that components are modular. Everyone should not have to run everything, especially if they only need a subnet of the contracts. A social networking application developer runs full nodes for the purpose of displaying the entire state of its application to users. This social networking application has no need for the state associated with, for instance, a ride-sharing application's contract(s). ARISEN's software allows any full node to pick any subset of applications to run. Actions delivered to other applications are safely ignored if an application never depends upon the state related to another contract.

##### Subjective Best Effort Scheduling [BLOC18]
ARISEN cannot obligate or force a block producer to deliver any action to any other ARISEN account. Each block producer makes their own subjective measurement of the computational complexity and time required to process a transaction. This applies whether a transaction is generated by a user or automatically by a smart contract.

At the network level, all transactions are billed a computational bandwidth cost based on the number of WASM instructions executed. However, each block producer may calculate resource usage using their own algorithm and measurements by adjusting RSN VM's compile-time and run-time constraints. When a block producer concludes that a transaction or account has consumed a disproportionate amount of computation capacity, they simply reject the transaction when producing their own block; however, they will still process the transaction if other block producers consider it valid.

In general, so long as even one block producer considers a transaction as valid and under the resource usage limits, then all other block producers will also accept it, but it may take up to one minute for the transaction to find that producer. In some cases, a producer may create a block that includes transactions that are an order of magnitude outside of acceptable ranges. In this case, the next block producer may opt to reject the block and the tie will be broken by the third producer. This is no different than what would happen if a large block caused network propagation delays. The community would notice a pattern of abuse and eventually remove votes from the rogue producer.

This subjective evaluation of computation cost frees the blockchain from having to precisely and deterministically measure how long something takes to run. With this design, there is no need to count instructions at the VM-level, which drastically increases opportunities for optimization without breaking consensus.

##### Deferred Transactions [BLOC18]
ARISEN supports deferred transactions that are scheduled to execute in the future. This enables computation to move to different shards and/or the creation of long running processes that continuously schedule a continuance transaction.

##### Context Free Actions [BLOC18]
A Context Free Action involves computations that depend only on transaction data, but not upon the blockchain state. Signature Verification, for instance, is a computation that requires only the transaction date and a signature to determine the public key that singed the transaction. This is one of the most expensive individual computations a blockchain must perform, but because this computation is context free, it can be performed in parallel.

Context Free Actions are like other user actions, except they lack access to the blockchain state to perform validation. Not only does this enable ARISEN to process all Context Free Actions, such as signature verification, in parallel, but more importantly, this enables generated signature verification.

With support for Context Free Actions, scalability techniques such as Sharing, Raiden, Plasma, State Channels and others become much more parallelizable and practical. This development enables efficient inter-blockchain communication, as well as unlimited scalability for on-chain activity.

##### Schema-Derived Actions [BLOC18]
All actions sent between ARISEN accounts are defined by a schema that is part of the blockchain consensus state. This schema allows seamless conversion between binary and JSON representation of the actions held within a smart contract.

##### Schema-Derived On-Chain Database [BLOC18]
Database state is also defined using a similar schema. This insures that all data stored by all contracts/accounts, is in a format that can be interpreted as human-readable JSON, but stored and manipulated with the efficiency of binary.

##### Separating Authentication From Application [BLOC18]
ARISEN segregates validation logic into three separate segments to maximize parallelization opportunities and minimize the computation debt associated with reengineering application state from the transaction log:
-Validating that an action is internally consistent;
-Validating that all preconditions are valid; and
-Modifying the application state.

Validating the internal consistency of an action is read-only and requires no access to blockchain state. This means that it can be performed with maximum parallelism. Validating preconditions within a smart contract, such as required balance, is read-only and will certainly benefit from parallelism. Write access is only required when the modification of an application's state is taking place and therefore would be processed sequentially for each application in question.

Authentication is simply a read-only process, executed from within a smart contract in order to verify that a specific action can ultimately be applied. Under this model, websites and web applications are doing the work, not the contracts, or ARISEN for that matter. In real-time, the calculation of computation debt and the regeneration of application state from the transaction log are both required to be performed; although, once a transaction has been included in the blockchain, it is no longer necessary to perform the authentication operation(s).

##### VM & Language Agnostic
From a computer science perspective, ARISEN is responsible for coordinating the delivery of authenticated messages (called "actions") between ARISEN accounts. Truly, any programming language or virtual machine can be implemented to work with ARISEN's software, as these implementation specific details, for the most part, are independent from ARISEN's core design philosophy. Although, languages and virtual machines must be deterministic and properly sandboxed with sufficient performance if they are to be integrated with ARISEN's APIs.

#### Governance
ARISEN is ultimately designed to be a decentralized democracy, where the people are able to reach consensus concerning issues that involve the community at-large, as well as the safety and welfare of the community itself. The users of the dWeb are governed through a "governance" process concerning subjective matters that require collaborative action, the power to proceed forth with the decisions that are agreed upon by the governance themselves, and the vested power to create amendments to a network-wide Constitution that was ratified in order to protect the rights of the network's users.

Elected block producers that are instituted through the DPOS algorithm are also known and considered as elected Governance members. Essentially, Governance members must approve all changes to dWeb-based software that has been requested by the community itself. This model works because if Governance members go against the wishes of RIX holders, users of the network will most likely remove their votes from those specific Governance members who ultimately went against their wishes. As a safety value, non-producing full node validators (e.g., dApps that run their own nodes) have the power to reject changes that are made without the permission of RIX holders.

##### Elections
ARISEN's election system is very unique and coincides with ARISEN's DPOS algorithm.

**A few facts about ARISEN's election system:**
-Each ARISEN account can vote for up to 30 different block producers.
-The number of votes an account has directly correlates to the number of staked RIX in the account.
-A staked vote remains valid for an entire year.
-After one year, a staked vote decays to half a vote and so on.
-This is to maintain a need to validate the performance of block producers.
-If the RIX used for voting are unstaked, those votes are removed from the continual counting process since the RIX becomes transferrable liquidity.
-The counting process happens every 126 seconds so, in theory, block producers can be regularly interchanged with new candidates.
-Candidates who didn't quite make the block producer list act as backup block producers.
-Voting can be carried out via wallets like [dWallet](https://peepsx.com/dwallet).
-Voting relies on users creating a digital signature via a wallet or voting application that broadcasts their vote/signature to the ARISEN mainnet, providing cryptographic evidence of the authenticity of the vote, as well as the RIX ownership, as it is an interaction/transaction in the user's ARISEN account.

##### Freezing Accounts
The freezing of accounts on the EOS network was a heated debate amongst blockchain enthusiasts. In truth, every single blockchain is designed to allow block producers to pick and choose if specific transactions are included within a specific block. Therefore, block producers or miners on any blockchain have the ability to freeze accounts and reverse transactions simply through consensus. ARISEN formalizes this authority by making sure the process of freezing accounts has to be approved by a 15/21 vote amongst active governance members. ARISEN's blockchain expands upon EOS's vision, enabling people of the network to police for many illegal activities, that of which are within the [dWeb Constitution](https://github.com/arisenio/constitution).

You can read more about how the [dWeb Protocol](#DWEB) and its [Reporting System](#reporting-system) utilize this feature later in this paper.

##### Updating Contract Code
Contracts on ARISEN that act maliciously can shutdown the network entirely. ARISEN allows its governance to replace malicious code within contracts through a 15/21 vote of elected members. This enables ARISEN to proceed forth without a hard fork, which would otherwise be needed to remove the malicious contract.

##### dWeb Constitution
The rights of users on the dWeb are ratified in the [dWeb Constitution](https://github.com/arisenio/constitution) and are immutably available within ARISEN's state. While the Constitution is immutable, it can be amended with a 15/21 vote.

##### Process for Constitutional Amendments & Software Upgrades
The dWeb and ARISEN are designed around the idea that "the code is the law." For this reason, the protocol itself, as defined in the canonical source surrounding dWeb's on-chain and off-chain protocols and dWeb's Constitution, can only be updated through the following process:
-1. A change to the dWeb Constitution is proposed by governance members and approved by a 15/21 vote.
-2. The governance is able to maintain this approval count for 30 consecutive days.
-3. All accounts on ARISEN, when processing a transaction, would have to accept this new Constitution as a condition for future transactions.
-4. Governance members must adopt changes to the overall ARISEN software, as well as `off-chain` implementations, to reflect the change in the dWeb Constitution and must propose it to the network using a hash of the dWeb Constitution.
-5. Governance members maintain 15/21 approval of the new code for 30 consecutive days.
-6. Since the approval for the new code has maintained approval for 30 days, the code will take effect 7 days later, giving all non-producing full nodes exactly one week to upgrade after the new changes to various canonical implementations.
-7. All ARISEN nodes on the network who do not upgrade to the new ARISEN release will shutdown automatically.

The process for software upgrades (updating the blockchain) will take anywhere from 2 to 3 months, while updates to fix non-critical bugs that do not require changes to the constitution can take 1 to 2 months.

##### Emergency Changes
Governance members may accelerate the process of a software upgrade if it involves a harmful bug or security exploit that is actively harming users. Although this does go against the dWeb Constitution. it is generally up to the elected governance to make these emergency decisions.

##### Governance Rewards
Being that governance members are also block producers, they're awarded new RIX coins every time they produce a block on ARISEN. The number of RIX that are minted is determined by the median of the desired pay published by all governance members. ARISEN may be configured to enforce a cap on governance rewards to where the total annual increase in RIX's coin supply does not exceed 5%.

##### dWeb Improved Proposal (DWIP)
While RIX holders can elect governance members, they can also craft dWeb Improvement Proposals (DWIP) that can be voted on by community members, whom can elect a number of DWIPs in order to advance the ideals of the community. Winning DWIPs will receive a certain amount of RIX via ARISEN's `savings account`, which is completely funded via inflation. DWIPs will only receive RIX proportional to the amount of votes each DWIP has received from RIX holders.

#### Distributed Computing Resources
Running contracts requires server capacity and there need to be safeguards against spam generated by these contracts. The resources needed are defined as RAM, Bandwidth (NET) and CPU. Each are explained below:

##### RAM
RAM is required to write data to the blockchain database, which takes up system capacity (change of state). Every account must have RAM to be a useable account (a minimum of 5 KB). The price of RAM fluctuates on an open market. The price of RAM fluctuates automatically depending on supply and demand. Extra RAM can be purchased for accounts, and this RAM can be sold at whatever the current market price is, when it is no longer needed.

Developers who deploy contracts will require more RAM than the standard account because RAM comes under the scrutiny of the account carrying out the function (change of state). This means the RAM requirements of a contract are paid for by the developer of the account that deployed it, not the user interacting with it. RAM is not traditional silicon RAM; rather, it refers to storage in ARISEN's distributed memory database, which is all the data currently being processed by the blockchain's collective CPU.

##### CPU
Refers to computation (processing power). This is accessed by staking a minimum of 3.0 RIX (depending on network load). This can be defined as how long transactions/actions run for. CPU is measured by average consumption in microseconds over a 3 day period, decreasing to 0 over time. This is the amount of time a transaction runs for with network bandwidth; in other words, it is the size of the transaction.

##### NET
The average consumption in bytes over a 3 day period, decreasing to 0 over time. This can be referred to as bandwidth and log storage required when sending a transaction (the number of transactions/actions). This is accessed by staking a minimum of 0.1 RIX (depending on network load).

#### Cryptocurrencies & Decentralized Payments
Like other blockchains, at the very foundation of ARISEN is the ability for developers to create their own digital currencies, also known as "tokens." For this reason, ARISEN is considered a multi-asset network. P2P currencies give way to decentralized payments, which enable developers to add economic features within their decentralized applications. For example, a decentralized social network built on ARISEN could launch its own currency. This currency could be used to pay users for their posts, thereby providing them incentive to join the network.

##### Cross-Chain Transfer Protocol (CTTP)
RIX and other ARISEN currencies can be transferred to/from wholly unrelated blockchains like Ethereum, EOS and TRON, using the Cross-Chain Transfer Protocol (CTTP). You can read the full specification [here](https://github.com/arisenio/cttp-whitepaper).

#### Blockchain Interoperability
ARISEN was designed to coordinate and facilitate blockchain interoperability through the generation of both `action existence` and `action sequence`. This combo, teamed with ARISEN's contract architecture designed around `action passing`, enables high-level abstractions that are ultimately presented to developers by concealing all blockchain communications and proof validations.

#### LCV
EOS first presented the concept of Merkle Proofs for Light Client Validation (LCV) - with the idea that all clients don't need to process all transactions - in order to make the integration with other blockchains much easier. For example, a decentralized social network that operates on an ARISEN-based blockchain would only be concerned with the transactions that are taking place within the social network itself. This is supported by the idea that a blockchain's block producers would want the smallest possible overhead when syncing with another blockchain.

ARISEN uses LCV to prove the existence of any transaction with a proof of less than 1024 bytes in size (a valid proof on the Bitcoin network is about 512 bytes). By using LCV, ARISEN can prove that a block is included in another blockchain by simply utilizing a specific `blockID`, as well as the corresponding headers of a block, as long as the block is trusted and irreversible. This sort of proof takes `ceil(log 2(N))` digests for its path, where `N` is the number of blocks in a blockchain that contains 100 million blocks, in 864 bytes.

Therefore, tracking all block headers is cheap at 420 MB/year and will ultimately keep proof sizes small. Although, over time it makes more sense to have one blockchain that contains the entire histories of other blockchains to completely eliminate the need for proofs. By minimizing the frequency of inter-chain proofs, we can further boost the network's performance.

##### Latency
Thanks to ARISEN's consensus algorithm, ARISEN-based blockchains are able to provide rapid irreversibility. This means there is only 0.5 second latency between ARISEN blockchains that use the same consensus model. This is because when one blockchain communicates with another, block producers have to wait until a transaction's irreversibility has been confirmed by the blockchain.

##### Proof of Completeness (POC)
ARISEN uses Proof of Completeness to prove there are no gaps in the transaction history when verifying transactions from a remote blockchain rather than trying to attempt to prove that all transactions on the other blockchain are valid, since it's impossible to prove that all of the most recent transactions are known. By identifying each action sent to an account by sequencing these actions, an ARISEN user is able to prove that each action intended for a particular ARISEN account has ultimately been processed in order.

##### SegWit
ARISEN utilizes the concept of Segregated Witness (SegWit) to eliminate the need for storing the SHA256 hashes used by proofs to derive blockchain state. Once a Merkle Proof is accepted and deemed irreversible, the 2KB of hashes used for derivation are no longer stored. As it applies to blockchain interoperability, savings are 32x greater than using typical signatures.

## DWEB
The previous sections identified the foundational protocols that together, form the DWEB protocol suite. This section will serve as an explanation for how these protocols work together to enable the simple and secure exchange of data between a web of peers, which in turn forms a decentralized web for the peer-to-peer exchange of information.

The dWeb is made possible through the following processes and models which take place across different dWeb protocol and data layers:

-[dWeb Network Addresses](#dweb-network-addresses)
-[Address Registration](#address-registration)
-[Address Announcement](#address-announcement)
-[Address Lookup](#address-lookup)
-[Peer Messaging Structure](#peer-messaging structure)
-[dWeb Data Model](#dweb-data-model)
-[Peer Data Exchange](#peer-data-exchange)

The sub-sections that follow explain how all of the above areas work, including a review of the [dWeb Data Model](#dweb-data-model) (that was explained in the [dDatabase](#ddatabase) section), which together ultimately allow for peers to host and distribute datasets, websites and web applications amongst themselves without a single central point of failure.

**NOTE:** This section covers version 8 of the DWEB protocol suite, which now includes a NOISE-based handshake for protocol-level encryption.

### dWeb Network Addresses
As discussed in [dDatabase](#ddatabase), a dWeb network address is a 32-byte hexadecimal address that can represent a device, distributed dataset (e.g. a distributed key/value database, distributed file system, etc.) or simply a binary data feed (e.g. a plain dDatabase feed).

A dWeb network address is identified by it's protocol identifying prefix: `dweb://40a7f6b6147ae695bcbcff432f684c7bb5291ea339c22c1755896cc`

If a network address represents a distributed file system like a [dDrive](#ddrive), the address may include a suffix that identifies specific files and/or folders within a dDrive, like so:
`dweb://40a7f6b6147ae695bcbcff432f684c7bb5291ea339c22c1755896cc/index.html`

In the case of a dDrive, this suffix could also include its version number:
`/?version=1`; or if the version is tagged, `/?version=live`

The `dweb://` identifier makes dWeb URLs easily identifiable, so that dWeb-capable applications can register protocols with an underlying operating system, which enables `dweb://` links to be programmatically recognizable (e.g., [dBrowser](http://dbrowser).

It's important to note that a dWeb address suffix can be used by abstractions other than a dDrive to represent other data structures (other than files/folders) on behalf of a dWeb-capable application that is ultimately designed to digest a particular data structure via a dWeb network address suffix. For example, a message app could receive new messages using the following format:
`dweb://<key>/{messageID}/{messagePayload}`

### Address Registration
When announcing a dWeb network address on dWeb's DHT, if the address being announced does not exist, an address must be announced on ARISEN prior to it being announced on the DHT. This not only provides a bridge between dWeb's off-chain peer discovery network and on-chain computing infrastructure, but further empowers dWeb's [decentralized domain name system](#ddns) and [decentralized reporting system](#reporting-system).

This is made possible by the `dweb` contract on ARISEN and the `register` action. The `register` action accepts the following parameters:

| Parameter Name | Data Type | Description |
| --- | --- | --- |
| address | name | The dWeb network address |
| structure | name | The data structure type |
| status | uint8 | 0 for live and 1 for blacklisted |

The action does not require the address creator to authenticate with the contract since the contract self-authenticates with the contract creator; in this case, the `@dweb` account on ARISEN auto-authenticates every time an action is executed and pays all RAM, CPU and NET fees.

A dWeb network address can then be blacklisted via a 15/21 vote by governance members, who can then modify the entry by switching the status to `1`.

### Address Announcement
Once an address is registered, it can be announced via dWeb's DHT using the DWDHT protocol. An announcing peer, whether it is the creator of the dWeb network address or a peer joining an already existent address, provides the following announcement parameters to the DHT:
```
{
  key: <dweb-key>,
  ip address: <peer-public-ip>,
  port: <peer-port-number>,
  localAddress: {
    localIP: <peer-local-ip>
  }
}
```

To stay subscribed to a particular dWeb network address, a peer (creating or joining peer), must re-announce themselves every 60 seconds. The DHT server will also cycle its tokens periodically, so a client should remember the token it received last and update it when they receive a new one.

### Address Lookup
When a dWeb client (e.g., a web browser) performs a lookup, all it needs is a dWeb network address. A dWeb client queries dWeb's DHT for the address and the DHT will return the following response for any given dWeb network address:

{
  node: {host, port},
  // List of peers
  peers: [{host, port}, ...],
  // List of LAN peers
  localPeers: [{host, port}, ...]
}

### Peer Messaging Structure
Once a peer has discovered another peer's IP address and port number, it will open a TCP connection to the other peer, which allows peers to exchange data. This peer messaging protocol was briefly described in [dDatabase Protocol](#ddatabase-protocol), and handles a large portion of DWEB's data exchange process. All message structures adhere to the Protocol Buffers data serialization format.

Each half of a peer exchange has the following structure, which repeats until the end of the connection:

| Field | Description |
| --- | --- |
| Length | Number of bytes until the start of the next length field |
| Channel and type | A single number (up to 11 bits long) that encodes two sub-fields |

-The `channel` and `type` subfields are as follows:

| Field | Description |
| --- | --- |
| Channel | Peer can exchange multiple data feeds using the same TCP connection. The channel number is `0` for the first feed, `1` for the second and so on.
| Type | Number relating to the type of message being sent |

-The following message types can be used in the `type` field:

| Type | Name | Meaning |
| --- | --- | --- |
| 0 | Feed | I want to talk to you about a dWeb address |
| 1 | Handshake | I want to negotiate how we can communicate over TCP |
| 2 | Info | Whether I'm starting or stopping uploading or downloading |
| 3 | Have | I have some data that you said you wanted |
| 4 | Unhave | I no longer have the data I said I had |
| 5 | Want | This is the data I want |
| 6 | Unwant | I no longer want this data |
| 7 | Request | Please send me this data now |
| 8 | Cancel | Cancel this `Request |
| 9 | Data | Here is the data you requested |
| 10-14 | (unused) |
| 15 | Custom extension message, not a part of the core protocol |

#### Bit Notation
Throughout the underlying dDatabase protocol, multiple fields will be packed into a single number. Looking at this number as a sequence of bits makes the fields within the message far more visible. It is important to note that the most significant bit is always on the left (big endian).

Eight bits make up an entire byte, however this number and many others are `varints` which can be up to 64 bits long. The fields on the right are always a fixed number of bits, but the leftmost field can be as much as 64 bits.

#### Varints
The first two fields are encoded as `variable-length integers` and therefore do not have a fixed size. Each field must be read from the beginning in order to determine the length of the field and where the next field begins.

Varints have the abilities to represent small numbers with a few bytes and large numbers by using more bytes. The only disadvantage is the overhead introduced during the process to encode and decode integers.

##### Keepalive
A keepalive message is a BGP-4 Message used to acknowledge an open message, and to periodically confirm that the other peer is still a part of the TCP relationship.

In the case of the dDatabase protocol, it's an empty message containing no channel number, type or body. They are discarded upon being received by the remote. This can also be used by peers looking to live sync a dDatabase feed, which means they would have to keep a TCP connection open inevitably, which in turn would equate to a steady stream of `keepalive` messages. The actual rate is determined by the underlying TCP settings.

#### Body Structure
Within each `Body` field, is a series of field tags and values:

-`Field Tag ` - varint
-The most significant indicates which field within the message this is:
--`1` = dWeb network address
--`2` = nonce
--The least significant bits are the type of the field.

The two types of fields are:
| Varint | The field tag followed by a varint value. Used for simple numeric values & booleans. |
| Length-prefixed | The field tag followed by a varint to say how many bytes the field contains. |

##### Message Pseudo-Representation
A visual representation of DWEB protocol messages is as follows:
```
-Length
-Channel/Type
--Channel
--Message Type
-Message Body
```
As you will see in subsequent sections, the `Message Body` contains different fields and content, dependent on the `Message Type`.

### Peer Connection Initiation
When a peer discovers a remote peer, a TCP connection is opened with the remote peer and the following takes place in order to prepare the connection for the exchange of data between peers:

#### 1. Feed Message Sent To Remote Peer
After opening the TCP connection, the first message sent to the remote peer is of the `Feed` type (0). The `Feed` `Message Body` has two fields:

| Field No. | Name | Type | Description |
| 1 | dWeb network address | Length-prefixed | dWeb network address |
| 2 | Nonce | Length-prefixed | 24-byte random nonce generated for this TCP connection |

This allows the remote peer to know which dWeb network address address you're interested in.

#### 2. Peers Exchange Handshake Message
After the `Feed` message is sent, a `Handshake` message is sent on each side of the TCP connection.

The `Handshake` `Message Body` has the following fields:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | ID | Length-prefixed | Random ID generated by this peer upon starting; 32 bytes long; used to detect multiple connections to the same peer.
| 2 | Live | Varint | 0 = end connection when neither peer is downloading; 1 = keep connection open indefinitely. |
| 3 | User data | Length-prefixed | Arbitrary bytes that can be used by higher-level applications for any purpose. |
| 4 | Extensions | Length-prefixed | Name of any extension the peer wants to use. |
| 5 | Acknowledge | Varint | 0 = no need to acknowledge each fragment of data received; 1 = must acknowledge each fragment of data. |

-The `Extensions` field (#4) can appear multiple times, one for each extension. Both peers need to request an extension in their handshake messages for it to become active.

This same structure, using DWEBv8, can be used for a NOISE-based handshake so that all messages are encrypted and MAC'd using NOISE's `XX` pattern. In the process, both peers exchange a keypair for the purpose of stream authentication (message authentication code). This way, both peers can be assured that each is receiving a message from the other, since both will end up generating the same MAC (checksum) from the actual message contents using the exchanged private key. For example, PeerA would encrypt the message using the secret as follows:

`MAC = E(K,M)` (where `K` is the key, `M` is the message and `E` is the encryption function)

The MAC is attached to the end of the received message (as a checksum) and sent to PeerB. PeerB, using the exchanged secret key, can perform `E`, derive a MAC checksum from the received message, and compare to the received MAC checksum. This confirms that the message derived from PeerA.

### dWeb Data Model
Technically, peers can exchange any kind of data. A [dDatabase](#ddatabase) is the first standardized distributed dataset for the dWeb and is used by abstractions like [dDrive](#ddrive) and [dWebTrie](#dwebtrie). Other custom abstractions of a dDatabase feed can be created, but at the end of the day, it's still a dDatabase, which means it's a bunch of entries that each equate to an abstract blob of data.

The dWeb protocol can work with any dDatabase-alternative as long as it contains a list (log or feed) of variable-sized fragments (entries) of bytes. A dDatabase is an immutable, cryptographically secure, append-only log, which means new fragments can be added to the end by dDatabase's author, but existing fragments can't be deleted or modified.

Therefore, the following data mode is used in the exchange of data between peers:

-Each fragment (index or entry) in the feed is separated by boundaries so that a feed can be sent over the course of several protocol messages.
-Each fragment has a corresponding hash to verify the integrity of the data.
-There are also parent hashes which verify the integrity of two other hashes. Parent hashes form a tree structure known as a Merkle tree.
-Fragment hashes are even-numbered and parent hashes are odd-numbered.
-Hash trees eliminate the need for downloaders to verify a specific fragment without having to download the entire Merkle tree.
-Each time the dDatabase creator appends data (adds a new fragment), a new root hash is calculated and signed with the creator's private key.
-A remote peer who is downloading an entire feed or a specific fragment can use the creator's public key to verify the signature, which, as a result, verifies the integrity of other fragments and hashes.

#### Hashes And Signatures
The following types of hashes are found within a dDatabase's Merkle tree:
-`Fragment hashes` - Hashes the contents of a single fragment.
-`Parent hashes` - Hash of two fragments, forming a tree structure.
-`Root hashes` - Hash of all other parent hashes, signed by the dDatabase creator using the dDatabase's private key.

### Peer Data Exchange
The peer exchange process is actually quite simple to understand when you view one peer as a downloader and the other as an uploader, who are negotiating what fragments they `Want` and `Have`. In some cases, one peer is downloading and the other is uploading. In other cases, both peers are uploading and downloading at the same time as they both only have a portion of the feed (certain fragments). This is because neither peer, in the process of exchanging that feed, are the creators of the feed or the possessors of the feed. Every connection differs from the next, which is why peers must tell each other what they want and what they have.

#### Identifying Data
Each peer in the data exchange process remembers which fragments the other wants or has. The `Want` message is used by a downloader to inform the other peer of the fragment range (index range) they want to download and, as a result, want the other peer to inform them of whether or not they have it. The `Have` message is used by an uploader to indicate to the downloader that they do indeed have what the downloader seeks and will send if asked to.

As peers download or delete data, the list of fragments that have and want will change. This state is communicated to the other peer using four message types:
-`want`
-`unwant`
-`have`
-`unhave`
(see [Peer Message Structure](#peer-messaging-structure))

Each of these message types consist of the following structure in the their `Message Body`:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Start | Varint | Number of the first fragment you want/unwant/have/unhave. |
| 2 | Length | Varint | 1 = Just the start fragment; 2 = The start fragment and the next one and so on. |

If the `Length` field isn't included, this signals to the other peer that the entire feed is wanted, including new fragments, as they're appended.

##### Bitfield Field
If an uploader has many small, non-contiguous ranges of data, it can take a large amount of `Have` messages to communicate what the uploader actually has with the downloader.

A new `Have` message can be used that represents contiguous and non-contiguous ranges of data quite efficiently, using the following `Message Body` structure:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Start | Varint | ... |
| 2 | Length | Varint | ... |
| 3 | Bitfield | Length-prefixed | A sequence of contiguous and non-contiguous fragment ranges. |

Fragments can be divided into ranges, where each range is either contiguous (all fragments are there or none are there) or non-contiguous (some fragments are present). Each fragment range must be 8 fragments long. Range types, in some cases, alternate but it's possible for contiguous ranges to be included in a series of ranges within feeds where all fragments are present.

###### Range Encoding Structure
These ranges are encoded as follows within the `BitField` field of the `Have` `Message Body`:

| No. | Name | Description |
| 1 | Contiguous | A single varint containing sub-fields that represents how many 8-fragment sections there are and whether all are present. |
| 2 | Non-Contiguous | Varint saying how many 8-fragment sections the bitfield represents, followed by bitfield. |

The first byte of the bitfield represents the first 8-fragment section. The most significant bit of each byte represents the first fragment of each 8-fragment section.

#### Requesting Data
Once an uploader has signaled that they have the data you want, you can send a `Request` message to ask them for it. A `Request` `Message Body` has the following structure:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Index | Varint | Number of the fragment to send back. |
| 2 | Bytes | Varint | If included, ignore the `Index` field and send the fragment containing this byte. |
| 3 | Hash | Varint | 0 = Send fragments and hashes; 1 = Just send fragments, no hashes. |
| 4 | Nodes | Varint | 0 = Send back all hashes to verify this fragment; 1 = Just send fragments, no hashes. |

**NOTE:** A `Request` can only be used to request a single fragment.

##### Cancelling A Request
If a fragment is no longer needed, a `Cancel` message can be sent. The `Cancel` `Message Body` has the following structure:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Index | Varint | Number of a fragment to cancel. |
| 2 | Bytes | Varint | Ignore the `Index` field and cancel the fragment request that contains this byte. |
| 3 | Hash | Varint | Set to the same value as the hash field of the request you want to cancel. |

#### Sending Data
When an uploader receives a `Request`, they can fulfill the `Request` by sending a `Data` message to the downloader. The `Data` `Message Body` has the following structure:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Index | Varint | Fragment number. |
| 2 | Value | Length-prefixed | Contents of fragment. |
| 3 | Nodes | Length-prefixed | This field is repeated for each hash the requestor needs. |

The `Nodes` field uses the following recursive structure:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Index | Varint | Hash number. |
| 2 | Hash | Length-prefixed | 32-byte fragment hash of parent hash. |
| 3 | Size | Varint | Total length of data in fragments that hash covers. |
| 4 | Signature | Length-prefixed | 64-byte ed25519 signature of roothash corresponding to fragment hash. |

### An Example DWEB Communication
-PeerA looks up dWeb network address on the DHT and finds that PeerB is swarming.
-PeerA pulls PeerB's address and port number and initiates a TCP connection.
-PeerA sends a `Feed` message to PeerB as follows:
```
-Length (in bytes)
-Channel/Type:
--Channel: 0
--Type: 0
-Message body:
--dWeb network address (Field1)
--nonce (Field2)
```
-PeerA then sends a `Handshake` message to PeerB as follows:
```
-Length
-Channel/Type:
--Channel: 0
--Type: 1
-Message Body:
--ID: 0156721
--Live: 1 (keep connection open indefinitely for live replication)
--User data: <noise details>
--Extensions: noise
--Acknowledge: 1 (must acknowledge each fragment received)
```
-PeerB then sends back a `Handshake` message to complete the handshake:
```
-Length
-C/T:
--Channel: 0
--Type: 1
-Message Body:
--ID: 0156721
--Live: 1
--User Data: <noise details>
--Extensions: noise
--Acknowledge: 1
```
-PeerA sends PeerB a `Want` message, indicating it wants all fragments:
```
-Length
-C/T:
--Channel: 0
--Type: 5 (Want)
-Message Body:
--Start: 0
```
**NOTE:** The `Length` field isn't included in the `Message Body` to indicate PeerA wants the entire feed.
-PeerB then sends a `Have` message to `PeerA`, indicating they have the data:
```
-Length
-C/T:
--Channel: 0
--Type: 3 (Have)
-Message Body:
--Start: 0
--Length: <feed length>
```
**NOTE:** This indicates that PeerB has the entire feed (fragments 0 and 1).
-PeerA then sends a series of `Request` messages to PeerB for each fragment:
Request for fragment *0*:
```
-Length
-C/T:
--Channel: 0
--Type: 7 (Request)
-Message Body:
--Index: 0
--Nodes: 0 (send back all hashes to verify this fragment)
```
Request for fragment *1*:
```
-Length
-C/T:
--Channel: 0
--Type: 7 (Request)
-Message Body:
--Index: 1
--Nodes: 0
```
-Upon receiving the `Request` for fragment 0, PeerB immediately sends fragment 0 in a `Data` message:
```
-Length
C/T:
--Channel: 0
--Type: 9 (Data)
-Message Body:
--Index: 0
--Value: <contents of index in binary>
--Nodes:
----Index: <hash number of fragment 0>
----Hash: <32 byte hash for fragment>
----Size: <fragment data length>
----Signature: <signature of root hash>
```
-Upon receiving the `Request` for fragment 1, PeerB immediately sends fragment 1 in a `Data` message:
```
-Length
-C/T:
--Channel: 0
--Type: 9
-Message Body:
--Index: 1
--Value: <contents of index 1 in binary>
--Node 1:
----Index: <hash number of fragment 1>
----Hash: <32 byte hash for fragment 1>
----Size: <fragment data length>
----Signature: <signature of root hash>
--Node 2:
----Index: <hash number of parent 1>
----Hash: <32 byte hash for parent 1> (hash of fragment 0 and fragment 1)
----Size: <parent data length>
----Signature: <signature of root hash>
```

This is absolutely a giant pseudo-representation, considering all messages over the wire are in binary. The above examples aren't completely scientifically accurate, but should help build a mental model of how DWEB-based communications take place. It should be clear that multiple datasets can be exchanged with the same peer over the same TCP connection by using multiple channels. This is also one of the reasons a `Handshake` message utilizes a unique ID and the initial `Feed` message uses a randomly generated `nonce`.

### Protocol For A New Web & Beyond
While HTTP allows a client to request and receive data from a server, DWEB allows a peer to request and receive data from another peer or a set of peers. While one of the main use-cases for the dWeb is for the exchange of a dDrive that mostly contain websites and web applications, it can also be used to exchange distributed databases or even plain binary data. This means DWEB could be used to power a decentralized domain name system, a peer-to-peer phone system, live video streaming, as well as a multitude of other use-cases.

The most powerful aspect about the dWeb is that the requestor can cryptographically validate what the sender is sending. At the very same time, the requestor can request specific versions of specific fragments, which in the the case of a dDrive, equates to a requestor being able to view a specific version of a website or a web application. Since a dWeb network address is generated and wholly owned by a dataset's author and announced on a DHT that's distributed across potentially thousands or millions of computers globally - where the announced dataset is spread across the peers who consume it - it's safe to say that it would be impossible to take the dWeb offline. And the same goes for the data distributed across it, as long as the data maintains a group of peers in multiple locations.

Even though data exchanged over the dWeb protocol is not natively encrypted, overlaying protocols like [DWCIP](https://github.com/peepsx/dwcip-whitepaper) can be used to handle message encryption. NOISE can be used at the `Handshake` phase as well, which may make encryption seem native. When it's all said and done, websites and web applications are for more secure when distributed over DWEB compared to when they are distributed over HTTP. For hackers, a DDOS attack is useless because of how data is accessed and distributed. This is great news for website owners on the dWeb.

DWEB's [advantages](#advances-and-advantages) are clear when compared to the HTTP protocol, as well as the World Wide Web, for which HTTP happens to be the glue for. But it's DWEB's ability to grow beyond a World Wide Web alternative, in the process liberating many other forms of communication where freedom is under attack, that make DWEB such a viable solution to the attacks on freedom that are becoming rampant on the World Wide Web.

##dWeb Applications
While Electronic Mail and DNS are considered to be foundational Internet applications, dDNS and dReport are examples of what can be considered as foundational dWeb applications. The subsections that follow will layout the dWeb's decentralized domain name system and reporting system, both of which utilize dWeb's `on-chain` and `off-chain` protocols.

### dDNS
Since the dWeb and the DWEB's protocol suite are not dependent on centralized domain names and the Internet's tree-structured name space, and are not compatible with the Internet's "Domain Name System" (as defined in RFC 1034 and 1035), the dWeb needed its own domain name system that was compatible with dWeb network addresses. The dWeb would be pretty hard to use without domain names or a directory-like service for dWeb network addresses, considering that 32-byte addresses are difficult to remember.

This section explains the Decentralized Domain Name System (dDNS) and how human-readable names can be resolved to dWeb network addresses.

#### dDNS Overview
The Domain Name System is a directory lookup service that provides mapping between the names of a host on the Internet and a numerical address or a canonical name. DNS is essential to the functioning of the Internet. Like DNS, dDNS is a directory lookup service and provides a mapping between the name of a dDrive, device or peer and a dWeb network address. dDNS is essential to the functioning of the dWeb.

#### dDNS Operation Overview
-1. A user requests a dWeb network address for a given dTLD.
-2. A resolver queries the local `NameDrive` in the same location as the resolver (typically on a user's computer) to see if the record is being kept in a local database or cache; if so, returns the dWeb network address to the requestor.
-3. If the record cannot be found locally, the dWeb's root system is queried via ARISEN for the "authoritative NameDrive(s)" (ND record) for the domain name. An ND record, like other record types, points to a dWeb network address that usually resolves a dDrive where the domain's records are stored in various JSON files.
-4. The database within the NameDrive(s) is queried for the requested record. If found, the record is returned to the requestor and cached in the requestor's local NameDrive for the time specified in the TTL record field of the retriever `Resource Record` (RR).
-5. The user's dWeb client or web browser is given the dWeb key, or an error message (see [Error Messages](#ddns-error-messages), so it can begin the download process using [DWEB](#dweb).
**NOTE:** It's important to note that a NameDrive is simply a [dDrive](#ddrive) that contains files/folders that store various RRs using the [Resource Record Format](#resource-record-format) in various JSON files.

```
[User System]
         |
         X
         |
[dWeb Client]
         |
         X
         |
   [Local ND] <--> cache
         |
         X
         |
     DWEB <-->> [Root System] <--> [ND Record For Domain]
                                                                     |
                                                                     X
                                                                     |
                                                      [Domain's Name Drive]
                                                                     |
                                                                     X
                                                                     |
                                                        [Resource Record]
```

#### The Root Network
The dWeb utilizes [ARISEN](#arisen) and the [dDNS Contract](https://github.com/peepsx/ddns-contract) as its Root dDNS system. This means ARISEN is responsible for the management of the dWeb's domain name space and each domain's authoritative ND records. New dTLDs can be added to the root of the domain tree through dTLD auctions as previously explained in this paper.

##### Root Database Actions
The Root System's database is managed by the `ddns` contract on ARISEN, where its RR data structure is clearly defined. The contract defines the following actions as well:

###### *_add_*:
The `add` action allows a user to add an ND record for a domain. The user must provide the following parameters:
-`domain` - This is the domain the record is associated with; also used for authentication.
-`record_name` - The record name for the ND record (e.g., nd1.domain.dcom).
-`type` - See [Resource Record Types](#resource-record-types) (`ND` in this case).
-`class` - This autofills `DW` (dWeb); similar to `IN` with regular DNS.
-`TTL` - Typically when an RR is retrieved from a nameserver, or a NameDrive in this case, the retriever will cache the RR so that there is no need to query the ND or root network repeatedly. This fields specifies the time interval that the resource record can be cached before the ND or root network should be queried once more. Format should be in seconds.
-`rdata` - Similar to the `Rdata` field with regular DNS systems, only instead of 32 bit IPv4 addresses, this field stores a 64 bit dWeb network address.
**NOTE:** There is no need for `Rdata Field Length` since dWeb network addresses always have the same length in octets.

###### Updating a record
There isn't an update action. Instead, if an `add` action is executed using a `domain` and `rname` that already exists, the record will be modified with the new record data that is submitted as a part of the required parameters for `add` (ttl, rclass, rtype and rdata).

###### *_delete_*:
The `delete` action allow users to remove an ND record.

The user must provide the following parameters with the `delete` action:
-`domain` - Domain associated with the ND record; used for authentication.
-`record_name` - The ND record name being deleted.

Many tools are available for interacting with the root system and managing a domain's ND records:
-[ARISEN's Open RPC API](https://docs.arisen.network)
-[arisecli](https://arisen.network/arisecli)
-[dDNS CLI](https://github.com/peepsx/ddns-cli)

#### Decentralized Top-Level Domains (dTLDs)
As mentioned in previous sections, dTLDs can be put up for auction and won by anyone. While the winner has the sole power to determine who can sell the dTLDs they win, they cannot control the domain names associated with a dTLD. As an example, @peeps can sell ".dnet" domains since they won the ".dnet" dTLD, but they cannot control the domains they register for others, nor do they ever see the keys associated with the domains they act as registrars for.

This is because a domain name, regardless of the dTLD within the root name space it belongs to, is not different than a blockchain account. Unlike typical blockchain accounts, each domain operates around a permissions system where different ND records can each be managed by different permission-levels and their associated keypairs. Changes to or the creation of ND records associated with a domain name, must be signed with the private key associated with the permission level that is ultimately associated with a domain name's ND record. All permission levels of a domain fall under the domain's root permission level known as `owner`. Therefore, domain names are forever controlled by the user who possesses the `owner` keypair, which can reset the keypairs related to any permission level existing beneath it.

For example, a domain can have unlimited ND records, which means its RRs can be stored across many Name Drives, each of which contain a distributed database that stores the RRs, just like RRs can be stored across many name servers on a traditional domain name system. With this setup, one ND record could be under the control of one permission level and another ND could be under the control of an entirely different permission level. This further protects ND records from being altered or manipulated by hackers. As an added bonus, like accounts, domains can hold a multi-currency balance while also being able to send and receive payments.

### Name Drives
A "NameDrive" (ND) is the dWeb's version of a "Name Server" (NS) and is where a domain's "Resource Records" (RRs) are stored. This relieves the "Root Network" from having to manage every single RR for every domain on the dWeb, while also further decentralizing the domain name system amongst domain holders.

A NameDrive, as previously mentioned, is a dDrive where domains are root folders in the drive, and record types for the domain (D, CNAME, MINFO, ND, SRV and TXT) are sub-folders of a domain folder. Records are stored within individual files located within a record-type folder and are formatted using the Resource Record Markup Language which utilizes the `.rrml` file extension.

An example NameDrive would look something like this:
```
domainA.dcom/ (domain folder in root of the drive)
--/D/ (record-type folder; a sub-folder of a root domain folder)
----*.rrml (root wildcard record of the `D` record type)
----www.rrml ("www" record of the `D` record type)
--/CNAME/ <record-type folder of the `CNAME` type)
----test.rrml ("test" record of the `CNAME` record type)
domainB.dcom/
--/D/
----*.rrml
--/MINFO/
----jared.dcom (record associated with the `jared.domainA.dcom` address (mailbox))
```

#### Resource Record Data Structure
Every single RR has the same record structure seen with the `ND` record structure at the `root system`. That structure includes the following:

-`record_name` - The record name.
-`type` - The record type.
-`class` - The record class.
-`ttl` - The time-to-live for the record.
-`rdata` - The dWeb network address or `record_name` (for a CNAME record).

The difference being that within the files that store RRML-formatted record data, the record_name is omitted since it's the name of the file as well as the `class`, and it's the name of the folder that the record record data resides in. RRML adds a few more markup tags that provide extra data regarding the record itself, which are discussed in [Resource Record Markup Language](#resource-record-markup-language).

#### Resource Record Types
As mentioned previously, RR types are always sub-folders within a domain folder. The following RR types are a part of the dDNS standard (DWRFC-0001):

| Type | Description |
| --- | --- |
| D | A dWeb network address; the dWeb alternative to an `A` record. |
| CNAME | A canonical name or alias name for another `D` and maps this to the canonical name. |
| MINFO | An email address (mailbox) name that maps to a dWeb network address. |
| ND | Authoritative NameDrive for this domain. |
| SRV | Provides the name of a peer, service or device that is mapped to a dWeb network address. |
| TXT | Arbitrary text that provides a way to add text comments to the database. |

#### Resource Record Classes
The following are RR classes found within a record's `rrml` file:

| Type | Description |
| --- | --- |
| DW | Indentifies the DWEB protocol family. |
| DM | Identifies the DMESH protocol family. |

#### Resource Record Name
An RR name (record name) is simply the name of the record itself (e.g., "www"). Valid names include A-Z, a-z and 0-9.

#### Resource Record TTL
Must be a numeric value for how long the record should be cached.

#### Resource Record Data
RR data (rdata) is either a dWeb network address (for D, MINFO, ND, SRV and TXT types) or a canonical (alias) name for an already existent `D` record.

#### Resource Record Markup Language
`.rrml` files are formatted using the Resource Record Markup Language, which is a JSON-style format that is easily parseable by languages like JavaScript.

Below is an example RRML file for a `www` `D` record:
```JSON
{
  "class": "DW",
  "ttl": 300,
  "rdata": "aae4f36bd061a7a8bf68aa0bdd0b93997fd8ff053f4a3e816cb629210aa17737",
  "created": 1463443200000,
  "description": "my website",
  "author": "@jared",
  "modified": 1474371000000
}
```

##### RRML Tags
The following RRML tags are a part of the RRML Specification (DWRFC-0002):

-`class` (Required) - The record class; see [Resource Record Class Types](#resource-record-classes).
-`ttl` (Required) - The record's [Time-to-live](#resource-record-ttl).
-`rdata` (Required) - The [Record data](#resource-record-data).
-`created` (Optional) - UNIX Epoch of the time the record was created.
-`description` (Optional) - Description of record (string).
-`author` (Optional) - Author of information (string).
-`modified` - UNIX Epoch of the time the record was last modified.

#### Record Resolution
Each query begins at a name resolver located in the user's host system. Each resolver is configured to know the name and address of a local NameDrive that's used for caching records. The local ND is used to cache RRs for various domains that the user accesses to limit the lookups for frequently requested records.

##### 1. Querying The Local NameDrive
Like with public NameDrives, a local NameDrive stores domains, record types and record data in the same formats discussed previously. The only difference is that it only stores records for the allotted time found in each record's `ttl` RRML tag.

Querying a local ND for a record is as easy as accessing the record's anticipated path, like so:

`/home/LocalND/domainA.dcom/D/www.rrml`

This should return the RRML for the given record which can be easily parsed.

##### 2. Querying The Root System
If a domain or RR for the specified domain is not found within the local NameDrive, the Root System (ARISEN's `ddns` database) is queried for the domain's ND record(s). Most domains only need one ND and since it's not a server and is completely distributed amongst peers, it will probably never go offline. The Root System can be queried via [ARISEN's Open RPC-based API](https://docs.arisen.network) and will return a dWeb network address for a specified domain and record name, if the ND record is stored in the Root System.

##### 3. Looking Up dWeb Network Address
The dWeb network address is searched for on dWeb's DHT until it returns a list of peers for the address.

##### 4. Downloading And Querying NameDrive
Once (peers) are located, the NameDrive is download over [DWEB](#dweb) and the record can be queried via the local location of the NameDrive, like so:
`/home/DDrives/<namedrive-key>/domainA.dcom/D/www.rrml`

Once the `rdata` is parsed from the RRML, the dWeb network address has been successfully resolved.

A dWeb client can now move the record to the local NameDrive for the allotted TTL and delete the rest of the downloaded NameDrive. Another alternative dWeb clients can take is to live replicate a NameDrive within a local folder, so that a frequently accessed domain's records are always up-to-date and available locally, which means the TTL can be ignored altogether. This is made possible through the nesting of other NameDrives within the local NameDrive.

#### Truly Distributed Resource Records
dWeb's DNS system is far more distributed and decentralized than the World Wide Web's traditional DNS system. Many factors stand out, such as:

-Domains are fully and forever owned by users and cannot be used in any way without the domain owner's `owner` keys.
-dTLDs are not controlled by a centralized organization who has the ability to choose who can or cannot register a domain, nor do they have the ability to take control of or restrict a domain that belongs to its part of the domain tree.
-NameDrives are a server-less alternative to a NameSever, which is used to store multiple domain zones and RRs.
-A Domain's authoritative ND record is stored on ARISEN and is controlled only by the domain owner.
-A NameDrive, unlike a NameServer, is distributed by peers who are currently accessing its records as well as those who choose to seed (host) it. This means that most NDs do not have a central point of failure.

For these reasons, dWeb's dDNS system is a far more effective and reliable solution when compared to traditional DNS systems.

### Reporting System
dWeb's reporting system, dReport, is used by the community to report content that is considered illegal under the dWeb's Constitution. Entire websites and accounts that take part in illegal activity can be reported as well. The goal of dReport is to prevent the dWeb from becoming a darknet-like network and to keep the dWeb from becoming a safe-haven for criminal activity.

dReport is an ARISEN contract deployed under the name `dreport` that allows report submissions and community members to vote on specific submissions. dWeb's elected governance has the authority to remove data, accounts, dWeb network addresses and domains related to the activity reported in submissions. User Interfaces can easily be developed around the @`dreport` contract, providing an easy way for anyone to submit and vote on reports. Applications like a social network or marketplace could integrate a reporting feature that interacts with the `dreport` contract as well. See [dSocial Reporting System](https://github.com/peepsx/dsocial-whitepaper) to see how dSocial is developing their dWeb-based social network around dReport.

#### System Reference
The following `actions` can be executed via the `dreport` system:
-[`report`](#report)
-[`expire`](#expire)
-[`vote`](#vote)
-[`unvote`](#unvote)
-[`comment`](#comment)
-[`uncomment`](#uncomment)
-[`alert`](#alert)

##### Actions

###### `report`
Propose a new report to the community.

**Parameters**
| Name | Type | Description |
| --- | --- | --- |
| reporter | name | ARISEN account creating the report; used for authentication. |
| report_uuid | name | UUIDv4 for the report. |
| title | string | The report's title (must be less than 1024 characters. |
| report_json | string | See [Report JSON Structure] (#report-json-structure). |
| expires_at | time_point_sec | Expiration data of report; must be no later than six months in the future. |

**Rejected**
-When missing signature of `reporter`.
-When `report-uuid` already exists.
-When `title` is longer than 1024 characters.
-When `report_json` JSON is invalid or too large (must be a JSON object and be less than 32,768 characters).
-When `expires_at` data is earlier than now, or later than 6 months in the future.

###### `expire`
Immediately expires a current active report. The report can only be expired by the original reporter that created it. It's not valid to expire an already expired report.

**Parameters**
| Name | Type | Description |
| --- | --- | --- |
| report_uuid | name | The report's UUIDv4 to expire. |

**Rejections**
-When missing signature of report's `reporter`.
-When `report_uuid` does not exist.
-When `report-uuid` is already expired.

###### `vote`
Vote for a given report using your account.

**Parameters**
| Name | Type | Description |
| --- | --- | --- |
| voter | name | The voter's ARISEN account. |
| report_uuid | name | The report's UUIDv4 to vote on. |
| vote | uint8 | Your vote: `0` = negative vote; `1` = positive vote. |

**Rejections**
-When missing signature of `voter`.
-When `report_uuid` does not exist.
-When `report_uuid` is already expired.

###### `unvote`
Remove your current active vote from a report, which will allow the voter to reclaim the RAM used to store their vote.

**Parameters**
| Name | Type | Description |
| --- | --- | --- |
| voter | name | The voter's ARISEN account. |
| report_uuid | name | The report's UUID to remove your vote from. |

**Rejections**
-When missing the signature of `voter`.
-When `report_uuid` does not exist.
-When `report_uuid` is expired but within its freeze period of 3 days.

###### `comment`
Create a comment in relation to a `report_uuid`.

**Parameters**
| Name | Type | Description |
| --- | --- | --- |
| commenter | name | The ARISEN account of the commenter. |
| comment_uuid | string | The comment UUID (for reply purposes). |
| comment_content | string | The comment data. |
| reply_to_commenter | name | The initial comment's commenter that your comment replies to. |
| certify | bool | Reserved for future use. |
| reply_to_commenter_uuid | string | The initial comment's UUID that your comment replies to. |

**Rejections**
-When missing signature of `commenter`.
-When `comment_content` is an empty string.
-When `comment_content` is bigger than 10,240 characters.
-When `comment_uuid` is an empty string.
-When `comment_uuid` is bigger than 128 characters.
-When `reply_to_commenter` is not set but `reply_to_commenter_uuid` is.
-When `reply_to_commenter` is not an existing account.
-When `reply_to_commenter` is set and `reply_to_commenter_uuid` is an empty string.
-When `reply_to_commenter` is set and `reply_to_commenter_uuid` is bigger than 128 characters.

###### `uncomment`
Remove a comment.

**Parameters**
| Name | Type | Description |
| --- | ---- | --- |
| commenter | name | ARISEN account of commenter. |
| comment_uuid | string | The UUIDv4 of the comment to remove. |

**Rejections**
-When missing signature of `commenter`.
-When `comment_uuid` is an empty string.
-When `comment_uuid` is bigger than 128 characters.

###### `alert`
An `alert` action can be executed by any ARISEN user if a report has reached the required `threshold` of votes (See [Report JSON Structure](#report-json-structure)). `alert` sends details to all 21 of dWeb's elected governance.

**Parameters**
| Name | Type | Description |
| alerter_account | name | The account initiating alert (will pay CPU/Net fees). |
| report_uuid | name | Report UUIDv4 of report. |

**Rejections**
-When `report_uuid` has not met voting threshold.
-When `report_uuid` has expired.

##### Tables, Data Structures & On-Chain Database
The `dreport` data and database tables exist in an `on-chain` database on ARISEN's mainnet, under the name `dreportdb`.

Data that derives from the actions previously outlined are saved in one of the following tables:

###### `reports` (Table)
**Row**
| Row Name | Type | Description |
| report_uuid | name | UUIDv4 of the report. |
| reporter | name | The ARISEN account of the reporter's account. |
| title | string | The report's title. |
| report_json | string | The report's JSON metadata. |
| created_at | time_point_sec | The date the report was created. |
| expires_at | time_point_sec | The date at which the report expires. |

**Indexes**
-First (`1` type `name`); index by `reporter_uuid` field.
-Second (`2` type `name`); index by `reporter` field.

###### `vote (Table)
**Row**
| Row Name | Type | Description |
| vote_uuid | uint64 | The unique ID of the vote. |
| report_uuid | name | The UUIDv4 of the report the vote is for. |
| voter | name | ARISEN account of the voter. |
| vote | uint8 | The vote value for the vote (0 or 1) |
| vote_json | string | The vote's JSON metadata. |
| updated_at | time_point_sec | The date at which the vote was last updated. |

*Indexes**
-First (`1` type `i64`); index of `vote_uuid` field.
-Second (`2` type `i28` input in hexadecimal little-endian format); index by `report_uuid`, the key is composed in the high bytes using the `report_uuid` and the low bytes of the `vote`.
-Third (`3` type `i28` input in hexadecimal little-endian format); index by `voter`, the key is composed in the high bytes using the `voter` and the low bytes are the `report_uuid`.

##### Report JSON Structure
The `report_json` parameter requires a specific JSON-valid structure, which should contain many required fields. Report and reason types must be valid, per DWRFC-3`.

An example of Report Metadata is below:
```
{
  "type": "content", // Type of medium where activity is taking place.
  "reason": "illegal-pornography", // Report reason category.
  "threshold": "300", // Vote threshold to qualify for `alert` action.
  "status": "emergency", // Report status.
  "domain": "porn.dcom", // Domain where content exists.
  "dweb_address": "<dweb-key>, // Related dWeb address.
  "full_domain": "dweb://porn.dcom/user/post/3", // Full link to illegal content.
  "risk": "10", // Risk to community.
}
```

###### Report Types (`type`) (Required)
Report types should be listed in the `type` field of the `report_json` structure. The following types are valid:
-`content` - Report involving specific content on the dWeb.
-`website` - Report involves website where illegal content makes up a large portion of its content.
-`application` - Report involves an application where illegal content makes up a large portion of its content.
-`drive` - Report involves a dDrive or distributed file system where illegal content makes up a large portion of its content.

###### Report Reasons (`reasons`) (Required)
Report reasons should be listed in the `reason` field of the `report_json` structure. The following reasons are valid:

-`counterfeit_items` - Content or activity involves counterfeit items.
-`counterfeit_money` - Content or activity involves counterfeit money.
-`fraud` - Content or activity involves fraud.
-`illegal_drugs` - Content or activity involves illegal drugs.
-`illegal_pornography` - Content or activity involves illegal pornography.
-`violence` - Content or activity involves illegal violence.

###### Report Threshold (`threshold`) (Required)
A report threshold is the amount of positive votes needed for a message to be sent to all 21 governance members concerning the issue, using the `alert` action. By default, `alert` requires AT LEAST `300` positive votes. Report thresholds should be listed in the `threshold` field of the `report_json` structure.

###### Report Status (`status`) (Required)
Report statuses should be listed in the `status` field of the `report_json` structure. The following statuses are valid:
-`regular` - A regular report.
-`emergency` - An emergency report, which should drop the default `alert` threshold to `10` votes.

###### Report dWeb Address (`dweb_address`) (Optional)
The dWeb address related to a report should be listed in the `dweb_address` field of the `report_json` structure. It is only required if the illegal activity or content involves a dWeb network address.

###### Report Full Domain (`full_domain`) (Optional)
The full domain path related to a report should be listed in the `full_domain` field of the `report_json` structure. It is only required if the illegal activity or content involves a specific address, not a website or web application at-large.

###### Report Risk Rating (`risk rating`) (Required)
The risk rating related to a report should be listed in the `risk rating` field of the `report.json` structure. It is required for all reports. A risk rating is a rating from `1` (the least) to `10` (the most) and is for grading the risk that the activity or content poses to the community.

#### A Safer Web
Users and developers on the dWeb must have a way to protect themselves and the dWeb at-large from digital habitats that harbor criminal activity without introducing centralization into the network, which would inevitably be abused and used to eventually restrict the freedoms of the dWeb's users and developers. dReport utilizes the limited and checked authority of ARISEN's [Governance](#governance) and a decentralized community reporting system to give community members the ability to properly alert governance members of illegal activity and content, so that the governance can move to take one of the following actions:

-Change keys for the `domain` involved.
-Blacklist the dWeb network address involved.
-Reverse fraudulent transactions (stolen funds).
-Change keys of ARISEN accounts involved.
-Remove illegal content from on-chain datastore.

Morally unacceptable behavior like hateful comments, fall under dWeb's Constitution as within a user's right, and are therefore "unbannable behavior." Illegal activity, on the other hand, such as a user selling illegal drugs, is considered "bannable behavior." dWeb governance members are not permitted to ban accounts for morally unacceptable behavior and if they do, users of the dWeb can elect new governance members and reverse the actions taken. Illegal activity, on the other hand, will not be tolerated and shouldn't be.

Any governance action mentioned above requires a 15/21 majority vote.

## The Decentralized Web
I hope this paper has thoroughly explained the various protocols and abstractions that help form dWeb's unbreakable fortress of freedom. As I have pointed out in past papers, the dWeb provides true end-to-end decentralization for communications, websites, web applications and other mediums. While the dWeb utilizes transport protocols like TCP and UDP, it is truly transport agnostic. Like most protocols, it doesn't know anything about the layers beneath it. The same goes for the Internet Protocol (IP). Even through dWeb's DHT stores peers in a swarm according to their IP address and port number, the DHT will store any kind of data and therefore the dWeb is Internet-agnostic as well.

I say all of that to emphasize that the dWeb can and will provide end-to-end decentralization across any and all environments, and is here to stay. Below, I want to briefly review each area of dWeb's decentralized spectrum of services.

### Decentralized Web Hosting
The dWeb eliminates the need for web hosting since the files for websites and web applications are distributed within a dDrive and hosted amongst the peers who view them. The dWeb makes way for a serverless web.

### Decentralized Network Addresses
The dWeb doesn't use IP addresses for addressing entities on the network; rather it uses 64-bit hexadecimal addresses that are wholly owned by the creator of the address.

### Decentralized Data
Web applications can store app-related data via `on-chain` databases, or via `off-chain` databases such as [dWebTrie's](#dwebtrie) key value store. Either way, the users who create the data are in control of the data and not the application itself. The data is also immutable, although the creator always has the option to remove the data from the dWeb.

### Decentralized Authentication
Applications that utilize ARISEN's universal authentication layer provide users with a simple and decentralized form of authentication. Applications on the dWeb, unlike traditional web applications, do not store a user's authentication details. Instead, a user is always in control of their authentication details and is able to authenticate using ARISEN's autonomous and decentralized authentication layer.

### Decentralized Domain Names
Users of the dWeb are able to become registrars of their own dTLDs, and other users are able to register domains under those dTLDs which they wholly own and control for the lifetime of the dWeb. Domains are impossible to seize and are completely decentralized. The dWeb is also backed by a decentralized domain name system, where DNS records are completely distributed and only modifiable by the domain's owner.

### Decentralized Payments
The dWeb and the applications that are distributed across it, are only compatible with decentralized payment network like Bitcoin, due to the code of all dWeb-based applications being open source. This means that the earnings of a user or business can never be seized and, as a result, have actual intrinsic value.

### Decentralized Binary Exchange
At its lowest levels, dWeb is simply a protocol for the peer-to-peer exchange of binary, which opens the door to the decentralization of all kinds of systems - from phone systems to live-streaming services. Even the decentralization of the physical and centralized Internet infrastructure beneath it.

### Decentralized Abstraction & Governance
The dWeb is the first web ever created that is governed by duly elected governors, whom are voted in by the network's users. dWeb's governance insures that the dWeb will always remain safe for users and is programmatically prepared to halt illegal activity and reverse fraud against its users. This form of decentralized arbitration, regulation and governance rids the dWeb and its community of needing a central government or central regulatory body to "protect" it from bad actors, or even worse, "terrorism."

## Beyond The Coin
The terms "blockchain" and "peer-to-peer" have a bad reputation, and it's due to the legal attacks that have been leveled on the many projects associated with these terms over the years. I'm sure you've seen them:

-ICO scams.
-Bitcoin and drug cartels.
-Torrents.
-Illegal file sharing.
-The DarkNet.

The problem is, I rarely here these very same voices speak out about the many technological advances that blockchain and peer-to-peer technologies have helped to bring about. In fact, most people have never heard of these advances, because unfortunately, they only hear about the so-called scams and failures associated with them. At the very same time, one must not forget how much deep state neo-fascists hate decentralization. The controlling elite are very worried about the threat decentralized and encrypted communication protocols pose to the likelihood that they can pull off the "Great Reset."

Brave individuals like myself have attempted to utilize various decentralized technologies to re-imagine things like our banking system, and we were attacked by deep state operatives within our own government and imprisoned for simple mistakes found within published blog posts. This is because people like me understand how scared these factions are that decentralization will help spark the "Great Awakening" before they can successfully pull of their Great Reset. The legal onslaught unleashed on me and a handful of others has created "obedient" software developers who are scared to face the beast. At the very same time, many of those developers are "all about the coins," as I like to say, and are only it for the money. I do this because the beast you were just awakened to, is a beast I've been at battle with for years. Now that I've been inside it for 26 months, I am far more familiar with it than any of my counterparts, and it has played a major role in our development of the dWeb.

For the past few years, I've come to understand the beast's greatest weapon - the prison industrial complex - and how to avoid falling victim to it once more. One may even wonder how an imprisoned team of software developers were able to do all of this from within a prison to begin with? The answer is simple: our clear and concise understanding of the best itself. One must first understand what the beast uses to attack those who try to free its slaves and how it attacks. The beast uses its judicial and executive arms in tandem to:

-1. Accuse the opposition of fraud where "commerce" is the means (e.g. accuse the opposition of scheming for money in some capacity).

This is almost always successful because decentralized software is open source, highly complex, takes an extraordinary time to develop and, considering it's decentralized, the developers almost never have a way to create any sort of income to keep, at very minimum, the lights on. It's also important to note that projects of this size are rarely built by a single individual. Considering cryptocurrencies are an element of blockchain technology, developers conjured up the ICO (Initial Coin Offering), in which coins are sold to supporters in order to support the cost of developing their various ideas, just as I did with AriseBank. It wasn't long before the SEC figured out that these developers were selling what they referred to as "unregistered securities," and I was eventually the guinea pig for the country's first crypto-based securities fraud case. It's also a way for the beast to somehow convince the public that a few patriotic software developers were actually just a bunch of scammers. Nothing to see here folks! We're glad we could protect you from these evil, benevolent men. Carry on!

-2. They then use the court system to seize the threat (the software), while convincing "their" court that it's in the best interest of the "harmed supporters" that they also raid the developers and seize all available funds, computers, servers, domains and documentation.

By seizing everything, onlookers in the general public reading the news about the alleged fraud having nothing to base their opinion on except the contrived allegations. It's the sort of strategy you're seeing with the 2020 election and the whole "where's the evidence" debate (evidence is uploaded to Twitter, then quickly fact-checked and removed before anyone has a chance to see it). AriseBank's websites and software were only available over the centralized web, as well as the HTTP protocol, which made the seizure of the "arisebank.com" domain and the thousands of pages of documentation on our centralized servers as easy as serving a warrant on the web hosting company and domain registrar. It also eliminated any evidence of the tremendous amount of work put into the project, which helped them sell the "intent" of our alleged fraud.

-3. Convince the court to imprison the threat and publicize the criminal indictment of the developer(s) in order to scare other developers from pursuing similar ideas. What a mess, right? Welcome to my life.

I realized rather quickly that I had made two critical mistakes. My first mistake was selling coins. I should have figured out a way to self-fund the project, but the biggest mistake I made was putting the cart before the horse. How silly was I to think that I could develop decentralized bank using centralized mediums? I even involved centralized companies like VISA into my vision. Looking back, I realized something that most are just now starting to realize. Before we can ever put banking, healthcare, media and other industries bank into the hands of the people, we have to to first have a decentralized foundation on which we can build, because the World Wide Web is certainly not the place to free America's slaves. The beast has its claws all over it.

It's why less than 24 hours after the seizure of AriseBank, I began work on the dWeb. I forked other protocols that I was able to combine into what has been compiled into this paper, because I didn't trust other developers nor did I want a single central point of failure to come back to destroy that foundation. I have deeply studied the ins-and-outs of all these protocols and have thumbed through the thousands of lines of code that help form their existence to such extent, that I literally dream about them at night.

I have studied numerous Data Communications courses in order to gain a deeper understanding of the holes that the beast could exploit to bring the dWeb to its knees. I have taken the last 26 months to handwrite specifications like this one, as well as many blog posts, in order to raise awareness of what is to come (until my wrist could no longer be used to write and my pen-resting finger was blistered). More importantly, I faced the challenge of doing it from behind bars and without a computer. I even had to self-fund development without a single dollar to my name which, quite frankly, was the biggest challenge of them all.

The journey to launching the dWeb has been amazing, although our work is only just beginning. I can safely say that we can now "decentralize everything," considering that the dWeb gives us the necessary foundation to do so. Now it's time for developers like you to move beyond the coins and figure out a way to decentralize something. Truly decentralized cryptocurrency exchanges, even though the software cannot be seized, still find themselves in legal troubles, and it's all due to the coins being traded on their platforms, and greed getting the best of these developers. **Make the coin the utility and not the product** and you will be able to use these technologies to improve the lives of millions or even billions of people. That doesn't mean you can't take money like the rest of the world. Start a ride sharing app and take a percentage of the crypto spent on rides. Start a marketplace and take a percentage of the overall sales. Start a real news website and sell some advertising. The dWeb has the potential to create the right kind of changes for the right kind of people, and for all the right reasons. We simply must not make the coin our god or our product. We must move beyond the coin, and I promise, the best will fall.

## The Case For The dWeb
I wanted to end this paper by making the case for the dWeb. I want to discuss what many may believe to be its weaknesses, as well as its obvious advantages and advances when compared to the World Wide Web.

### Perceived Weaknesses
Below are many of dWeb's perceived weaknesses and what we're doing about them.

#### Underlying Physical Internet Infrastructure
It is true that Internet Service Providers could organize residential customers into groups that are separate from their enterprise customers, in an attempt to keep residential internet subscribers from sending packets to the ISP's switching nodes that have `control information` containing the destination of a residential customer. In other words, ISPs could share a database of residential customers, along with each customer's public IP address, so that ISPs could, in theory, prevent two dWeb peers from connecting directly to one another via their packet-switching networks.

Consider the diagram below:
```
[UserA]
    |
    |
[AT&T| ---- [Node4] ---- [Node5] ---- [TimeWarner]
                                                                 |
                                                                 |
                                                            [Node5]
                                                                 |
                                                                 |
                                                            [Node3] ---- [UserB]
```
It is true that ISPs could effectively shut down peer-to-peer transport in this fashion. Before I speak on our solution to this, it's important to understand how a dWeb message makes it from one peer to the next.

-1. UserA creates the following dWeb message (`Feed` message):
```
-Length
-C/T:
--Channel: 0
--Type: 0
-Message Body:
--dWeb network address
--Random nonce
```
-2. The dWeb message is packaged within a UDP datagram like so:
```
-PeerA port
-PeerB port
-Length
-Checksum
-UDP Body: (dWeb message)
--Length
--C/T:
----Channel: 0
----Type: 0
--Message Body:
----dWeb network address
----Random nonce
```
-3. The UDP datagram is then packaged in an IP datagram, like so:
```
-Version: 4
-IHL: 5
-DS:
-ECN: 00
-Total Length: 1000
-Identification: 000001
-Flags: [Don't Fragment]
-Fragment Offset
-Time To Live: 10
-Protocol: 17 (UDP)
-Header Checksum: <header checksum>
-Source Address: UserA's public IP
-Destination Address: UserB's public IP
-Options: <user options>
-Padding: <variable header padding>
-Data: (UDP and dWeb datagrams)
--PeerA Port
--PeerB Port
--Length
--Checksum
--UDP Body:
----Length
----C/T:
------Channel: 0
------Type: 0
----Message Body:
------dWeb Network Address
------Nonce
```
-4. The IP datagram is handed off to different switch nodes that route packets to the destination (PeerB).
-5. Once PeerB receives the IP datagram, it strips off the IP header to get to the UDP message, which shows the port the UDP data is meant for on PeerB's device and then strips off the UDP header (PeerA Port, PeerB Port, Length & Checksum).
-6. PeerB's computer sends the UDP Body (dWeb message) to the specified port, where the dWeb client operates and can consume the dWeb message in whatever way the client consumes dWeb messages.

Simply put, a dWeb message is handed down to the transport layer (in this case UDP) to prepare the message for transport over the Internet, where it is packaged within a UDP datagram along with PeerB's port. The UDP datagram is then packaged within an IP datagram that contains PeerB's public IP address and other control information, which is then transported out via PeerA's IP and their Wide Area Network (WAN). The IP datagram contains PeerB's destination information which each switching node (router) will use to route this datagram to PeerB.

If these switching nodes are configured to reject destinations that belong to residential subscribers, and if the source of the datagram is also a residential subscriber, then this would completely eliminate the ability for two residential subscribers (two dWeb peers) to communicate.

But the problems wouldn't stop there.

I just showed how this sort of adjustment would shutdown the communication (sharing of data) between peers. Although, this would also shutdown the discovery of peers as well. If you recall, in [DWDHT](#dwdht) I explained the peer discovery process and how peers of the dWeb each represent a DHT node on the network, each of which ends up storing a portion of the table (a portion of the announced dWeb network addresses and the peer info (IP and Port) of the peers swarming (announcing) the address).

Let's say PeerA wants to query the DHT for a specific dWeb network address. First, DWDHT calculates which node (via algorithms previously explained) is storing the peer info related to a particular dWeb address and then PeerA queries this node for the record. Considering PeerA and the DHT node (another peer somewhere) are probably residential subscribers, the same problem that occurred in the dWeb communication process would also occur during the peer discovery process.

While PeerA querying a another peer's DHT node doesn't involve the exchange of dWeb protocol messages, it does involve PeerA sending a particular question (a DWDHT message) to the DHT node, to which the DHT node would respond. In this case, PeerA would say to the DHT, "Hey, I heard you know about <dWeb network address>, do you have any info on it?" (Note: this is a pseudo-representation of the question.) Since every DHT operates on the same port (44578), PeerA creates a UDP datagram with the DHT question in the body, attaches PeerA's source port (for the remote DHT node to send the answer to) and the DHT node's destination port (44578), which is placed into an IP packet with the DHT node's IP address, and sent out to the Internet. Considering that the ISP's routers notice this packet's source is a residential subscriber and that the destination (the DHT node) is also a residential subscriber, the router aborts this transport. This means that PeerA never receives a list of peers associated with the dWeb network address since the DHT node (the destination) never receives the lookup question.

This would effectively shutdown the dWeb discovery process since peers could never query DHT nodes hosted by other peers, which means they couldn't resolve a dWeb network address (retrieve a list of peers who hold the data related to a network address). This would render the [DWEB Protocol](#dweb) (the dWeb communication process) useless, since PeerA would have no idea who to send DWEB messages to (want, request, etc.) since it never received a list of peers associated with the dWeb network address it was interested in. Therefore, the dWeb communication process would never take place.

I would be lying if I suggested this wasn't possible, although I will say that it would be a massive undertaking for ISPs globally to redesign their networks to prevent peer-to-peer transport. But after everything we've seen in 2020, I put nothing past the world's elite to do everything they can to advance their efforts to censor people who communicate over "their" physical infrastructure, whether it's the World Wide Web or the dWeb. It's important to note that due to dWeb's end-to-end decentralization, there is no way for an ISP, or anyone for that matter, to interfere with or exert control over the dWeb itself. However, using the doomsday scenario described above, an ISP could simply prevent residential subscribers from communicating with one another.

This has nothing to do with the dWeb specifically, but rather the types of measures ISPs could take to stop the transport of peer-to-peer communications deriving from applications that utilize peer-to-peer transport, including Bitcoin, Skype, Telegram and others. While this is an unlikely scenario that will probably never transpire, if you know me then you know that our "decentralize everything" motto is not just a slogan, it represents our entire mission and I loathe potential points of failure.

Therefore, it should come as no surprise that we've been hard at work on a research project called dMesh (you may have noticed this in [dDNS](#ddns) within `Record Classes`). With dMesh, we're experimenting with various analog frequencies, satellite and other forms or long range communications. The project has also included a deep drive into Bluetooth and infrared communications. In the first quarter of 2021, we will formally release some of that research, including our plans to launch a new network via dPhone, the world's first decentralized cellular device. dPhone will be powered by [PeerOS](https://peepsx.com/peeros) which will be the world's first decentralized operating system. Our goal is to build a powerful mesh network through the combination of these devices and various satellite-based transceivers. And with enough support, over the next several years we're confident that dMesh can have successful rollouts in some of America's largest cities.

While the solution will take time for there to be enough devices to build a strong enough mesh network over which peer-to-peer communications can be effective over long distances, any other attempt to build an alternative physical transport medium in a more rapid fashion is guaranteed to involve some sort of government intervention and licensing, and therefore centralization. With that said, as much as we want to remove ourselves from the physical Internet that has been used to violate our privacy at every turn, I don't believe it will be used to shutdown peer-to-peer communications any time soon, due to the massive effect these types of measures would have on the many important and widely used systems that rely on it.

Those systems include massive financial systems like Bitcoin that are worth hundreds of billions of dollars, as well as the Internet of Things - and I do not see major IoT companies standing idly by as the global elite destroy their companies. Let's not forget the major companies that are heavily invested in Bitcoin and other cryptocurrencies as well. Considering the DWEB cannot be individually targeted, I don't see this perceived weakness coming to fruition.

#### Peer Connectivity Issues
There are issues from time-to-time involving users on residential or mobile internet connections directly connecting to each other due to not having dedicated IP addresses or being able to accept incoming TCP connections. [dWebSwarm](#dwebswarm-api) is able to hole-punch through NAT (Network Address Translation) devices on networks which, in turn, solves the issue for most users. That being said, we are still working out issues involving specific kinds of NAT devices. We do not see this as a long-term issue, nor do we feel it will impact a large percentage of the dWeb's users.

#### Closed Source Applications
For app developers who want their applications to remain closed source, they can deploy a production build of their application using a framework like WebKit and do just that. However, these developers should keep in mind that the dWeb was not built to distribute closed source applications or applications that utilize configuration files that contain "secret" details such as passwords or API keys.

The open web allows its users to see what's happening in the background of applications, which keeps applications from spying on their users without it being completely obvious in the underlying code. The closed web has allowed companies like Facebook, Google and others to easily spy on their users, without anyone having a way of spotting these sorts of contraptions. But some of these companies may argue that the dWeb is not a place for proprietary software anyway, and they would be correct. The dWeb is a web that is run by the people and is meant to protect the people. This forces software developers to constantly update their software in order to keep up with the competition. I shouldn't have to explain why open source software is better, considering open source applications have overtaken their closed source counterparts in market share time and time again. The dWeb will usher in a new open source revolution where mainstream software can be studied, audited and improved by the public. In my humble opinion, this will not only speed up the dWeb's growth over time, but will also help bring about technological advances much faster, and in a world where collaboration is truly open and conducted without any sort of interference.

#### The Birth Of Another DarkNet
Many people have brought themselves to believe that fully decentralized systems are havens for criminal activity because there is no one who is in charge or capable of stopping illicit activity. In a general sense, these people would be correct. This is why I chose to integrate a blockchain like ARISEN into DWEB's off-chain protocols. As I covered in [Governance](#governance), [Address Registration](#address-registration) and [dDNS](#ddns), the elected governance has the ability through a 15/21 vote to render network addresses and domain names useless, taking them offline and redirecting would be visitors to a blank dDrive.

This is made possible by the [dReport](#reporting-system) reporting system, which allows dWeb users to report illicit content, websites and web applications so that other dWeb community members can vote on their removal. Once a vote passes a specific threshold, it is passed to the elected governance who can then take the actions necessary to remove the activity through a 15/21 majority vote.

dWeb's governance and reporting system ensure that the dWeb never becomes a safe haven for criminal activity, without introducing any sort of centralization to the network and ensuring that users of the dWeb remain in control of their data, network addresses, accounts, authentication details, money and domain names.

This also ensures that users cannot have their money stolen, since the governance can reverse fraudulent transactions. I'll never forget how Jones Day and the court-appointed receiver in the AriseBank case seized the money of our contributors, all to spend half of it (or possibly more) on "legal fees and expenses" with the supporters only receiving a small portion of their money back. One the dWeb, rogue government agencies like the SEC are no longer needed, since fraudulent transactions can be reversed without any loss to the "victim." If there truly are victims, they shouldn't become victims of the government as well. They should simply have their money returned.

The point being that we can code protections into our decentralized applications and systems so that they never become sanctuaries for evil, or reliant on the unnecessary services of overreaching governments. And the dWeb does just that.

### Advances and Advantages
Below are many of the advances dWeb brings to the Internet, as well as the clear advantages it has over protocols like HTTP and the World Wide Web at large.

#### User-Controlled Data
Whether it's data within some sort of [dDatabase](#ddatabase)-based abstraction (like a [dDrive](#ddrive)), or data stored on [ARISEN](#arisen) via the execution of smart contract-based actions, users are always in control of their data. It's a dramatic shift from how applications on the World Wide Web are inherently in control of user data, where it has inevitably lead to the abuse of users.

Due to the open source nature of the dWeb and how it's underlying protocols are designed, application data must derive from append-only binary feeds that are solely owned and operated by their creators, or on-chain data stores where the data can only be modified or removed by the ARISEN account that initially created it. This means that users can remove data from the applications they use, whether the apps want to give them that ability or not. User-controlled data is one of the more liberating features of the dWeb - a major advantage it has over the World Wide Web - and one of the biggest drivers for enabling true privacy on the dWeb.

#### Zero Infrastructure Costs
While users of the dWeb have the advantage of controlling their data, web developers have the advantage of avoiding infrastructure costs (webhosting costs) altogether. Considering the files related to a website or web application are stored within a dDrive and hosted amongst peers, servers are no longer needed to make websites or web applications available over the Internet. A web application could even utilize a smart contract deployed on ARISEN, which could be executed from within the application's code (in the dDrive) and used to authenticate the users and store related data. While smart contract executions do have RAM, CPU and NET costs, they pale in comparison to the infrastructure costs associated with hosting websites and applications on the World Wide Web. That being said, not all applications will have to use ARISEN since some developers will certainly get creative with [Multi-Writer Databases](#multi-writer-databases), while other applications don't require user authentication or an alternative to the server-based backends used within traditional web applications.

#### The End Of Online Censorship
The dWeb ultimately puts an end to online censorship by putting users in control of their data, and putting application developers in control of application distribution, network address, domain names and dDNS records. To truly showcase the difference between dWeb and the World Wide Web as it pertains to censorship, I'd like to compare an example of a social network on the centralized web (Gab) with an example a social network on the dWeb (dSocial).

Many centralized social networks claim to be "censorship-resistant" because their founders and the companies that control the data produced by these applications claim that they'll never use their control to censor the users who curate data on these platforms. This is misleading because many outside tentacles exist that have a higher level of control over these applications than the actual founders do. When the domain name gab.com was taken offline, it wasn't due to the actions of their founders. It was because of the centralized company in charge of domain administration for `.com` TLDs who, by design, will always have control of "gab.com," as well as the leftwing hosting providers who jumped into "save the world" mode and subsequently shut down gab.com's servers. While Gab and other similar networks should be recognized for their efforts to create a "safe space" for users wanting to voice their opinions, users of these applications should also be aware that these platforms cannot truly claim that they're censorship-resistant since their entire platform can be taken offline at any time.

**Centralized social networks like Gab can be completely censored and taken offline in the following ways:**

-The Internet Corporation for Assigned Names and Numbers (ICANN) administers the assignment of top-level names and addresses [STALL14] (such as gab.com) through domain registrars that they approve. At any time, ICANN or one of these centralized domain registrars can adjust their policies in order to seize a domain name. Rogue governments or judges looking to make a political statement (see Judge Ed Sullivan in US vs Flynn) also have the ability to order the seizure of domain names through ICANN or US-based domain registrars, which may point to websites that go against their political views or the views of their affiliates.

-Centralized hosting companies that operate within centralized data centers are where the data related to these centralized social networks is stored, as well as the files related to the web application itself. Without these centralized hosting providers, there would be no way to access the website or web application. As was seen in the gab.com debacle, hosting companies were targeted by leftists and were subsequently pressured to cancel their hosting agreements with Gab. This forced Gab to find a new hosting provider. Like top-level domains, rogue political factions can seize control of these servers and take the application files and data offline.

-Hackers can organize attacks aimed at taking these platforms offline, in the process gaining temporary control of accounts and deleting or editing (censoring) specific content.

-The owners of these application can have a change in views (see Drudge Report) and can all of a sudden elect to censor the views of their users, since they have central control over the data itself. This forces users to start their entire digital existence from scratch. In an age when businesses, personalities and regular everyday people store their entire digital existence on these networks, it's important that users are ensured by an application's source code that a sudden change in operation simply isn't possible without their say. With centralized applications like social networks, this will never be possible.

**dSocial lacks these central points of failure in the following ways:**

-dSocial uses [Decentralized Network Addressing](#decentralized-network-addressing), including its own decentralized domain name (`dsocial.dcom`) that is not administered by anyone, and uses [ARISEN](#arisen) and [dDNS](#ddns) as a means for storing its domain name records. dSocial can be accessed over the dWeb, via `dweb://dsocial.dcom`.

-dSocial's dWeb-based application files are stored within a dDrive and distributed amongst its users, rather than being hosted via "centralized server farms." dSocial's data that derives from actions is hosted across the nodes that make up ARISEN, while each post or comment's media files uploaded to the platform are packed within their own individual dDrives and distributed and stored amongst the viewers of these posts.

-dSocial's application code is distributed in a dDrive, which means that it can only be edited by the original creator of the dDrive, who also happens to be the holder of the private key related to the dDrive. The private key, through Elliptic Curve Multiplication (one-way function), creates the dDrive's public key which is then used to generate the dWeb network address, which is a "BLAKE2B" hash (one way hash) of the public key [RICE19A]. A dWeb network address is only discoverable by those who know it. Once discovered, a peer can then find a peer or peers from which they can download the dDrive. At that point, the peer or peers will have the dDrive's public key, which can then be used to verify the integrity of the data. The private key, which is needed to make edits to the state of the dDrive, is never exposed. And it is a well-established scientific fact, backed by a strong mathematical foundation, that the public key derivation process (`K=k*G`) cannot be reversed [WOOD19, AMTP17, ECEM19]. In other words, the public key cannot be used to calculate the private key.

**NOTE:** In the above equation, `K` is the public key, `k` is a randomly generated number in the form of a private key and `G` is a pre-determined generator point on the elliptic curve.

With that said, hackers would be unable to edit or exploit the application code within a dDrive without the private key. Also, even if dSocial's developers (who have the private key of its dDrive and control the private keys associated with the ARISEN account that deployed its smart contract) were to edit the application code within the dDrive or the smart contract code on ARISEN, several things are certain:

-1. Developers, regardless of the code they write, cannot change how the dWeb works. As explained in [User-Controlled Data](#user-controlled-data), users control their data, including the data they submit to applications. An application is simply pulling data from its on-chain datastore. It's not up to the application which data is allowed and which data is not. In other words, since dSocial uses ARISEN for user authentication and as a datastore, developers have to let ANY ARISEN account log in to their application and users data related to their use of dSocial's smart contract will certainly accompany them.

-2. The code is open source (all dDrives are open source by nature) and can be forked by other developers with a fresh set of keys. Since all of dSocial's data is openly available on ARISEN, the new clone would automatically pull all of dSocial's data, and dSocial's users would be able to log in and use the same system once again. Apps on the dWeb can be forked and re-launched within seconds using [dBrowser](#http://dbrowser.com).

While I can promise that Peeps will never change its position as it pertains to censorship, you don't have to take my word for it. Users of dSocial are in control of their own data and the application is open to the community. So much in fact, that if the community wanted to overthrow the dSocial application, while keeping their data and accounts in tact, they could do just that by launching an all new version of the application that would still use ARISEN as its immutable data source. This way, users would retain their data and everything pertaining to their digital existence on dSocial, while a dSocial alternative (fork) is simply dSocial reincarnated within a new dDrive and a new dWeb network address.

This is possible because dSocial is built on a truly decentralized foundation where users are programmatically in control, as opposed to developers, hosting companies, domain registrars and abusive bureaucratic leaders who are controlled and incentivised by special interest groups that hate freedom. Therefore, apps built on the dWeb cannot censor their users thanks to the dWeb's truly censorship-resistance foundation.

#### DDOS and Hacker-Resistant
Since files for websites and web applications are contained within a dDrive, dWeb-based desktop and mobile applications are downloaded directly to a user's device, with their functionality taking place on the end-user's device, eliminating the DDOS threat to the application itself. While some of the application's code (smart contract) is stored externally on ARISEN's blockchain, and blockchain networks can be the target of sophisticated DOS-style attacks, these type of DOS attacks will have very little effect, if any, on the dWeb and it's applications.

It an individual node on ARISEN is hacked, it's likely this was not due to the actual ARISEN software but rather, was more likely due to an ARISEN node that was not properly secured. In any event, it wouldn't matter if a node, even a block producer node for that matter, was compromised. This is because a consensus of 15/21 block producers are needed to make changes to on-chain code (smart contracts). Also, a hacker would be unable to affect the operations of the network, nor would they be able to halt any of the network's smart contracts. The only thing a hacker exploiting an individual ARISEN node could do is steal the private keys from that particular node's user (this would be difficult since the keys are encrypted, but still possible). If a hacker attempts to propagate fake transactions to other nodes actively participating on ARISEN, nodes will notice these transactions are fake, block the node sending out these fake transactions and as a result, the transactions will never be validated or stored on the network's ledger.

To be able to track the origins of transactions, or interfere with propagation, an attacker would have to control a significant percentage of ARISEN's nodes and at least 15 of the 21 block producer positions. Simply put, it would never happen and if it did, the community would immediately vote out this sort of attack and replace the compromised block producers with new ones.

As for peers that share websites and web applications amongst each other via the dWeb, it's important to understand that files being shared can only be edited by the creator of the dDrive, and that those downloading a dDrive from peers on the dWeb are downloading a binary representation of the files and not the files themselves. A peer is only broadcasting the binary data in the form of a dDatabase, which in turn makes up the "file system" of the dDrive, which is then fragmented across the peers that share it (seeders) to those who request to download it.

As far as dWeb's DHT is concerned, we have implemented a number of Kademilia extensions to enable a secure key-based routing protocol. Kademilia also protects against the only known attacks against DHTs, specifically:
-Sybil Attacks - Where a user generates an extreme number of DHT nodes to flood the network; and
-Eclipse Attacks - Where an attacker attempts to isolate a DHT node or a set of DHT nodes in the network graph, by ensuring that all outbound connections reach malicious nodes.

Last but not least, dWeb's use of ARISEN's [Universal Authentication Layer](#universal-authentication-layer) and public key cryptography makes it next to impossible to gain access to a user's account.

While the dWeb is not perfect, it is also not riddled with the same issues as the World Wide Wide and, as a result, it is a much better and safter alternative. By putting users in control, rather than centralized tech companies, it should come as no surprise that the dWeb network and the applications built on top of it, are much more secure when compared to the World Wide Web and the applications built on top of it.

#### A Decentralized Economy
For developers and users alike, perhaps the greatest advantage of them all, when teamed with diminishing infrastructure costs, is app developers having the ability to tightly integrate decentralized payments and currencies within their applications. This simply isn't possible with fiat money like it is with decentralized digital currencies.

Using dSocial as an example once more, ARISEN's RIX currency and dSocial's LIKE currency are highly integrated into the dSocial app, where users can earn RIX and LIKE for their posts and comments. While at the same time, these same users can spend their earnings via a dWeb-based ridesharing app (such as dRide) or a decentralized marketplace (such as dBuy). This has the potential to create a decentralized economy where fiat money literally isn't needed.

Continuing the example, dSocial could be configured to take a small fee for each post or comment upvote, or dRide could be configured to take a small fee for each ride (both easily accomplished via an ARISEN contract). The small fees would help the developers of the apps further development and market their applications. More importantly, in the case of dRide, the developers would have little to no infrastructure costs and as a result, would be able to pay their driver's more and provide rides for less.

This equates to perhaps the greatest advantage of all. If you didn't read that correctly, read this: dWeb-based apps can literally compete with their centralized competitors, which wouldn't be possible on the centralized web thanks to Google, Facebook, Amazon, Microsoft and other major tech companies who receive massive discounts on infrastructure. Sadly, while trying to compete with these companies, you would probably be buying your infrastructure directly from them (Google Cloud, AWS, Microsoft Azure, just to name a few).

That's correct, with the dWeb developers can dream again. Even better, they can compete again and the dWeb's users reap the benefits in the process (cheaper rides, etc.).

## Conclusion
The dWeb is a web where both users and developers benefit one another, and in ways the World Wide Web could never facilitate. We live in interesting times, where it seems like the "golden age of information" is coming to an end, but it's far from over if We The People have anything to say about it. Like those in 1775, we must maintain our independent spirit and, above all, we must maintain our consciousness. The revolution is consciousness, and as we awaken from being slaves who have long been placed under a hypnosis of freedom and liberty, we are now starting to realize that those freedoms, as well as our liberties, have been eroded to a point where they remain barely intact.

Our freedom of speech is one of our most important rights as ratified by our founders. Yet it's no longer a right so much as it is a scarce privilege on the World Wide Web. Big tech companies, through their partnerships with China, have waged war on those who choose to expose their anti-American propaganda being spread throughout the web. From tweets to YouTube videos, you either agree with them or you're fact-checked into oblivion, and banned not long thereafter. It's a false matrix that is now riddled with censorship and controlled by the likes of communist neo-fascist dictators like Mark Zuckerberg and Jack Dorsey. You could search Google to confirm what I'm telling you, but they have already scrubbed those sources months ago, sadly.

The Rockefellers didn't want the masses to have free and fair access to the web and they have certainly found a way to finally restrict it. Though what they never counted on were a couple of inmates who were willing to defy them against all odds, and risk their own lives and freedom in a journey to build and launch an alternative web where people and companies alike, could have their online freedoms and liberties back at a time when they're needed most. And as much as they would like to shut it down, and as I have thoroughly pointed out in this paper, it simply isn't possible.

This web I write about isn't a dream or a scheme, it's a reality. And expanding that web will be one of the most important fights for freedom since a few patriots decided to take up arms on a cold night in 1775 and fight for their families, their future and most importantly, their freedom.

You too can join the fight and become a part of the dWeb revolution yourself, by simply downloading [dBrowser](#http://dbrowser.com). While this paper may seem like a lot, I can assure you, just as I'm writing this with blistered fingers, that we're just getting started. There is so much to do and so much to build. I hope our work motivates you to join the cause. I hope it liberates you. I hope it awakens you.

Fight on!
