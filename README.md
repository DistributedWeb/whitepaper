# dWeb Whitepaper
##### Authored By: Jared Rice Sr.
##### Date Published: August 26th, 2019
##### Date Revised: December 10th, 2020

## Preface
The World Wide Web, while it was once thought to be somewhat decentralized, has been transformed over the years into an overly centralized cesspool of authoritarians that are ultimately controlled by governments from around the world, as well as a handful of globally recognized tech companies. That was not how the visionaries of the web intended for it to be, nor would any of them agree with how the web is being ran today.

Beyond the evidence that was revealed by Booz Allen Hamilton contractor Edward Snowden, understanding the fact that the CIA, NSA and various private sector companies, including Google, Facebook and others are highly involved in manipulating mass thought, is crucial, in understanding why the web has to be reimagined [ACKE14]. The clear Chinese-based control over these companies, along with many elected officials within our government has become obvious to many Americans, which should put into plain English why all of the mass censorship and massive election interference is taking place.

It is us, the denizens of the World Wide Web, that have become extremely reliant on how quickly we can communicate with friends and access information via the WWW's many facilities. Although, it is during the process of these various communications and periods of research, that our information, our actions and even our reactions, are collected and then stored within the databases of not only the providers of these facilities but the "virtual, centralized grand database" once envisioned by President Reagan's former national security advisor John Poindexter [ROSE02]. It was Poindexter who was the brainchild of the "Total Information Awareness" program, which became the NSA's Advanced Research and Development Activity (ARDA), was later renamed "Terrorism Information Awareness" [IWAR] and eventually reincarnated as an alliance between the government and Silicon Valley for-profit companies. Enter Booz Allen Hamilton and other for-profit companies like Palantir, ran by so called "libertarians" like Peter Theil, who are paid by the CIA and the NSA to spy on and gather data  about law-abiding American citizens in the name of "terrorism" [GREE13].

History clearly shows us, even through the admissions of companies like Facebook and academics at Cornell University that they're clearly running psychological experiments on their users [ROBI14], [KRAM14], [GOEL14], [KRISH14]. While they do what all CIA and NSA contractors do and attempt to cover-up their tyrannical behavior with their typical talking points of "civil liberties" and "improvements to their service," it is clear to most that they're actively doing more than simply attempting to improve their services - and the dog regarding their care for our "civil liberties" will never hunt with any true American patriot.

Companies like Facebook, who have clear relationships with the Chinese Government, are far more interested in controlling what we see and quite frankly, what we believe and it's these constant internal studies that allow them to do just that. For years, these private sector companies have been developing internal algorithms, that are used each and every day to study your habits, your mannerisms and other psychological traits in the name of helping improve their various advertising services. What truly opened the eyes of Silicon Valley brainchilds like Mark Zuckerberg though, had little to do with predicting what you would buy - these authoritarians were blown away when their own studies revealed that voting behavior could be influenced by "undetectable social networking maneuvering" [LANI14].

This research I just mentioned happened in 2014 and don't worry, the source I just cited was the ***New York Times*** - just in case they say there is not "widespread evidence of voter manipulation." That's why I felt it was important to cite a New York Times article, regarding a study that Facebook conducted themselves! Facebook and Google's interest in our elections has grown extensively since 2014, where they collectively contributed hundreds of millions of dollars to local governments and various companies, in an attempt to support candidates that had unusual relationships with the Chinese government, like Joe Biden, whose son is the subject of an active FBI investigation for his dealings with Chinese companies as of December 10th, 2020.

What they used to refer as "undetectable maneuvering" is no longer truly undetectable, due to the fact that social networks are not only quietly choosing what you see in your news feed, but are now openly censoring those whose posts rail against the political propaganda machine most Silicon Valley-based social networks have become. This also takes place within search engines like Google, where conservative news stories and websites are rarely shown to those they know are registered democrats (look no further than the blacklisted Hunter Biden story). Their efforts to interfere in the 2020 election were extraordinary and quite obvious to anyone who is concerned with freedom in America.

All of this is made possible by a clear invasion of our privacy that we have allowed to go on for as long as most of us have been alive. This disregard for our own privacy has allowed companies in the private sector to gather data from every corner of our personalities, our lives and our interactions, which they've sold back to the government, in a clear violation of our Fourth Amendment rights. They've used this data to build a clear profile of all of us. They know every African American democrat from Dallas, Texas that ate McDonalds last week, while watching Netflix and they'll make sure that each and every one of them are fed anti-American leftist propaganda until each and every one of them join on with the "American woke."

It was this sort of invasion of privacy that the English administrators over the American colonies took part in, which in turn, according to John Adams, spirited the American Revolution, as much as any other factor. A young John Adams in 1761 once documented a speech of James Otis saying "every man of a crowded audience appeared to me to go away, as I did, ready to take up arms against writs of assistance." Otis was delivering a speech where he openly denounced the "writs of assistance," a clear and open demand for privacy, which became a clear motivator in the establishment of our republic. I believe the invasion of privacy we face today, far exceeds what any of our founders could have imagined. Alice Walker once said, "the most common way people give up their power, is by thinking they don't have any," but they have also clearly given up their rights to tech companies across our land, in exchange for convenience and style. Who could have imagined that?

I have said, for quite some time, that we're overdue for another revolution, but this time, a portion of that revolution will certainly be digital. As Chief John Roberts fairly pointed out in Riley vs. California, the Fourth Amendment doesn't simply cover our physical homes, but our digital ones as well. Likewise, true American patriots don't face just a violent physical revolution in order to secure their freedoms, but a digital one as well as and they'll have to have a place in the digital realm, where their voices can be heard, without any possible interference from the left. I say that because building alternatives on the centralized web to Facebook and Google, is not a long-term solution, nor is it a viable one. Their control over us on the web, goes much further than what we see or what we read. They now control our companies, our domain names, what we publish, what we sell and yes, our ability to accept online payments.

Andrew Torba, the founder of Gab, a Facebook alternative for conservatives, has already been banned from most online payment platforms, has had his domain name seized, his servers taken offline and even his family members have been blacklisted by VISA. While that sounds like a conspiracy theory, it's an unimaginable reality that is taking place with companies like Gab today. Organizations like ICANN and ARIN, who control IP address delegation amongst Internet Service Providers and who became domain registrars, are quickly becoming activists for the left's causes and enacting policies that will soon force domain registrars like GoDaddy and web hosting providers like Amazon Web Services (AWS), to move on from WWW-based facilities that allow for the distribution of information that goes against the left's propaganda, which is exactly what Gab is used for and it's why Gab was used as a test-case in their effort to shun conservative facilities form the web itself.

Beyond just the web and the threat of ICANN, ARIN and quite frankly Biden's FCC silencing conservatives, is the additional threat of Google and Apple doing the same to mobile applications like Ales Jone's InfoWars, which was abruptly removed from both app stores once it surpassed CNN as the most downloaded news application, within hours of its launch. To put all of this into perspective, companies in Silicon Valley are selling user data to communist governments, running psychological experiments on their users, silencing those who expose their spread of propaganda, interfering in our American elections and are now forming a powerful resistance to alternative facilities that emerge as a result of their total disregard for American civil liberties.

This is why I have embarked on a journey for the past several years, to reimagine the web and build a medium where the people themselves are in control; instead of governments and/or the private sector. A quick look at how long this GitHub profile has been around as well as the amount of research that has gone or here, should reveal that to any open-minded layman. The point of this paper is to make the case for a decentralized alternative to the web that we've been developing for the past four years, known as the "dWeb." It will also explain how the dWeb works and will lay down the clear scientific and mathematical proof, that ensures that the dWeb itself, nor anything on it, can be altered or removed by a single entity, other than the author. At the very same time, this paper will also explain, in-depth, how an elected governance, a distributed election system and a decentralized reporting system are used to ensure that the people of the dWeb, as well as the elected governance, have the delegated authority to remove illegal activity, as well as illegal content, without any centralized systems that could otherwise be used to corrupt and take control of the dWeb by bad actors in the future, just as they have with the traditional web.

While the dWeb facilitates protections from the tyrannical and authoritarian actions of government, as well as the private sector, it also presents technological advances that allow for the global distribution of websites, web applications and various forms of data, without the need for costly infrastructure or the threat of hackers. Those advances, as well as others, are covered in this paper as well.

The need for a decentralized web, during a time where the web has become over-centralized, has become obvious to the many who avidly use the web and have witnessed the constant violation of the human, civil and constitutional rights of the web's users. The influx of countries and their governments whom use the web, as well as the Internet at large, to spy on their citizens has gotten out of control. The lack of privacy is truly alarming and so are the security issues, where even central banks are still unable to protect worthless fiat, after over 20 years of attempting to secure their networks. Now social networks and search engines want to get involved in politics, control what we find or view on their networks and will delete our entire digital existence over a policy violation. Big tech and tyrannical governments like our own know that software developers are building ways to avoid their tactics and are using policies, as well as regulations, to scare of innovators in an attempt to maintain power and keep their usual control mechanisms in place.

Developers, innovators and disseminators of information need a better protocol. They need a completely decentralized web where their hard work can never be destroyed or shutdown by fearful organizations or governments. They simply need a better web where they can experience true privacy, freedom, security and transparency digitally, for the first first time since the inception of the web itself. Isn't that what the web was created for? It's what the dWeb was created for, only this time, we didn't forget to hardcode those rules into its immutable and irreversible existence.

Welcome to the dWeb.

"There is no central control."
***- Paul Baran - Godfather of packet switching***

"My bias was to always build decentralizations into the net. That way it would be hard for one group to gain control. I didn't trust large central organizations. It was just in my nature to distrust them."
***- Bob Taylor - ARPANET Engineer***

**Note:**
As an added plus, the dWeb is alive as you're reading this and can never be taken offline, as long as people like you are using it. You can start browsing the dWeb by downloading dBrowser [here](https://dbrowser.com) and can begin following our efforts to launch truly decentralized facilities like [dSocial](https://github.com/peepsx/dsocial-whitepaper) and [dSearch](https://peepsx.com/dsearch).


## Abstract
The dWeb is a decentralized web that is formed by a foundation of off-chain, peer-to-peer networking, data communication and data integrity protocols, which enable the exchange and cryptographic validation of data amongst peers. Many dWeb services like dDNS and its reporting system are layered on top of the ARISEN blockchain protocol suite in order to utilize ARISEN's universal authentication layer, public key authority, distributed virtual machine, decentralized network consensus, decentralized payment processing, as well as its on-chain data persistence layer.

This marriage between the dWeb and ARISEN at dWeb's low level service layers, gives way to a powerful ecosystem that can be used to develop secure, serverless and globally scalable applications that are decentralized from end-to-end. The dWeb improves upon the limitations and weaknesses of the World Wide Web, while ensuring that tyrannical governments and private sector companies are unable to regulate or control any aspect of the dWeb itself, without the approval of the dWeb's users. The dWeb is a futuristic web where infrastructure costs are nil, hackers are rendered useless, and the users themselves are placed in control of their data, along with the delegated authority to elect a governance that protects the dWeb at-large from fraud, illegal activity and illegal content, per the dWeb's ratified [Constitution](https://github.com/distributedweb/constitution).

This whitepaper explains the dWeb in-depth, from its foundational protocols and ARISEN's protocol suite, to dWeb-based services like dDNS and its decentralized reporting system.

## Foundational Protocols
The dWeb's foundation of off-chain, peer-to-peer networking, data communications and data integrity protocols, enable peers to announce data, remote peers to discover and swarm data, exchange data and validate the integrity of the data that is ultimately fetched via a dataset's swarm of peers. The following sub-sections will explain each of these foundational protocols and how their reference implementations function.

### DWDHT
DWDHT, an acronym for "dWeb DHT," is a protocol that forms a distributed hash table of dataset identifiers (dWeb network addresses) and the peers that are announcing them. Each database identifier and its announcing peers form what is referred to as a "swarm," which can be queried from connected peers or joined (announced) by a specified amount of peers. dWeb's DHT is literally how a particular dataset is discovered and downloaded from the peers that are openly seeding it (announcing it). Each peer who is announcing a dataset on a long-term basis, by design, becomes a DHT node and is responsible for storing a specific portion of dWeb's DHT. Put another way, each DHT node stores a specific portion of dWeb's DHT. Put another way, each DHT node stores a specific portion of the dWeb's dataset identifiers, along with the peers who currently posses the data related to each of those data identifiers.

While each DHT node stores various swarms, it also stores information regarding other DHT nodes, identified by `node identifiers`, which can be mathematically determined to possess information regarding a specific swarm. dWeb's DHT is based on the `Kademilia Distributed Hash Table` popularized by BitTorrent and others. Nodes and data in dWeb's DHT are assigned 160-bit integers as IDs, while swarms are stored in the form of key-value pairs, where the key is a 32 byte value generated by a one-way hash function, such as SHA-1 (normally a dWeb network address), where the value is an object containing public and LAN-based peers that are announcing the key (the dataset identifier).

#### Node & Data Distribution
The DHT defines the distance between 2 DHT nodes `i` and `j` by the bitwise exclusive OR operations `(XOR)`, i.e., `d(i,j) = i (XOR) j`. This distance means for any given key `i` and a distance `L > 0`, there can only be a single key `j` that satisfies `d(i,j) = L` [ZHAN13].

All key-value pairs are stored on `k` nodes whose UIDs are closest to the actual key. `K` is an important parameter that helps determine data redundancy and how stable the DHT is at any time. Each node `i` always maintains multiple `k`-buckets. Each `k`-bucket stores a list of other DHT nodes, which are organized in an order that reflects the most recently active nodes. The node that is most recently active is stored at the tail, while the least active node is stored at the head.

The node whose distance from node `i` is in the range of [pow(2,m), pow(2,m+1)] is stored in the `m`th `k`-bucket (node that 0 < `m` < 160). The nodes in the `k`-buckets are regarded as the neighbors of the node `i`. A dWeb DHT node dynamically updates its neighbors upon receiving any messages from them. This process can be better explained more specifically, when node `i` receives a message from another DHT node `j`, which is located in the `m`th `k`-bucket, where the `k`-bucket of node `i`, will be updated in the following way:

-If `j` already exists in the `k`-bucket, `i` moves `j` to the tail of the list, as node `j` is the most recently seen.
-If `j` is not in the `k`-bucket and the bucket has fewer than `k` nodes, node `i` inserts `j` at the tail of the list.
-If the bucket is full, `i` PINGs the node at the head of that particular `k`-bucket.
-If the head node responds, node `i` makes it to the tail and ignores node `j`.
-Otherwise, `i` removes the head node and inserts `j` at the tail.

##### DHT Primitives
-`PING` - Probes a DHT node to check whether it's online or not.
-`STORE` - Used to store a key-value pair.
-`FIND_NODE` - Finds a set of nodes that are closest to a given node.
-`FIND_VALUE` - Operates like `FIND_NODE` but returns a stored value.

These RPC-like primitives work in a recursive way, which improves the efficiency of dWeb's DHT. A `lookup` procedure is initiated by the `FIND_NODE` and `FIND_VALUE` primitives, where the lookup initiator chooses `B` nodes from its closest `k`-buckets and send many parallel `FIND_NODE` requests to these `B` nodes. If the node being searched for in a given iteration is not found, the lookup initiator resends the `FIND_NODE` to the nodes that were found during the previous recursive operation and repeats this iterative functionality.

A KV pair may be stored on multiple DHT nodes. Thanks to the recursive procedure explained above, the key-value pair spreads across the DHT network every hour. This process insures that multiple replicas of data exist across the network. Every key-value is deleted 24 hours after it is initially pushed into the network.

##### Joining and Leaving
When node `i` joins dWeb's DHT, it is assumed that it is aware of or knows about node `j`. The joining process consists of multiple steps as follows:
-1. Node `i` inserts `j` into its `k`-buckets
-2. Node `i` starts a node lookup procedure for its own ID, where `i` is made aware of other newer nodes.
-3. Node `i` updates the `k`-buckets

During this process, node `i` strengthens its `k`-buckets and inserts itself into other nodes' `k`-buckets. When other nodes leave or fail, they DO NOT notify any other node. There is no need for a special procedure to cope with node departures, as these mechanisms insure that leaving nodes will be removed from the `k`-buckets.

#### Swarm Announcement
Announcing a swarm means that you're either: announcing a dWeb network address that doesn't exist on dWeb's DHT and therefore become the first peer related to the address; or announcing a dWeb network address that does exist on dWeb's DHT and are added to a list of peers who are announcing the address.

A swarm entry in dWeb's DHT, takes on the following format:

```
Key: 8f0ab2... (32 byte hexadecimal address)
= = = = = = = = = = = = = = = = = = = = = = = = =
Value:
{
  node: { host, port },
  peers: [{ host, port }, { host, port }]
  localPeers: [{ host, port }, { host, port }]
}
```

If a peer is announcing a dWeb network address that matches a key in the DHT, the entry is mutated to include the peer's announced IP address and port. When announcing a dWeb network address, a peer must set their public port and public IP address during the announcement and can choose to set a lan-based address and port as well, so that those on the same public IP who are performing a DHT lookup, can retrieve local announcers for a particular dWeb address. It's important to note that when announcing a dWeb network address on dWeb's DHT, the announcer becomes a DHT node on the network by design.

#### Swarm Lookups
A swarm can be looked up by its dWeb network address, which is a 32 byte buffer (normally a hash of something). When performing a lookup, the querying peer is temporarily an announcing peer, but is removed as an announcing peer (unannounces), once the lookup is finalized.

A lookup for a particular dWeb key, returns the following data:
```
{
  // The dWeb DHT node that is returning this data
  node: { host, port }
  // List of Peers
  peers: [{ host, port }, ...]
  // List of LAN Peers
  localPeers: [{ host, port }, ...]
}
```

#### DHT Bootstrap Nodes
dWeb's network utilized various `bootstrap` nodes to launch the initial dWeb network. These 3 bootstrap nodes are as follows:

-`dht1.dwebx.net`
-`dht2.dwebx.net`
-`dht3.dwebx.net`

These nodes going down would not affect the dWeb or a user's ability to announce or lookup dWeb network addresses, due to the way other DHT nodes on the network share data and information regarding each other. dWeb developers have the option of launching their own bootstrap nodes so that their apps have a point of entry into the dWeb's network of DHT nodes. Those nodes would initially use a set of already existing nodes to gain access to the DHT and would no longer need access to the nodes used for bootstrapping. Any node on the DHT can be used to bootstrap a new node.

#### DWDHT Reference Implementation
The DWDHT reference implementation was written in JavaScript and was used to launch the initial dWeb DHT. You can find it [here](https://github.com/distributedweb/dht).

You can use this reference implementation to allow dWeb-based applications (desktop, mobile or web) to act as DHT nodes.

You can also launch your own dWeb DHT node via the command-line, by using the DHT CLI [here](https://github.com/distributedweb/cli).


#### dWeb Swarm API
The dWeb swarm API, also known as "Swarm Programming," is a high level API built on top of the [DWDHT(#dwdht) Protocol used for finding and connecting to peers of a particular swarm and interacting with the dWeb DHT.

It expands the default set of parameters when creating a DHT node, adds specific swarm events where callbacks can be used to react to specific swarm-based events and introduces several functions, like `join`, `leave`, and `connect`, which make it easy to programmatically interact with the dWeb's DHT.

##### DHT Node Initiation Parameters
Using the dWeb Swarm API, a DHT node can be created with the `dwebswarm()` function, using the following parameters:

-`bootstrap` - An array of bootstrap servers used to initiate the dWeb DHT node.
-`ephemeral` - A boolean that is set to `false` if this is intended to be a long running DHT node.
-`maxPeers` - The total amount of peers that the node initiator will connect to.
-`maxServerSockets` - The number to restrict the number of server socket based peer connections. Set to `Infinity` by default.
-`maxClientSockets` - The number to restrict the number of client socket based peer connections. Set to `Infinity` by default.
-`validatePeer` - A function for applying filters before connecting to a peer.
-`queue` - An object for configuring peer management behavior.
--`requeue` - An array of backoff times in milliseconds every time a failing peer connection is retried.
--`forget` - An object for when to forget certain peer characteristics and treat them as fresh connections again.
---`unresponsive` - How long to wait before forgetting that a peer has become unresponsive.
---`banned` - How long to wait before forgetting that a peer has been banned.
---`multiplex` - Set to `true` in order to reuse existing connections between peers across multiple dWeb network addresses.

##### Swarm Event Emitters
Using the dWeb Swarm API, applications can listen for the following events:

-`connection` - A new connection has been created. Event should be handled by using the socket. This emits an `info` object that describes the connection using the following details:
```
-type (string) - Should be either "tcp" or "udp."
-client (boolean) - If true, the connection was initiated by this node.
-topics (array) - The list of dWeb network addresses associated with this connection, if "multiplex" was set to true, during DHT node initiation.
-peer (object) - Object describing peer (port, host, LAN peer details, referrer and the dWeb network address the peer was discovered under).
```
Using the `info` object, the `info.ban()` method can be called to ban the connected peer and will keep the DHT node from connecting to the peer in the future. The `info.backoff()` method can be called to get the DHT node to backoff from connecting to the peer.

-`disconnection` - A connection has been dropped. Emits an `info` object that is identical to the `connection` `info` object, describing the dropped connection.

-`peer` - A new peer has been discovered on the network and has been queued for connection. Emits a `peer` object, including the port, host referrer, and dWeb network address peer was discovered under. Also includes a boolean on whether the peer is a LAN-based peer.

-`peer-rejected` - A peer has been rejected as a connection candidate. Emits a `peer` object that is identical to the `peer` event's `peer` object, describing the rejected peer.

-`updated` - Emitted once a discovery cycle for a particular dWeb network address has completed. Emits a `key` object that identifies the dWeb network address the discovery cycle is related to.

For more information on dWeb Swarm's events, take a look at the README.md file within the reference implementation of the dWeb Swarm API [here](https://github.com/distributedweb/dwebswarm).

##### Swarm API Functions
There are several functions (methods in the reference JavaScript implementation) that can be used to interact with a dWeb DHT node. These functions and their required parameters are described below.

###### `join` - Join the swarm for a given `topic` (dWeb network address). This will cause peer to be discovered for the topic (`peer` event).

###### `join()` Parameters
-`topic` (Buffer) - The dWeb network address to list under. Must be 32 bytes in length.
-`options` (Object)
--`announce` (Boolean) - List this peer under the topic as a connectable target. Defaults to `false`.
--`lookup` (Boolean) - Look for peers in the topic and attempt to connect to them. If `announce` is false, this automatically becomes true.
-`onion` - A function that is called when your topic has been fully announced to the local network and the DHT.

###### `connect()` - Establish a connection to a given peer. You usually won't need to use this function, because the DHT node connects to discovered peers automatically. Accepts a `peer` object (identical to the `peer` object emitted by the `peer` event) for the peer the function is to establish a connection with. Also has a callback function that emits either a connection error (err), the established TCP or UDP socket (socket) and details regarding the established connection (details).

###### `leave` - Leave the swarm for a given dWeb network address.

###### `leave()` Parameters
-`topic` (Buffer) - The identifier of the peer-group to delist from.
-`onleave` - A function that is called when your topic has been fully unannounced to the local network and the DHT.

For more in-depth examples on how to call these functions within dWeb-based applications, view the `README.md` file within the reference JavaScript implementation [here](https://github.com/distributedweb/dwebswarm).

### dDatabase
A dDatabase is a distributed append-only log, also referred to as a distributed and immutable data feed, which can be exchanged between peers, using the [dDatabase Protocol](#ddatabaseprotocol) and validated via functions available in a dDatabase implementation.

At a very basic level, the dWeb is made up of dDatabase feeds, but it's important to note that a dWeb network address doesn't have to represent just a dataset, it can represent an entity like a peer or a device. Although, as the dWeb stands today, most of what you see distributed across it are dDatabases.

Below is a pseudo-representation of what a dDatabase would look like:
```
Index                 Entry
= = = = = = = = = = = = = = = = = = =
0                       Hello
1                       World
```

As you will see later in this paper with the [dDrive](#ddrive) Protocol, a distributed file system can be created using a dDatabase, by converting the content and metadata of a file system's files into a particular encoding format, such as binary, and storing in a dDatabase entry like so:
```
Index                 Entry
0                       {content:010110011110...}, {metadata:1110100101...}
1                       {content:11101001111001...}, metadata:0100110000...}
```

In other words, a dDatabase can easily be used to distribute an entire file system between peers, or any dataset for that matter. This section is intended to breakdown how dDatabases work, how data is determined to have the utmost integrity, and how data is exchanged between peers using the dDatabase Protocol.

A dDatabase, represented by a generated dWeb network address, can easily be announced and discovered via dWeb's DHT, where other peers can download the dDatabase from peers in the dDatabase's swarm who have download the data and subsequently announced their joining of the swarm itself. Put another way, a dDrive that contains a website can announce its dWeb network address on the dWeb DHT and peers can join the dDrive's swarm and thereby increase the number of peers the website can be downloaded (replicated) from. This is just an example, as a dDatabase can contain an endless amount of dataset types, such as an encrypted conversation or an encrypted voice call.

#### Distributed, Immutable Data Feed
dDatabase is a secure, distributed and append-only (immutable) feed that is designed for distributing large datasets and streams of real time data between peers of the dWeb.

#### Value Encoding
The value of an append entry to a dDatabase can be encoded into the following formats:

-`binary` (Default)
-`json`
-`utf-8`

#### Data Integrity
It is important that those who download a dDatabase can verify that it actually derived from its proclaimed author. This is made possible thanks to dDatabase's use of public key cryptography, which a dDatabase's dWeb network address is derived from, alongside Merkle trees and FIO trees, in order to verify a dDatabase's root hash signature(s). This insures that the data itself derived from the author of the dDatabase and the original announcer of the dWeb network address.

##### Merkle Trees and FIO Trees
A dDatabase uses a custom encoding method when laying out dDatabase-based update logs into a Merkle tree. This particular encoding method positions hashes into a scheme known as "binary in-order interval numbering," or just "bin" numbering for short. This is a deterministic method of positioning leaf nodes in a Merkle Tree.

A Merkle Tree with seven leaf nodes will always be laid out like this:
```
0L
      1L
2L
            3L
4L
      5L
6L
```

From a computer science perspective, dDatabases are binary append-only streams, the contents of which are cryptographically hashed and signed. Therefore, any dDatabase can be verified by anyone with access to the public key of the creator. Over the HTTP protocol, datasets are shared every day, although there is no built-in support for version control or the content-addressing of specific data. dDatabase is the solution to this, allowing multiple un-trusted devices to act as a single virtual host. For the dWeb to work, it requires a data structure that authenticates the content's integrity and one that keeps a historical log of the revisions - and dDatabase feeds certainly provide that.

dDatabases are identified internally by signed Merkle trees, are identified publicly over the dWeb by a public key, and are discovered over dWeb's DHT by a dWeb network address, which in turn derives from a dDatabase's public key. A dDatabase's public key is used to verify the signature that relates to the received data. A dDatabase's internal Merkle tree is output as a "Flat In-Order Tree" or "FIO Tree."

FIO trees, per PP5P RFC 7574, are defined as "bin numbers." These FIO trees allow for numerical-based identification of each leaf node within a binary-based Merkle tree and therefore create a simplistic way of representing a binary tree as a list. These properties of FIO trees are used in the simplification of "wire protocols" that of which utilize Merkle tree structures within distributed applications.

Below is an example FIO tree, that is sized at 4 blocks of data:
```
0L
1P
2L
3P
4L
5P
6L
```

In the above FIO tree example, even numbers (0, 2, 4, 6) represent leaf nodes on the tree and odd numbers represent parent-nodes that each contain two children.

Using binary notation, we can count the total number of "trailing 1s" to calculate the depth of the tree's nodes. For example, the following numbers below are converted to binary:
```
5 = 101
3 = 011
4 = 100
```

The number 5 has one trailing 1, the number 3 has two trailing 1s and the number 4 has zero trailing 1s. In the FIO example, 1 is the parent node of (0, 2), 5 is the parent node of (4, 6) and 3 is the parent node of (1, 5). The FIO tree would only have a single root if the leaf node count is a power of 2, otherwise a FIO tree will always have more than one root.

Below is another FIO tree (pseudo-representation) with a total of 6 leaf nodes:
```
0
1
2
3
4
5
6
7
8
9
10
```

In the above example, the roots are 3 and 9.

A Merkle tree, named after cryptographer and mathematician Ralph Merkle, is best described as a tree of binary-based data, where every "leaf" (an even-numbered tree node that has no children) is a hash of a data block and every "parent" (an odd-numbered tree node that has two children) is the hash of both of its children. dDatabases are feeds represented by Merkle trees that are ultimately encoded with "bin numbers."

For example, a dDatabase of 4 values would always map to the 0, 2, 4 and 6 leaf nodes. Below is a pseudo-representation:
```
fragment0 -> 0
fragment1 -> 2
fragment2 -> 4
fragment3 -> 6
```

When converting to FIO tree-style notation, a Merkle tree spanning these data blocks looks like:
```
0 = hash(0 + 2)
2 = hash(fragment1)
3 = hash(1 + 5)
4 = hash(fragment2)
5 = hash(4 + 6)
6 = hash(fragment3)
```

The even and odd nodes store different types of information:
-`Even Numbers` - List of data hashes [f0, f1, f2, ...]
-`Odd Numbers` - List of Merkle hashes (hashes of child nodes) [h0, h1, h2, ...]

The root node within a Merkle tree hashes the entire dataset. In the example of 4 fragments, node #3 hashes the entire dataset and node #3 is used to verify the rest of the dataset. Although the root node will change every time data is added to a dDatabase.

```
0
1
2
3 (root node)
4
5
6
7
8
9 (root node)
10
```

The Merkle tree's nodes are calculated as follows:
```
0 = hash(fragment0)
1 = hash(0 + 2)
2 = hash(fragment1)
3 = hash(1 + 5)
4 = hash(fragment2)
5 = hash(4 + 6)
6 = hash(fragment6)
7 = hash (6 + 8)
8 = hash(fragment4)
9 = hash(8 + 10)
10 = hash(fragment5)
```

When there are multiple root hashes, it is convenient to capture the entire state of a dDatabase as a `fixed-size hash` by hashing all of the root hashes into one single hash, where in the example above:

`root = hash(9 + 3)`
or
`tr = h(r1 + r2)`
(tr = top root; hash = hash function; r1 = root hash 1; r2 = root hash 2)

##### Root Hash Signatures
Merkle trees are used by dDatabases to create a way of indentifying the content of a dataset through hashes. The concept is simple, if the underlying content of a dDatabases changes, the hash changes. For example, a dDatabase acts as a list that calls the `append()` mutation when an entry is added to the database feed, thereby adding a new leaf to the tree, which ultimately generates a new root hash. When a dDatabase is created, a public/private keypair is generated. The public key is used as a public identifier for data validation, whereas the private key is used to sign the root hash every time a new one is generated. This digital signature is always distributed with the root hash to verify its integrity.

##### Data Validation
Data which is received that belongs to a dDatabase goes through the following process:
-1. The root hash's signature is verified.
-2. The received data is hashed with `ancestor hashes` in order to reproduce the `root hash`.
-3. If the root hash that was calculated is an exact match of the original root hash that was received, the data has been verified.

In an example of a dDatabase containing 4 values, our tree of hashes would look like this:
```
0
1
2
3 (root hash)
4
5
6
```

If we want to verify the data for 0 (fragment0), we first read (2), which is the sibling hash, then (5), which is the uncle hash and then (3), which is the signed root hash.

```
0 = hash(fragment0)
2 = (hash received)
1 = hash(0 + 2)
5 = (hash received)
3 = hash(1 + 5)
```

If what we calculate for 3 is equal to the signed root hash we received for 3, then fragment0 is valid.

It's important to note that all new signatures very the entire dDatabase since the signature spans all data in the Merkle tree. A dDatabase is considered corrupt if a signed mutation results in a conflict against previously verified trees. When a dDatabase is considered corrupt, the dDatabase protocol is designed to stop distribution.

##### Cryptography Specification
-The hash function uses BLAKE 2B-256 encryption, while signatures are ed25519 with the SHA-512 hash function.
-Hash function inputs are prefixed with different "constants" based on the type of data being hashed. The constants include:
--0x00 -Leaf
--0x01 -Parent
--0x02 -Root

This protects against a "second preimage attack."
-Hashes, a lot of the time, will include the sizes and indexes of their content so that the structure of a tree can be described along with the tree's data.

#### Data Replication
The easiest way to describe how dDatabase replication works between a set of peers using pseudo-code examples, is in the context of a file system - so we will use a dDrive abstraction in the following examples:

Let's say we create a dDrive and add two files to it:
-`jefferson.png`
-`trump.png`

Once those files are added to the dDrive, they are split into fragments, where the content data is represented within a constructed `content` object and the metadata is represented within a constructed `metadata` object, where each file itself is split into multiple dDatabase entries. Assuming `jefferson.png` and `trump.png` are both split into three fragments a piece, that each equate to a file size of 134 Kb, the representation would be placed in a list like this:

```
jefferson1.png
jefferson2.png
jefferson3.png
trump1.png
trump2.png
trump3.png
```

These fragments of the two image files each get hashed and those hashes are organized into a Merkle tree, like the example below:

```
0L                         -hash(jefferson1)
      1L                   -hash(0L + 2L)
2L                         -hash(jefferson2)
            3L             -hash(1L + 5L)
4L                         -hash(jefferson3)
      5L                   -hash(4L + 6L)
6L                         -hash(trump1)
8L                         -hash(trump2)
      9L                   -hash(8L + 10L)
10L                       -hash(trump3)
```

The next part of the process is to calculate the root hashes of the Merkle tree, which are `3L` and `9L`, then hash the number that derives from that. Lastly, we will cryptographically sign the `combined root hash` (sH). The signed hash is utilized to validate all other leaf nodes (parent/child) within the tree, where the signature easily proves who this dDrive was published by.

The pseudo-tree above is for the hashes that derive from the fragments of our two photos. A second Merkle tree is generated as a representation of the files and their metadata, known as a `metadata.log`.

An example of a metadata log-based Merkle tree is below:

```
0L - hash(contentLog: {`4f21f567...`})
1L - hash(0 + 2)
2L - hash({"jefferson.png", first: 0, length: 3})
4L - hash({"trump.png", first: 3, length: 3})
```

A few notes regarding the above pseudo-representation of the metalog's Merkle tree:
-The 1st entry in this log is the metadata entry point to a hash of the content log (the content log that the first Merkle tree represented).
-Notice that the 3rd leaf node is not included yet. That's because 3 is the hash of 1 + 5 and 5 does not exist yet, so it will be written at a later update.

All that's left in the replication process is to send the metadata to the other device (peer). The replicating peer-to-peer messaging protocol, known as the dDatabase Protocol, is tasked with communicating between peers over what is known as a "duplex binary channel." Below is a play-by-play of the communications between two peers, Bob and Alice.

-1. Bob sends the first message, known as the `announce` message that includes a dWeb network address.
-2. Alice sends Bob a `want` message, signaling that Alice wants all leaf nodes in the metadata log of the dDrive that Bob is announcing.
-3. Alice sends three `request` messages, one for each leaf node (0, 2, 4).
-4. Bob sends back three `data` messages. The data messages contain the `contentLog` key, the hash of the sibling, which in this case is (2), the hash of the uncle root (4), and the signature for the root hashes (1, 4).
-5. Alice can validate the integrity of the first data message by hashing the metadata received for the `contentLog` metadata to produce the hash for fragment0. She then hashes the leaf node (0), with the hash (2), that was included to reproduce hash (1) and hashes (1) with the value of (4). Lastly, the digital signature that was received is used in order to verify it was the same dDatabase that was originally requested.
-6. When the next `data` message is received, a similar replication process to the one shown above is performed to verify the content of the dDrive again. Alice now has a full list of the files in the dDrive and can choose which ones she wants to download.

#### dDatabase Data Structure
Data within a dDatabase is stored as `blocks`, which are indentified by an `index`. Each `block` is signed by its creator, so that an entire dDatabase feed can be audited, where all currently stored data matches the hashes in the Merkle tree. Put another way, all data blocks must match the hashes contained within the dDatabase Merkle tree, explained previously.

This data structure ensures that peers can download a specific block range, rather than the entire block base of the dDatabase itself. The `index` is an auto-incrementing number that is zero-based.

A pseudo-representation of a dDatabase feed uses binary encoding:
```
Index              Entry
0                    0111010010011...
1                    1011000110000...
```

As data is appended to the feed, a new index is created. Those who are live replicating the feed would receive the new index in real-time.

#### dDatabase Protocol
Before diving into the dDatabase Protocol, it is important to reiterate that a dDatabase is an append-only data feed, where the data within a data feed is an abstract "blob" of data.

**dDatabase feeds can:**
-Be distributed partially between peers.
-Be distributed fully between peers.
-Be distributed to/received by multiple peers at once.

The dDatabase Protocol is a process by which two or more peers exchange a dDatabase over binary duplex streams. Remote peers are able to identify with seeders of a dDatabase whether they are looking for specific portions of a dDatabase (partial) or the entire dDatabase.

Using various message types, peers are able to ask a specific peer for a specific portion of the feed, whereas the feed distributor(s) can signal whether they have that data and can subsequently fulfill the data request. The protocol uses a sender/handler lifecycle for message exchange and message handling.

Binary duplex streams can be encrypted, use a public/private keypair for stream authentication, and utilize a NOISE-based stream handshake by default.

This section will cover all of these aspects, including dDatabase's handshake phase and message exchange phase.

##### Handshake Phase
A dDatabase protocol duplex stream can use an optional NOISE-based handshake. For the handshake, NOISE uses the `XX` pattern. Each NOISE message is sent with variant framing. After the handshake is finalized between peers, a message exchange phase can begin.

The handshake phase allows two or more peers to authenticate each other and to securely negotiate an encryption and MAC algorithm, along with cryptographic keys to be used to protect the data sent between them. After the initial handshake transport, encryption is enabled to ensure a stream is private.

As an added plus, each NOISE session is unique and is identified by a unique `handshake hash` in order to enable channel binding.

##### Message Phase
The NOISE-based message exchange phase uses a basic variant length prefixed format to send messages over the wire.

All messages contain a header indicating the type and the feedID of the dDatabase feed, and a protobuf-encoded payload:

`message = header + payload`

A header is a variant that looks like this:

`header = feedID << 4 | numeric-type`

The `feedID` is just an incrementing number for every feed shared and the `numeric-type` corresponds to which protobuf schema should be used to decode the payload.

The message is then wrapped in another variant containing the length of the message:
`wire = length(message) + message + length(message2) + message2 + ...`

A good example of how the protocol functions can be found in [Data Replication](#data-replication).

###### Message Types
There are several message types that can be used to open a channel, request data and send data. Each of the below message types correspond with a specific [Message Handler](#message-handlers). The following message types are available:

###### `open` Type
Opens a channel and signals the other end that you are sharing a dDatabase feed. Accepts a `key` parameter, which is the dDatabase public key. Before being hashed, the public key of the dDatabase is hashed and sent as the dWeb network address for any connecting peers. This protects the dDatabase's public key from being learned by remote peers who do not already posses it. Also includes a cryptographic proof that the local possesses the public key, which can be implicitly verified by using the [removeVerified API](#removeverified-api).

Also accepts an object of functions, as a parameter, for handling incoming messages, using [Message Handler](#message-handlers).

###### `close` Type
Signals to the other side that the sender does not have the key corresponding to the dWeb network address.

###### `destroy` Type
Destroys the stream and closes all dDatabase feeds as well.

###### `finalize` Type
Gracefully end the stream and close all dDatabase feeds.

###### `options` Type
Sends an `options` message on a channel.

###### `status` Type
Sends a `status` message on a channel. This message is intended to indicate state changes.

###### `have` Type
Sends a `have` message on a channel, to see if the local can fulfill a particular request. Should reference the available index range (starting index and feed length). This should be in response to a `want` message. Feed length can be defined as `Infinity` for live replication.

###### `unhave` Type
Sends an `unhave` message on a channel, indicating an index range that is no longer available.

###### `want` Type
Sends a `want` message on a channel, indicating a specific index range that is wanted.

###### `unwant` Type
Sends an `unwant` message on a channel, indicating a specific index range that is no longer wanted.

###### `request` Type
Sends a `request` message on a channel, in order to request a specific piece of data from the dDatabase feed.

###### `cancel` Type
Sends a `cancel` message on a channel, in order to cancel a `request`.

###### `data` Type
Sends a `data` message on a channel, in order to fulfill a `request`. Includes a digital signature, accompanied by a Merkle tree representation of the data.

For more information on each of these messages, see the [dDatabase Protocol ProtoBuf Schema](https://github.com/distributedweb/ddatabase-protocol/blob/patriot1/schema.proto).

###### Message Handler
Message Handlers are triggered after specific [Message Types](#message-types) are received. The following handlers can be used:

###### `onopen`
Triggered after receiving an `open` message.

###### `onoptions`
Triggered after receiving an `options` message.

###### `onhave`
Triggered after receiving a `have` message.

###### `onunhave`
Triggered after receiving an `unhave` message.

###### `onwant`
Triggered after receiving a `want` message.

###### `onunwant`
Triggered after receiving an `unwant` message.

###### `onrequest`
Triggered after receiving a `request` message.

###### `oncancel`
Triggered after receiving a `cancel` message.

###### `ondata`
Triggered after receiving a `data` message.

#### The dWeb Lifecycle
It is crucial to understand how DWDHT and dDatabase can be used in tandem and how they both, when layered on top of one another, create a truly decentralized data transfer network, which help form the dWeb. Below is an example of what I call the `dWeb Lifecycle` - how data is created, announced, discovered by a remote, downloaded and then re-announced by the remote.

-1. Bob creates a dDatabase, and appends "Hello" to the feed, at index 0.
-2. Bob opens a channel to share his dDatabase, and generates a dWeb network address for the dDatabase in the process.
-3. Bob announces the dWeb network address, along with his public IP address and the port number peers can connect with him on, via dWeb's DHT.
-4. Alice learns of Bob's network address and wants to access it, so Alice performs a lookup for Bob's network address on dWeb's DHT.
-5. The DHT returns a list of peers, which for now only includes Bob's public IP address and port number.
-6. Using the dDatabase protocol, Alice sends a `request` message for the entire index range to Bob, using the connection details found on the DHT. This also creates a binary duplex channel between Alice and Bob.
-7. Bob responds with a `data` message, with all the indices from his dDatabase.
-8. Alice chooses to keep the channel open, so that she can listen for updates to Bob's dDatabase, which she can now receive in real-time.
-9. Alice now announces Bob's dDatabase (its dWeb network address) on dWeb's DHT, adding her public IP and port to the swarm now surrounding Bob's dDatabase.
-10. Jim comes along and learns about Bob's dDatabase and performs a lookup of its network address on dWeb's DHT.
-11. dWeb's DHT returns a list of peers who are announcing (broadcasting) Bob's dDatabase (now Alice and Bob), from whom Jim can now download the dDatabase. All Jim has to do is form a `request` message, detailing the `index` range he'd like to download.

### dWebTrie
dWebTrie is an abstraction layer that utilizes [Hash Array Mapped Tries](https://en.wikipedia.org/wiki/hash_array_mapped_trie) in order to provide a general purpose, distributed key/value store over the [dDatabase Protocol](#ddatabase). dWeb is a single writer key/value store which is able to map key/value data to a matching dDatabase index using a builtin rolling hash array mapped trie.

A dWebTrie utilizes the underlying dDatabase's public/private key pair, as well as its dWeb network address, and is represented by this single dDatabase feed. A [dAppDB](https://github.com/distributedweb/dappdb) is a multiwriter distributed key/value store that can be represented by multiple dDatabase feeds and therefore multiple key pairs, although its dWeb network address is derived from the dAppDB's initial dDatabase feed's public key.

dWeb was created as a way of storing an entire file system, with possibly thousands or even millions of files, within a dDatabase, where keys are path-like strings (e.g., /make/the/web/great/again) and values are arbitrary binary blobs. Without the dWeb abstraction, a distributed file system like [dDrive](#ddrive) simply wasn't possible, since there would have been too many complexities and bottlenecks if a protocol like dDrive would have used dDatabase directly. Optionally, we could have built a trie-structured key/value API within a dDrive implementation, but that would have been far too much overhead. It made more sense to develop a separate trie-structured key/value API, layered on dDatabase, so that many other protocols and applications could use dWeb as an off-chain distributed database management system.

#### Database Semantics
-Keys can be any UTF-8 string (e.g. "maga")
-If a key uses path segments (like a folder or file location) (e.g., /this/folder) path segments must be separated by the forward slash character ( /). Repeated slashes (//) are not allowed.
-Leading and trailing (/) are options (e.g., "/hello" and "hello" are equal).
-A key can be both a path segment and a key at the same time (e.g., /a/b/c and /a/b can both be keys at the same time).
-Values can be any binary blob, including an empty blob (zero bit length).
-Acceptable values can be UTF-8 encoded strings, JSON encoded objects, protobuf messages, or a raw uint64 integer (endianness does not matter).
-Length of value (in bits) is the only form of type or metadata stored about the value.
-Deserialization and validation are left to library and application developers.

#### dWebTrie Representation
Below is a pseudo-representation of a dWebTrie database:
```
Key                    Value
= = = = = = = = = = = = = = = = = = = =
Name                Jared Rice Sr.
Location            Dallas, TX
```

While this database is truly logical, since it is actually just a bunch of binary-based blob entries within a dDatabase feed, using dWeb's abstract layer, one could easily execute `get(name)`, which would return `Jared Rice Sr.` This is far simpler than working directly with the dDatabase feed, which would require farm more steps and much more programming overhead.

#### Database API
A dWebTrie-based database is created by opening an existing dDatabase feed with dWebTrie content or by simply creating a new dDatabase feed.

The following API calls are part of the dWebTrie standard:

##### `put(key, value)`
Inserts a value of an arbitrary byte size under the specified key. Requires read-write access. Returns an error via callback if there is an issue.

##### `get(key)`
Retrieves the value for a given key.

##### `delete(key)`
Retrieves the value for a given key.

##### `list(prefix)`
Returns a flat (non-nested) list of all keys currently in the database under the given prefix.

##### `batch(batch)`
Insert/delete multiple values atomically.

##### `watch(prefix)`
Watch a prefix of the DB (or key) and get notifications when it changes.

#### Overhead and Scaling
Depending on the size of the database, the metadata overhead can vary.

Consider the case of a two-path-segment key, with an entirely saturated trie and a uint32-sized feed and entry index points:

-`trie:` 4 * 2 * 64 bytes = 512 bytes
-`total:` 512 bytes

In a light-case, with few trie entries and single-byte varint feed and entry index pointers:
-`trie:` 2 * 2 * 4 bytes = 16 bytes
=`total:` 16 bytes

For a database with most keys having `N` path segments, the cost of a `get()` scales with the number of entries `M`, as `O(log(M))` with the best case `1` lookup and the worst case `4 * 32 * N - 128 * N` lookups.

The cost of `put()` or `delete()` is proportional to the cost of `get()`. The cost of `list()` is linear `(O(M))` in the number of matching entries, plus the cost of a single `get()`.

The total metadata overhead for a dWebTrie-based database with `M` entries, scales with the `O(M log(M))`.

#### dWebTrie Schemas
A dWebTrie-based dDatabase feed consists of a sequence of protobuf-encoded message of `Entry` or `InflatedEntry` type. A "protocol header" header entry should be the first entry in the feed, using "dWebTrie" as the `type`. dWebTrie itself does not specify the content of the optional header extension field, as higher level protocols are left to handle this portion of the protocol.

When data doesn't fit, due to the limited size constraint, for any given value, there is a second `content` feed associated with the dWebTrie key/value feed. The optional `contentFeed` field described in the schema below is used to identify a dWebTrie key/value feed, that needs a second `content` feed.

The sequence of entries includes an incremental index, where the most-recently appended entry in the feed contains metadata pointers that can be followed to efficiently locate any key in the database, without having to perform a linear scan across the database's entire history, or generate an index data structure that's independent of the database itself. Although it is completely up to the implementation on whether they choose to implement their own index or not.

The protobuf message schemas for `Entry` and `InflatedEntry` are:
```
message Entry {
  message InflatedEntry
  required string key = 1;
  optional bytes value = 2;
  optional bool deleted = 3;
  required bytes trie = 4;
  repeated uint64 clock = 5;
  optional uint64 inflate = 6;
  repeated Feed feeds = 7;
  optional bytes contentFeed  = 8;
}
```

The fields that are common to both message types are:
-`key` - UTF-8 key that this node describes. All slashes are removed before storing in message.
-`value` - Byte array of an arbitrary size.
-`deleted` - Boolean that converts entry into a "dead" entry, if `true`. It is recommended that if `false` to keep this value `undefined`, rather than using `false`. This can also be used to store user-defined metadata related to the deletion.
-`trie` - A structured array of pointers to other `Entry` entries in the dDatabase feed. This is used for navigating the tree of keys.
-`inflate` - A reference to the feed index number of the most recent `InflatedEntry` entry in the feed. **NOTE:** This should not be set on a feed's first `InflatedEntry` entry.
-`contentLog` - For applications that require a parallel `content` dDatabase feed. This filed is used to store the 43-byte public key for that feed. It is sufficient to write a single `InflatedEntry` message in the dDatabase feed, with feeds containing a single entry (a pointer to the current feed itself) and `ContentLog` optionally set to a pointer to a paired content feed. The `Entry` type can be used for all other messages, with `inflate` pointing back to the single `InflatedEntry` message.

#### Key Path Hashing
Every key path has a fixed-size hash representation that is used by the trie. When all path segments are concatenated, they are combined into what is known as a `path hash array`. Similar in many ways to that of a `hash map data structure`, a `path hash array` can have collisions where one key (string) and another key have the same hash, without suffering any issues because of it. This is because each hash is a reference (pointer) to a linked-list `container` of Entries, which can be linearly iterated over until the sought after value is found.

Each element (segment) in a path equates to 32 values, which also equates to a 64-bit hash. The key `/presidents/trump` has two path segments (presidents and trump) which equates to a 65  element path has array, made up of 32 element hashes and a terminator.

The `SipHash-2-4` hash algorithm is used, along with an 8-byte output and a 16-byte key. The corresponding input is the UTF-8 encoded path string segment, excluding slashes or other separators, as well as any terminators. A 16-byte secret key is required where all zeros is used, for this particular use-case. When an 8-byte outputted hash is converted to an array of 2-bit bytes, the ordering of the hash array is handled byte-by-byte, where for each byte, take the two lowest-value bits as `byte index 0` in the hash array and the next two bits as `byte index 1`, etc. When path hashes are combined into an array of greater length, the left-most path element hash will relation to byte indices `0` to `31`. The terminator, `4`, will have the highest index in the hash array (right-most).

**Note:** dWebTrie derived from a project known as HyperDB and the following examples have been lifted from [DEP-0004[(https://datprotocol.com/deps/0004-hyperdb), which was the original specification/proposal for HyperDB. A special thanks to the incredible work of Mathias Buus, Bryan Newbold and Stephen Whitmore.

-For example, consider the key `/tree/willow`. `tree` has the following hash:
`[0xAC, 0xDC, 0x05, 0x6C, 0x63, 0x9D, 0x87, 0xCA]`

-This converts into the following array:
`[0,3,2,2,0,3,1,3,1,1,0,0,0,3,2,1,3,0,2,1,1,3,1,2,3,1,0,2,2,2,0,3]`

-`willow` has a 64-bit hash:
`[0x72, 0x30, 0x34, 0x39, 0x35, 0xA8, 0x21, 0x44]`

-This converts into the following array:
`[2,0,3,1,0,0,3,0,0,1,3,0,1,0,3,0,1,1,3,0,0,2,2,2,1,0,2,0,0,1,0,1]`

-These two combine into the unified byte array with 65 elements:
`[0,3,2,2,0,3,1,3,1,1,0,0,0,3,2,1,3,0,2,1,1,3,1,2,3,1,0,2,2,2,2,0,3,2,0,3,1,0,0,3,0,0,1,3,0,1,2,3,0,1,1,3,0,0,2,2,2,1,0,2,0,0,1,0,1,4]`

In another example, the key `/a/b/c` converts into a 96-byte hash array, i.e., `32 + 32 + 32 + 1`. (1) in this case the terminator bit (4).

##### Incremental Index Trie
Each individual node stores a prefix trie that can be used to lookup other keys and can also be used to list all keys that are related to a given prefix. The prefix trie is stored in the `trie` field within an `Entry` message, as referenced in [dWebTrie Schema(#dwebtrie-schema).

Simply put, the `trie` is equal to the `path hash array`. As mentioned in the schema, each individual element within a `trie` is referred to as a `container`. Each container that isn't empty, is a pointer to the newest `Entries`, where the path is equal (identical) up to that specific prefix location. This is true, because each `trie` has 4 values at each node, which means there can be a pointer to up to 3 other values at a given element in the trie array. (Containers can be empty, if at that particular node, there are zero `branches`).

**NOTE:** Only non-null elements will be transmitted as stored on disk.

The trie data structure is a sparse array of pointers to other `Entry` entries. Each pointer references a specific feed, which indexes to the same value.

###### Looking Up A Key In A Database
The process for key lookups, is to:
-1. Calculate the `path hash array` for the key you are looking for.
-2. Select the most recent ("latest") `Entry` in the feed.
-3. Compare `path hash arrays` for exactly matching paths.
-4. If they match exactly, then compare keys. If keys match, the lookup was successful.
-5. Check whether the `deleted` flag of the `Entry` schema is set. If so, this entry actually represents the deletion of the `Entry`.
-6. If the path segments (concatenated) match, look for a pointer in the last `trie array index` and iterate from step #3 with the new `Entry`.
-7. If the path segments (concatenated) do not match, find the first index in each `path hash array` where both arrays differ and look up the corresponding element in this `Entry's` trie array.
-8. If the element is empty, or doesn't contain a pointer corresponding to your 2-bit value, then the key does not exist in the dWebTrie.
-9. If the trie element is not empty, then follow that pointer to select the next `Entry`. Recursively repeat this process from step #3, which will allow you to descend the trie in a search where the search will either terminate in your search or find that the key is not defined in the dWebTrie.

###### Writing A Key To A Database
In order to write a key to a dWebTrie database, follow the specified process below:
-1. Calculate the `path hash array` for the key to be stored and start with an empty trie array of the same length; where writing to this trie array will start at the current index of `0`.
-2. Select the most-recent ("latest") `Entry` in the feed.
-3. Compare `path hash arrays` for exactly matching paths.
-4. If they match exactly, then compare keys. If keys match, then you are overwriting the current `Entry` and can copy the `remainder` of its trie up to the current trie index. **NOTE:** When I say "overwriting", the previous `Entry` is not removed, for the simple reason that the dDatabase feed below the dWebTrie is immutable and append-only. A more-recent `Entry` is created with its own schema, which could, for example, set the `deleted` flag to true, which would then flag this `Entry` as deleted for future lookups. This works because in immutable datasets, where the management of state is paramount, we can see the entire lifetime of the data, from the moment it was created to the moment it was deleted. It's also important to note that during the lookup process, by selecting the "most-recent" `Entry`  for a key in the database, we are pulling its most-recent state.
-5. If the path segments (concatenated) match, but not the keys, then you are adding a new key to an existing `hash container`. Copy the trie array and extend it to the full length and add a pointer at the last index of the array of the same hash as the `Entry`.
-6. If the path segments (concatenated) do no match, compare both trie arrays and find the differing portion of the array. Copy all of the elements of the trie array, between the `current index` and the `diff index`, into a new trie array. **NOTE:** It doesn't matter whether the located trie array is empty or not.
-7. In this `Entry's` trie array, look up the corresponding element at the `diff index`. If empty, then the most similar `Entry` has been located.
-8. Create a pointer to this node, to the trie at the `diff index`, and the write process is finished. **NOTE:** All remaining trie elements will be empty and can be removed.
-9. If the `diff index` has a pointer, this means it isn't empty. If a pointer exists, follow that pointer to the next `Entry`. Recursively repeat this process from step #3.

**NOTE:** To delete a key/value, follow the same write procedure above and set `deleted` to `true` in the `Entry`. `Deletion nodes` will persist in the database forever.

##### Trie Encoding
Trie data structures are encoded into a variable length byte string as the `trie` field of an `Entry` message. It's important to reiterate that trie data structures are simply sparse, indexed arrays of pointers to entries.

**NOTE:** The following encoding schema and examples were also lifted from `DEP-0004`; only `buckets` in a dWebTrie are referred to as `containers`.

Consider a trie array with `N` `containers` and `M` non-containers `(O <= M <= N)`. In the encoded trie field, there will be `M` concatenated bytestrings in the following form:

`trie index (varint) | bucket bitfield * (packed in varint) | pointer sets **`

- * - Bitfield encodes which of the 5 values (4 values if the index is not mod 32) at this node of the trie have pointers.
- ** - Pointer sets each reference an entry at (feed index, entry index), where `feed index` is a varint with an extra "more pointers at this value" low bit, encoded as `feed_index << 1 | more_bit` and where `entry index` is simply a varint.

When dealing with a small/sparse dWebTrie, there will be a small number of non-empty containers; put another way, a small/sparse dWebTrie will have a small `M`. For a very large/dense dWebTrie (millions of key/value pairs), there will be many non-empty containers; put another way, `M` will approach `N` and containers may have up to the full 4 pointer sets.

-Consider an entry with a path hash:
`[1,1,0,0,3,1,2,3,3,1,1,1,2,2,1,1,1,0,2,3,3,0,1,2,1,1,2,3,0,0,2,1,0,2,1,0,1,1,0,1,0,1,3,1,0,0,2,3,0,1,3,2,0,3,2,0,1,0,3,2,0,2,1,1,4]`

-and the `trie`:
`[, ]`

In this case, `N = 64` (or you could count as 2, if you ignore trailing empty entries) and `M = 1`.

There will be a single bytestring fragment (chunk):

-`trie index` varint = 1 (second element in trie array).
-`bitfield` (varint 2) = 0b0010
-There is only `pointer set` for value `1` (the second value).
-There is a single pointer in the `pointer set` which is: `feed = 0 << 1 | 0, index = 1` or `(varint 2, varint 1)`.
-Combined, the `trie bytestring` will be:
`[0x01, 0x02, 0x02, 0x02]`

For a more complex example, consider the same `path hash array` with the `trie array`:
`[,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,]`

 If we `db.put ('/a/b', '24')`, we expect to see a single `Entry` of the `InflatedEntry` type at `index1` of the underlying dDatabase feed.

For reference, the first 104 bytes in the path match those of  the `/a/b/c` example from earlier, because it shares two of its 3 path segments. Since this is the second entry, the `entry index` is `1`.

**NOTE:** There is a major different between an `entry index` and an `array index`. An `entry index` is a record (new index) added to the dWebTrie's underlying dDatabase, while a specific index within a trie array, is its position from within the array. To make sure that I kill all confusion, consider the below example:

The example of `/a/b` being `put` into the key-value store, if data was in plain English and not binary, would look something like this pseudo-representation, once inside a dDatabase feed:

```
Index                           Value
= = = = = = = = = = = = = = = = = = = = = = = = = = =
1                                 {
                                     type: dwebtrie,
                                     key: Entry,
                                     value: 24,
                                     deleted: null,
                                     trie: [0x01, 0x02, 0x02, 0x02],
                                     ...
                                   {
```

The placement (index) with a dDatabase and the placement of an element in the above trie array, are clearly apples and oranges and I did not want there to be any confusion.

Now, if we `db.put('/a/c', 'hello')` and expect a second `Entry` of `Entry` type, the `path hash array` for this key (index 2) is:
`[1,2,0,1,2,0,2,2,3,0,1,2,1,3,0,3,0,0,2,1,0,2,0,0,2,0,0,3,2,1,1,2,0,1,1,0,1,2,3,2,2,2,0,0,3,1,2,1,3,3,3,3,3,3,0,3,3,2,3,2,3,0,1,0,4]`

The first 32 characters of this path are common with the first `Entry` (they share a common prefix, `/a`). `trie` is defined, but is mostly sparse. The first 32 elements of common prefix match the first `Entry` and then two additional hash elements ([0,1]) happen to match as well. There is not a `diff index`, until `index 34` of the array (zero-indexed). At this entry, there is a reference pointing to the first `Entry`. An additional 29 trailing null entries have been trimmed in the reduced metadata overhead.

Next, we insert a third node with `db.put('/x/y', 'other')`, and get a third `Entry`, where the `path hash array` is (index 3) is:
`[1,1,0,0,3,1,2,3,3,1,1,1,2,2,1,1,1,0,2,3,3,0,1,2,1,1,2,3,0,0,2,1,0,2,1,0,1,1,0,1,0,1,0,3,1,0,0,2,3,0,1,3,2,0,1,3,2,0,1,0,3,2,0,2,1,1,4]`

Consider the lookup process for `db.get('/a/b')`, which we expect to return `24`, as written in the first `Entry`. First, we calculate the path for the key `a/b`, which will be the same as the first `Entry`. Then we take the "latest" (most-recent) Entry, with entry `index 3`. We compare the `path hash arrays`, starting at the first element, and find the `diff index` at `index 1` of the array `(1 == 1, then 1 ! = 2)`.

We look at `index 1` in the current `Entry's` trie, and find a pointer to the entry at `index 2` of the underlying dDatabase itself, so we fetch the `Entry` and recurse. Comparing `path hash arrays`, we now get all the way to `index 34` of the array before there is a difference (`diff index`). We again look at the trie, find a pointer to the entry at `index 1` of the underlying dDatabase and fetch the first `Entry` and recurse. The path elements of the entry at `index 1` of the feed (dDatabase feed) match exactly and therefore, we have found the entry we are looking for and it has an existing value, so we return the value, which is `24`.

Lastly, consider a lookup for `db.get('/a/z')`. This key does not exist, so we expect dWebTrie to return with `key not found`. We calculate the `hash path array` for this key:
`[1,2,0,1,2,0,2,2,3,0,1,2,1,3,0,3,0,0,2,1,0,2,0,0,2,0,0,36,2,1,1,2,1,2,3,0,1,0,1,1,1,1,1,2,1,1,1,0,1,0,3,3,2,0,3,3,1,1,0,23,1,0,1,1,2,4]`

Similar to the first lookup, we start with the `entry index` 3, and follow the pointer to `entry index` 2. This time, when we compare `path hash arrays`, the first `diff index` is at `array index` 32. There is not a trie entry at this index, which tells us that the key does not exist in the database.

###### Listing A Prefix
Continuing on with the current state of the dDatabase below dWebTrie abstraction layer, we call `db.list('/a')`, to list all keys with the prefix `/a`.

We generate a `path hash array` for the key `/a` without the terminating symbol (4):
`[1,2,0,1,2,0,2,2,3,0,1,2,1,3,0,3,0,0,2,1,0,2,0,0,2,0,0,3,2,1,1,2]`

Using the same process as the `get()` lookup, we find the first `Entry` that entirely matches the prefix, which is `entry index` 2. If we had failed to locate any `Entry` with a complete prefix match, then we would return an empty list of matching keys. If we start with the first prefix-match node, we save that key as a match (unless `deleted` is `true` in the `Entry`). Then, select all trie pointers with a higher index than the length of the prefix and recursively inspect all pointer-to `Entries` and the `Entries` they point to and so on, until a complete list tree is built.

#### The Baseline Foundation For A Distributed File System
For a server-less web like the dWeb to work, it would have to find a way to not only transfer data between peers, but to transfer entire files and therefore, entire file systems between peers, just as servers do when teamed with the HTTP protocol on a server-to-client basis. Doing this on a peer-to-peer basis, as the dWeb has accomplished, not only requires a protocol for exchanging index abstract data blobs between peers (dDatabase), but another protocol (dWebTrie) layered above this for creating a file system-like logical and persistent database structure, which utilizes a key-value format, so that file-paths (keys) can be appended to a blob, with file data (values) and efficiently traversed by receiving peers performing specific lookups.

This way, an entire file system can be stored within a single file (a single dDatabase feed) and any peer in the world who is able to locate the feed via dWeb's DHT, can then communicate with the peers within dDatabase's swarm, request the dDatabase and subsequently download the dDatabase to their machine and easily consume the data via higher-level abstractions (like dWebTrie, or even higher levels like dDrive's actual file system layer).

dWebTrie was designed to store the data that derives from an entire file system within a dDatabase and allow higher-level abstractions like [dDrive](#ddrive) to consume that data and reproduce the entire file system on a remote peer's device. dWebTrie can traverse millions of records within a dDatabase and relate specific path segments via `list()`, without any bottlenecks. This makes dWebTrie the perfect file system database for a distributed file system, considering it is built on top of a distributed, append-only data feed that can be exchanged between peers in real-time. I can't make the rationale for dWebTrie as a file system for distributed file systems any clearer, other than to say that our current dWebTrie implementation scales with `O(N log(N))`.

###dDrive
dDrive is a secure, real-time distributed file system abstraction layer on top of dWebTrie, designed for the real-time exchange of file systems on a peer-to-peer basis. When comparing the dWeb to the World Wide Web, one could easily arrive at the conclusion that a dDrive is the equivalent to a server on the World Wide Web for some use-cases, like handling a client's request for a specific file. This is somewhat true, since a dDrive can be discovered on dWeb's DHT and a request can be sent to the peer(s) in its underlying swarm for specific files, or the entire file system for that matter - although the process of doing this on the dWeb is more secure and far more efficient.

**NOTE:** a dDrive cannot replace a server and its ability to act as a backend for both user authentication and remote application execution (currently, at the time of this writing). Web applications within a dDrive can utilize blockchain-based protocol suites like dWeb's [ARISEN](#arisen) to build truly serverless and decentralized applications that do require a backend and/or user authentication.

Just as I noted within my explanation of the dWebTrie, dDrive is also completely logical; in other words, it doesn't technically exist. A dDrive is an abstraction for storing files from a file system as dWebTrie-based `Entries` within a dDatabase and reproducing the actual file system and its files from the same `Entries`.

Consider the following pseudo-representation of how files in a dDrive are logically referenced via a dWebTrie and actually stored within a dDatabase:

-Alice creates a dDrive with her website files:
```
index.html
img/logo.png
```

-dDrive creates the following key-value entries into its underlying dWebTrie (put):
```
Key                              Value
= = = = = = = = = = = = = = = = = = = = = = = = = =
/index.html                 <html><head>...
/img/logo.png             8fe1e4... (image file converted to hexadecimal notation)
```

-dWebTrie then creates a dDatabase and the following entries:
```
Index                           Entry
= = = = = = = = = = = = = = = = = = = = = = = = = =
1                                 {
                                     type: dwebtrie,
                                     message: Entry,
                                     key: /index.html,
                                     value: <byte array of file data>
                                     deleted: null,
                                     ...
                                   }

2                                 {
                                     type: dwebtrie,
                                     message: Entry,
                                     key: /img/logo.png,
                                     value: <byte array of file data>,
                                     deleted: null,
                                     ...
                                   }
```

**NOTE:** The first entry in the feed (index 0), as noted in [dWebTrie Schema](#dwebtrie-schema) is a special protocol header entry, which is not shown in the above example in order to keep all pseudo-representations simple.

-dDatabase then encodes both entries in binary notation and stores them in the feed:
```
Index                           Entry
= = = = = = = = = = = = = = = = = = = = = = = = = =
0                                 01011101101010...
1                                 00101001110101110...
2                                 0101101011011001...
```

#### dWeb Network Address
Since a dDrive is simply a dDatabase, it uses the dDatabase's public/private keypair and therefore its dWeb network address.

Remember, a dWeb network address is simply a 32 byte hexadecimal hash and in the case of a dDrive, its a 32 byte hexadecimal hash of its underlying dDatabase's public key.

The following is an example of a dWeb network address:
`40a7f6b6147ae695bcbcff432f684c7bb529lea339c28c1755896cdeb80bd2f9`

#### dDatabase Protocol Headers
You will recall in [dWebTrie Schema](#dwebtrie-schema) that the first entry in its underlying dDatabase, contains a `protocol header`. Protocol headers at `index 0` are used to decipher what the proceeding entries in the dDatabase are related to.

Below are the fields included in a dDatabase's `protocol header`:

| Field No. | Name | Type | Description |
| --- | --- | ---- | --- |
| 1 | Type | Length-prefixed | The type of data in the dDatabase |
| 2 | Content | Length-prefixed | 32-byte public key of content feed (if type is `ddrive`) |

If the `type` is `dwebtrie`, then we know there is a single dDatabase feed and that the overlaying dWebTrie abstraction layer has formatted the data as a key-value store. If the `type` is `ddrive`, then we know there is a separate `content` feed (as explained in the next section [Content and Metadata Feeds](#content-and-metadata-feeds) ), whose 32-byte public key can be found in the `Content` field of the feed's header.

The `Content` field allows two or more feeds to be coupled together, while the overall `protocol header` is used to distinguish the data set type within an underlying dDatabase. This simple header, as you will see later in this paper, is giving way to the creation of many decentralized technologies, outside of just a decentralized web, like the dWeb.

That brings me to dDrive's coupling of dDatabase feeds.

#### Content and Metadata Feeds
Contrary to the pseudo-representation that was shown in the previous section, a dDrive doesn't use a single dDatabase, it uses two coupled feeds to represent files and folders. One feed is for metadata and the other represents files and folders. Put another way, one feed is for file metadata and the other feed is for the data related to files. The `metadata` dDatabase feed contains the names, sizes and other metadata for each file, and is typically sparse, even when the overlaying dDrive contains a large amount of files and folders. The `content` dDatabase feed contains the actually file contents and is of the plain "ddatabase" type, instead of the `dwebtrie` or overlaying `dDrive` type.

When a dDrive is created, dWebTrie is initiated and creates two underlying dDatabase feeds. In the protocol header of the `metadata` feed, the `Type` field is sent to `ddrive` and the `Content` field is set to the 32-byte public key of the `content` feed.

When a file is added to a dDrive, a dWebTrie `Inflated Entry` is created for the file metadata in the `metadata` feed and the actual file data is stored as an arbitrary data blob within the `content` feed. If you recall in the [dWebTrie Schema](#dwebtrie-schema), an `Inflated Entry` contains the `contentLog` field, to point to the location in the `content` dDatabase feed for a given dDrive, where the file contents, related to the `metadata` `Inflated Entry`, are located. Put another way, each `metadata` feed array points to where in the `content` feed the file data is located, so you only need to fetch the contents of files you are interested in.

Here is an updated pseudo-representation of a dDrive with a simple `index.html` file, so you can update your mental model of how a dDrive is actually constructed:

-A blank dDrive is created, where dWebTrie initiates the creation of a blank dDatabase for the `content` feed, and a blank dDatabase feed for the `metadata` feed.

-The following `protocol header` is added as the first entry in the `metadata` feed:
```
Index                            Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                  {
                                      Type: dDrive,
                                      Content: <public-key-of-content-dDatabase>,
                                    }
```

-A file called `index.html` is added to the dDrive and the following entries are added to the `content` and `metadata` feeds:
```
Metadata Feed:
--
Index                            Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                  { // Protocol Header
                                      Type: ddrive,
                                      Content: <public-key-of-content-database>
                                    }
-- newly added entry:
1                                  { // Metadata Inflated Entry
                                      key: /index.html,
                                      value: metadata-for-file,
                                      deleted: null,
                                      trie: <pointers to other files or folders where the path is created> (none in this case),
                                      contentLog: <index number in content feed, where file data is> (1)
                                    }
```
```
Content Feed
--
Index                            Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                  { // Protocol header
                                      Type: ddatabase
                                    }

1                                  010110... (Binary blob of index.html file content)
```

-In actuality, both dDatabases are truly in binary when it's all said and done:

```
Content Feed:
--
Index                             Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                   010110111...
1                                   1101110001...
```
```
Metadata Feed:
--
Index                             Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                   01011101110...
1                                   10110111011...
```

There are many reasons the `content` feed doesn't use the dWebTrie abstraction layer/format for appending data, one of which is the limited value constraint of the `value` field in a dWebTrie's `InflatedEntry`. Other reasons include the fact that there is no need for pointers via the `trie array`, since related path segments are sorted via the `metadata` feed where a `trie array`, via each `Inflated Entry's` `trie` field, can be found.

It's actually quite simple to build a tree of files and folders via the `metadata` feed by relating `path segments`. After a tree is calculated, thanks to the location of the `content` feed in the protocol header and the `contentLog` field in each `Inflated Entry`, the contents of a file can easily be located.

#### Metadata Value Field Schema
In the previous pseudo-representation of how a dDrive is created, you will notice that in the `Inflated Entry`, within the `Metadata` feed of the `index.html` file, the `value` field is where the metadata is located. This field carries its own schema, as follows:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Mode | uint32 (varint) | Unix permissions (setuid or setgia) |
| 2 | UID | uint32 (varint) | User ID (Unix) |
| 3 | GID | uint32 (varint) | Group ID (Unix) |
| 4 | Size | uint64 (varint) | File size in bytes |
| 5 | Blocks | uint64 (varint) | The number of object blocks |
| 6 | Offset | uint64 (varint) | Similar to Node's FS module |
| 7 | ByteOffset | uint64 (varint) | Similar to Node's FS module |
| 8 | MTIME | uint64 (varint) | Modified time |
| 9 | CTIME | uint64 (varint) | Created time |

This is packaged within a `protocol Buffers` encoded message and used as the value `field` within a dWebTrie-based `InflatedEntry` message, which is also encoded with `Protocol Buffers` and appended as an entry in the `metadata' feed.

#### Version Controlled
As mentioned in [dDatabase](#ddatabase), files transferred over `HTTP` are not versioned, which means the historical state of a file or files is not available. `HTTP` is used to transfer a file's or files' current state from the server it/they exist on, to the client they're being requested from. When it comes to the `DWEB` protocol and its underlying protocols, the underlying data is [immutable](#the-case-for-immutability), which means that any rendition of the underlying data's state can be requested and constructed. In the case of a dDrive, each time a file is added to a dDrive or the metadata/content of a file is modified, entries are made to its underlying dDatabase feeds, where each entry represents an instance of the dDrive's state.

Each modification to a dDrive represents a new version of the dDrive. String names can also be assigned to dDrive versions and these can be stored within the drive itself, creating a straightforward way to switch between semantically meaningful versions. Simply put, a dDrive is built on top of append-only logs where old versions of files are preserved by default. You can get a read-only snapshot of a specific version of a dDrive at any time. Additionally, as mentioned above, you can tag versions with string names, making them more parseable.

Tags are stored inside the dDrive's `hidden trie`, meaning they are not enumerable using dDrive's [standard filesystem methods](#standard-filesystem-methods). They will replicate with all other data in the drive, though.

For more information on version control method's, see dDrive's JavaScript implementation and its official documentation [here](https://github.com/distributedweb/ddrive#version-control).

#### Sparse Downloading
Now that it has been explained how a dDrive's file system is stored and versioned within two coupled dDatabase feeds, it becomes easier to understand how one peer can download a specific version of specific files from a specific dDrive, otherwise referred to as `Sparse Downloading`. Simply put, since all of the layers that overlay a dDatabase feed utilize its underlying [dDatabase Protocol](#ddatabase-protocol) for the request and fulfillment of data between peers, any of these abstraction layers can make a request for a specific index range, for whatever that equates to at a specific abstraction layer. For example, when a specific file or folder is requested at the dDrive layer, it's common for the entire `metadata` feed to be requested from the dDrive's swarm. Once received, the file or folder path is passed to the dWebTrie layer, which searches the `metadata` feed for the matching path. If the current version of the file or folder is wanted, rather than older versions, the `most-recent` `Inflated Entry` that matches the path is chosen. This `Inflated Entry` should contain a `contentLog` pointed to the `index` where the file's data is stored in the `content` feed. From there, a `request` message can be sent to the dDrive's swarm, using an underlying dDatabase Protocol-based duplex stream, where the specific `index` from the `contentLog` can be requested.

For example, if the link `dweb://<dweb-key>/index.html` is requested via a ` DWEB`-ready client, the entire dDrive in this instance is not downloaded; as a matter of fact, only the `index.html` file and any files linked within it are downloaded by the requesting peer.

This is how the process would take place:
-1. A remote peer requests `dweb://<dweb-key>/index.html` via a DWEB-ready client.
-2. The client looks up the dWeb key via dWeb's DHT and a list of peers who are announcing this dDrive (its swarm) are returned.
-3. The client chooses a peer from the returned list and requests to open a dDatabase protocol-based duplex channel on channel 1 with the peer (or even multiple peers).
-4. The client asks for only the `metadata` feed, once the peers accept the connection request, by sending a `request` message on the channel, requesting the entire `metadata` feed's index range.
-5. The peer sends a `data` message back over the duplex channel containing the `metadata` feed.
-6. The client passes the `/index.html` path segment to the dWebTrie abstraction layer, so it can perform a lookup in the `metadata` feed.
 -6. An `InflatedEntry` is found, with a `contentLog` field within the entry, pointing to `index 6` of the `content` feed.
-8. The client locates the `content` feed's public key in the `protocol header` of the `metadata` feed and opens another duplex channel on channel 2, with the `content` feed's public key, with the same peers from step #3 and sends a `request` message for `index 6`.
-9. The `content` feeds is sent in a `data` message to the client, containing **ONLY** `index 6`.

It should be noted, that at step #8, one could request the entire `content` feed, which would eliminate future remote requests, as long as both feeds are replicated live.

#### Drive Nesting
dDrive has a built-in "mounting" system, which allows one or more dDrive(s) to be mounted or "nested" within a parent dDrive. This allows Bob, to nest Alice's dDrive full of her published photos within his dDrive full of published photos. What is great about this, is that this enables true peer-to-peer collaboration, since when Alice adds a photo to her dDrive of published photos, they are automatically replicated within Bob's dDrive, since Alice's drive is nested within Bob's.

**NOTE:** It's important to note that Alice's dDrive is a totally separate dDrive from Bob's and can be accessed directly. Bob is simply a peer (seeder) of Alice's dDrive and mounts it within one his own dDrives, so that when a peer downloads Bob's dDrive, they also receive the most recent version of Alice's. In truth, this means that a portion of Bob's dDrive is downloaded from his dDrive's swarm and another portion from the swarm of Alice's dDrive.

For more information on dDrive mounting, please see dDrive's reference JavaScript implementation and its [Mounting docs](https://github.com/distributedweb/ddrive#ddrive-mounting).

#### Standard Filesystem Methods
What one will find is that a dDrive implementation, like our JavaScript implementation, truly mirrors the standard filesystem methods of the Node.js `fs` module. Below is a quick explanation of those methods.

##### `createReadStream`
Read a file out of the dDrive, as a stream. Similar to `fs.createReadStream`.

##### `readFile`
Read an entire file into memory. Similar to `fs.readFile`.

##### `createWriteStream`
Write a file as a stream. Similar to `fs.createWriteStream`.

##### `writeFile`
Write a file from a single buffer. Similar to `fs.writeFile`.

##### `unlink`
Unlinks (deletes) a file. Similar to `fs.unlink`.

##### `mkdir`
Creates a directory. Similar to `fs.mkdir`.

##### `rmdir`
Deletes an empty directory. Similar to `fs.rmdir`.

##### `readdir`
Lists a directory. Similar to `fs.readdir`.

##### `stat`
Stat an entry. Similar to `fs.stat`.

##### `lstat`
Stat an entry but do not follow symlinks. Similar to `fs.lstat`.

##### `info`
Get mount information about an entry.

##### `access`
Similar to `fs.access`.

##### `open`
Open a file and get a file descriptor back. Similar to `fs.open`.

##### `read`
Read from a file descriptor into a buffer. Similar to `fs.read`.

##### `write`
Write from a buffer into a file descriptor. Similar to `fs.write`.

##### `symlink`
Create a symlink from a link name to a target name.

##### `mount`
Mounts another dDrive at the specified mountpoint. A mount can be a specific version of a dDrive.

##### `unmount`
Unmount a previously-mounted dDrive.

#####  `createMountStream`
Create a stream containing content/metadata feeds for all mounted ddrives.

##### `getAllMounts`
Returns a `Map` of the content/metadata feeds for all mounted ddrives, keyed by their mountpoints.

##### `close`
Close a file. Similar to `fs.close`.

##### `version`
Returns the current version of the drive.

##### `key`
Returns public key indentifying the drive.

##### `discoveryKey`
Returns a key derived from the public key (the dweb network address).

##### `writable`
Returns a boolean indicating whether the drive is writable.

##### `peers`
Returns a list of peers currently replicating with a dDrive (via dweb's DHT).

##### `checkout`
Checkout a read-only copy of the dDrive at an old version.

##### `createTag`
Create a tag that maps to a given version. If a version is not provided, the current version will be used.

##### `getTaggedVersion`
Returns a version, corresponding to a tag.

##### `deleteTag`
Delete a tag. If the tag doesn't exist, this will be a no-op.

##### `getAllTags`
Return a `Map` of all tags.

##### `download`
Download all files, in path of current version. If no path is specified, this will download all files.

For more detailed documentation, please see dDrive's JavaScript implementation [here](https://github.com/distributedweb/dDrive).

#### Live Replication
A dDrive can be live-replicated, by keeping two duplex streams open with online peer(s) of a dDrive's swarm, while sending a content `request` message to said peers, for the entire index range of both the `content` and `metadata` feeds. This is easily done via the dDrive layer by simply using the [`replicate`](https://github.com/distributedweb/ddrive#replication). method in a dDrive implementation.

Replication can also take place on a per-file or per-path basis, rather than downloading updates for an entire dDrive's underlying files/paths within its filesystem.

##### dDrive Auditing
Since a dDrive is technically two dDatabase feeds, the data is easily  audited and validated as to having derived from the holder of the public-private keypair, and the dweb network address that mathematically derived from the public key.

Also, since a dWebTrie and a dDatabase are both single-writer, it's an easily proven fact that only the creator of a dDrive can write to it, preventing outside forces from, for example, manipulating a website's content. It is also easily provable that data within a dDrive has not been altered, since it's an append-only log. For more information on how data within a dDatabase is validated, please read about dDatabase's [Merkle Trees & FIO Trees](#merkle-tree-and-fio-trees).
