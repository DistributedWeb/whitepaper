# dWeb Whitepaper
##### Authored By: Jared Rice Sr.
##### Date Published: August 26th, 2019
##### Date Revised: December 10th, 2020

## Preface
The World Wide Web, while it was once thought to be somewhat decentralized, has been transformed over the years into an overly centralized cesspool of authoritarians that are ultimately controlled by governments from around the world, as well as a handful of globally recognized tech companies. That was not how the visionaries of the web intended for it to be, nor would any of them agree with how the web is being ran today.

Beyond the evidence that was revealed by Booz Allen Hamilton contractor Edward Snowden, understanding the fact that the CIA, NSA and various private sector companies, including Google, Facebook and others are highly involved in manipulating mass thought, is crucial, in understanding why the web has to be reimagined [ACKE14]. The clear Chinese-based control over these companies, along with many elected officials within our government has become obvious to many Americans, which should put into plain English why all of the mass censorship and massive election interference is taking place.

It is us, the denizens of the World Wide Web, that have become extremely reliant on how quickly we can communicate with friends and access information via the WWW's many facilities. Although, it is during the process of these various communications and periods of research, that our information, our actions and even our reactions, are collected and then stored within the databases of not only the providers of these facilities but the "virtual, centralized grand database" once envisioned by President Reagan's former national security advisor John Poindexter [ROSE02]. It was Poindexter who was the brainchild of the "Total Information Awareness" program, which became the NSA's Advanced Research and Development Activity (ARDA), was later renamed "Terrorism Information Awareness" [IWAR] and eventually reincarnated as an alliance between the government and Silicon Valley for-profit companies. Enter Booz Allen Hamilton and other for-profit companies like Palantir, ran by so called "libertarians" like Peter Theil, who are paid by the CIA and the NSA to spy on and gather data  about law-abiding American citizens in the name of "terrorism" [GREE13].

History clearly shows us, even through the admissions of companies like Facebook and academics at Cornell University that they're clearly running psychological experiments on their users [ROBI14], [KRAM14], [GOEL14], [KRISH14]. While they do what all CIA and NSA contractors do and attempt to cover-up their tyrannical behavior with their typical talking points of "civil liberties" and "improvements to their service," it is clear to most that they're actively doing more than simply attempting to improve their services - and the dog regarding their care for our "civil liberties" will never hunt with any true American patriot.

Companies like Facebook, who have clear relationships with the Chinese Government, are far more interested in controlling what we see and quite frankly, what we believe and it's these constant internal studies that allow them to do just that. For years, these private sector companies have been developing internal algorithms, that are used each and every day to study your habits, your mannerisms and other psychological traits in the name of helping improve their various advertising services. What truly opened the eyes of Silicon Valley brainchilds like Mark Zuckerberg though, had little to do with predicting what you would buy - these authoritarians were blown away when their own studies revealed that voting behavior could be influenced by "undetectable social networking maneuvering" [LANI14].

This research I just mentioned happened in 2014 and don't worry, the source I just cited was the ***New York Times*** - just in case they say there is not "widespread evidence of voter manipulation." That's why I felt it was important to cite a New York Times article, regarding a study that Facebook conducted themselves! Facebook and Google's interest in our elections has grown extensively since 2014, where they collectively contributed hundreds of millions of dollars to local governments and various companies, in an attempt to support candidates that had unusual relationships with the Chinese government, like Joe Biden, whose son is the subject of an active FBI investigation for his dealings with Chinese companies as of December 10th, 2020.

What they used to refer as "undetectable maneuvering" is no longer truly undetectable, due to the fact that social networks are not only quietly choosing what you see in your news feed, but are now openly censoring those whose posts rail against the political propaganda machine most Silicon Valley-based social networks have become. This also takes place within search engines like Google, where conservative news stories and websites are rarely shown to those they know are registered democrats (look no further than the blacklisted Hunter Biden story). Their efforts to interfere in the 2020 election were extraordinary and quite obvious to anyone who is concerned with freedom in America.

All of this is made possible by a clear invasion of our privacy that we have allowed to go on for as long as most of us have been alive. This disregard for our own privacy has allowed companies in the private sector to gather data from every corner of our personalities, our lives and our interactions, which they've sold back to the government, in a clear violation of our Fourth Amendment rights. They've used this data to build a clear profile of all of us. They know every African American democrat from Dallas, Texas that ate McDonalds last week, while watching Netflix and they'll make sure that each and every one of them are fed anti-American leftist propaganda until each and every one of them join on with the "American woke."

It was this sort of invasion of privacy that the English administrators over the American colonies took part in, which in turn, according to John Adams, spirited the American Revolution, as much as any other factor. A young John Adams in 1761 once documented a speech of James Otis saying "every man of a crowded audience appeared to me to go away, as I did, ready to take up arms against writs of assistance." Otis was delivering a speech where he openly denounced the "writs of assistance," a clear and open demand for privacy, which became a clear motivator in the establishment of our republic. I believe the invasion of privacy we face today, far exceeds what any of our founders could have imagined. Alice Walker once said, "the most common way people give up their power, is by thinking they don't have any," but they have also clearly given up their rights to tech companies across our land, in exchange for convenience and style. Who could have imagined that?

I have said, for quite some time, that we're overdue for another revolution, but this time, a portion of that revolution will certainly be digital. As Chief John Roberts fairly pointed out in Riley vs. California, the Fourth Amendment doesn't simply cover our physical homes, but our digital ones as well. Likewise, true American patriots don't face just a violent physical revolution in order to secure their freedoms, but a digital one as well as and they'll have to have a place in the digital realm, where their voices can be heard, without any possible interference from the left. I say that because building alternatives on the centralized web to Facebook and Google, is not a long-term solution, nor is it a viable one. Their control over us on the web, goes much further than what we see or what we read. They now control our companies, our domain names, what we publish, what we sell and yes, our ability to accept online payments.

Andrew Torba, the founder of Gab, a Facebook alternative for conservatives, has already been banned from most online payment platforms, has had his domain name seized, his servers taken offline and even his family members have been blacklisted by VISA. While that sounds like a conspiracy theory, it's an unimaginable reality that is taking place with companies like Gab today. Organizations like ICANN and ARIN, who control IP address delegation amongst Internet Service Providers and who became domain registrars, are quickly becoming activists for the left's causes and enacting policies that will soon force domain registrars like GoDaddy and web hosting providers like Amazon Web Services (AWS), to move on from WWW-based facilities that allow for the distribution of information that goes against the left's propaganda, which is exactly what Gab is used for and it's why Gab was used as a test-case in their effort to shun conservative facilities form the web itself.

Beyond just the web and the threat of ICANN, ARIN and quite frankly Biden's FCC silencing conservatives, is the additional threat of Google and Apple doing the same to mobile applications like Ales Jone's InfoWars, which was abruptly removed from both app stores once it surpassed CNN as the most downloaded news application, within hours of its launch. To put all of this into perspective, companies in Silicon Valley are selling user data to communist governments, running psychological experiments on their users, silencing those who expose their spread of propaganda, interfering in our American elections and are now forming a powerful resistance to alternative facilities that emerge as a result of their total disregard for American civil liberties.

This is why I have embarked on a journey for the past several years, to reimagine the web and build a medium where the people themselves are in control; instead of governments and/or the private sector. A quick look at how long this GitHub profile has been around as well as the amount of research that has gone or here, should reveal that to any open-minded layman. The point of this paper is to make the case for a decentralized alternative to the web that we've been developing for the past four years, known as the "dWeb." It will also explain how the dWeb works and will lay down the clear scientific and mathematical proof, that ensures that the dWeb itself, nor anything on it, can be altered or removed by a single entity, other than the author. At the very same time, this paper will also explain, in-depth, how an elected governance, a distributed election system and a decentralized reporting system are used to ensure that the people of the dWeb, as well as the elected governance, have the delegated authority to remove illegal activity, as well as illegal content, without any centralized systems that could otherwise be used to corrupt and take control of the dWeb by bad actors in the future, just as they have with the traditional web.

While the dWeb facilitates protections from the tyrannical and authoritarian actions of government, as well as the private sector, it also presents technological advances that allow for the global distribution of websites, web applications and various forms of data, without the need for costly infrastructure or the threat of hackers. Those advances, as well as others, are covered in this paper as well.

The need for a decentralized web, during a time where the web has become over-centralized, has become obvious to the many who avidly use the web and have witnessed the constant violation of the human, civil and constitutional rights of the web's users. The influx of countries and their governments whom use the web, as well as the Internet at large, to spy on their citizens has gotten out of control. The lack of privacy is truly alarming and so are the security issues, where even central banks are still unable to protect worthless fiat, after over 20 years of attempting to secure their networks. Now social networks and search engines want to get involved in politics, control what we find or view on their networks and will delete our entire digital existence over a policy violation. Big tech and tyrannical governments like our own know that software developers are building ways to avoid their tactics and are using policies, as well as regulations, to scare of innovators in an attempt to maintain power and keep their usual control mechanisms in place.

Developers, innovators and disseminators of information need a better protocol. They need a completely decentralized web where their hard work can never be destroyed or shutdown by fearful organizations or governments. They simply need a better web where they can experience true privacy, freedom, security and transparency digitally, for the first first time since the inception of the web itself. Isn't that what the web was created for? It's what the dWeb was created for, only this time, we didn't forget to hardcode those rules into its immutable and irreversible existence.

Welcome to the dWeb.

"There is no central control."
***- Paul Baran - Godfather of packet switching***

"My bias was to always build decentralizations into the net. That way it would be hard for one group to gain control. I didn't trust large central organizations. It was just in my nature to distrust them."
***- Bob Taylor - ARPANET Engineer***

**Note:**
As an added plus, the dWeb is alive as you're reading this and can never be taken offline, as long as people like you are using it. You can start browsing the dWeb by downloading dBrowser [here](https://dbrowser.com) and can begin following our efforts to launch truly decentralized facilities like [dSocial](https://github.com/peepsx/dsocial-whitepaper) and [dSearch](https://peepsx.com/dsearch).


## Abstract
The dWeb is a decentralized web that is formed by a foundation of off-chain, peer-to-peer networking, data communication and data integrity protocols, which enable the exchange and cryptographic validation of data amongst peers. Many dWeb services like dDNS and its reporting system are layered on top of the ARISEN blockchain protocol suite in order to utilize ARISEN's universal authentication layer, public key authority, distributed virtual machine, decentralized network consensus, decentralized payment processing, as well as its on-chain data persistence layer.

This marriage between the dWeb and ARISEN at dWeb's low level service layers, gives way to a powerful ecosystem that can be used to develop secure, serverless and globally scalable applications that are decentralized from end-to-end. The dWeb improves upon the limitations and weaknesses of the World Wide Web, while ensuring that tyrannical governments and private sector companies are unable to regulate or control any aspect of the dWeb itself, without the approval of the dWeb's users. The dWeb is a futuristic web where infrastructure costs are nil, hackers are rendered useless, and the users themselves are placed in control of their data, along with the delegated authority to elect a governance that protects the dWeb at-large from fraud, illegal activity and illegal content, per the dWeb's ratified [Constitution](https://github.com/distributedweb/constitution).

This whitepaper explains the dWeb in-depth, from its foundational protocols and ARISEN's protocol suite, to dWeb-based services like dDNS and its decentralized reporting system.

## Foundational Protocols
The dWeb's foundation of off-chain, peer-to-peer networking, data communications and data integrity protocols, enable peers to announce data, remote peers to discover and swarm data, exchange data and validate the integrity of the data that is ultimately fetched via a dataset's swarm of peers. The following sub-sections will explain each of these foundational protocols and how their reference implementations function.

### DWDHT
DWDHT, an acronym for "dWeb DHT," is a protocol that forms a distributed hash table of dataset identifiers (dWeb network addresses) and the peers that are announcing them. Each database identifier and its announcing peers form what is referred to as a "swarm," which can be queried from connected peers or joined (announced) by a specified amount of peers. dWeb's DHT is literally how a particular dataset is discovered and downloaded from the peers that are openly seeding it (announcing it). Each peer who is announcing a dataset on a long-term basis, by design, becomes a DHT node and is responsible for storing a specific portion of dWeb's DHT. Put another way, each DHT node stores a specific portion of dWeb's DHT. Put another way, each DHT node stores a specific portion of the dWeb's dataset identifiers, along with the peers who currently posses the data related to each of those data identifiers.

While each DHT node stores various swarms, it also stores information regarding other DHT nodes, identified by `node identifiers`, which can be mathematically determined to possess information regarding a specific swarm. dWeb's DHT is based on the `Kademilia Distributed Hash Table` popularized by BitTorrent and others. Nodes and data in dWeb's DHT are assigned 160-bit integers as IDs, while swarms are stored in the form of key-value pairs, where the key is a 32 byte value generated by a one-way hash function, such as SHA-1 (normally a dWeb network address), where the value is an object containing public and LAN-based peers that are announcing the key (the dataset identifier).

#### Node & Data Distribution
The DHT defines the distance between 2 DHT nodes `i` and `j` by the bitwise exclusive OR operations `(XOR)`, i.e., `d(i,j) = i (XOR) j`. This distance means for any given key `i` and a distance `L > 0`, there can only be a single key `j` that satisfies `d(i,j) = L` [ZHAN13].

All key-value pairs are stored on `k` nodes whose UIDs are closest to the actual key. `K` is an important parameter that helps determine data redundancy and how stable the DHT is at any time. Each node `i` always maintains multiple `k`-buckets. Each `k`-bucket stores a list of other DHT nodes, which are organized in an order that reflects the most recently active nodes. The node that is most recently active is stored at the tail, while the least active node is stored at the head.

The node whose distance from node `i` is in the range of [pow(2,m), pow(2,m+1)] is stored in the `m`th `k`-bucket (node that 0 < `m` < 160). The nodes in the `k`-buckets are regarded as the neighbors of the node `i`. A dWeb DHT node dynamically updates its neighbors upon receiving any messages from them. This process can be better explained more specifically, when node `i` receives a message from another DHT node `j`, which is located in the `m`th `k`-bucket, where the `k`-bucket of node `i`, will be updated in the following way:

-If `j` already exists in the `k`-bucket, `i` moves `j` to the tail of the list, as node `j` is the most recently seen.
-If `j` is not in the `k`-bucket and the bucket has fewer than `k` nodes, node `i` inserts `j` at the tail of the list.
-If the bucket is full, `i` PINGs the node at the head of that particular `k`-bucket.
-If the head node responds, node `i` makes it to the tail and ignores node `j`.
-Otherwise, `i` removes the head node and inserts `j` at the tail.

##### DHT Primitives
-`PING` - Probes a DHT node to check whether it's online or not.
-`STORE` - Used to store a key-value pair.
-`FIND_NODE` - Finds a set of nodes that are closest to a given node.
-`FIND_VALUE` - Operates like `FIND_NODE` but returns a stored value.

These RPC-like primitives work in a recursive way, which improves the efficiency of dWeb's DHT. A `lookup` procedure is initiated by the `FIND_NODE` and `FIND_VALUE` primitives, where the lookup initiator chooses `B` nodes from its closest `k`-buckets and send many parallel `FIND_NODE` requests to these `B` nodes. If the node being searched for in a given iteration is not found, the lookup initiator resends the `FIND_NODE` to the nodes that were found during the previous recursive operation and repeats this iterative functionality.

A KV pair may be stored on multiple DHT nodes. Thanks to the recursive procedure explained above, the key-value pair spreads across the DHT network every hour. This process insures that multiple replicas of data exist across the network. Every key-value is deleted 24 hours after it is initially pushed into the network.

##### Joining and Leaving
When node `i` joins dWeb's DHT, it is assumed that it is aware of or knows about node `j`. The joining process consists of multiple steps as follows:
-1. Node `i` inserts `j` into its `k`-buckets
-2. Node `i` starts a node lookup procedure for its own ID, where `i` is made aware of other newer nodes.
-3. Node `i` updates the `k`-buckets

During this process, node `i` strengthens its `k`-buckets and inserts itself into other nodes' `k`-buckets. When other nodes leave or fail, they DO NOT notify any other node. There is no need for a special procedure to cope with node departures, as these mechanisms insure that leaving nodes will be removed from the `k`-buckets.

#### Swarm Announcement
Announcing a swarm means that you're either: announcing a dWeb network address that doesn't exist on dWeb's DHT and therefore become the first peer related to the address; or announcing a dWeb network address that does exist on dWeb's DHT and are added to a list of peers who are announcing the address.

A swarm entry in dWeb's DHT, takes on the following format:

```
Key: 8f0ab2... (32 byte hexadecimal address)
= = = = = = = = = = = = = = = = = = = = = = = = =
Value:
{
  node: { host, port },
  peers: [{ host, port }, { host, port }]
  localPeers: [{ host, port }, { host, port }]
}
```

If a peer is announcing a dWeb network address that matches a key in the DHT, the entry is mutated to include the peer's announced IP address and port. When announcing a dWeb network address, a peer must set their public port and public IP address during the announcement and can choose to set a lan-based address and port as well, so that those on the same public IP who are performing a DHT lookup, can retrieve local announcers for a particular dWeb address. It's important to note that when announcing a dWeb network address on dWeb's DHT, the announcer becomes a DHT node on the network by design.

#### Swarm Lookups
A swarm can be looked up by its dWeb network address, which is a 32 byte buffer (normally a hash of something). When performing a lookup, the querying peer is temporarily an announcing peer, but is removed as an announcing peer (unannounces), once the lookup is finalized.

A lookup for a particular dWeb key, returns the following data:
```
{
  // The dWeb DHT node that is returning this data
  node: { host, port }
  // List of Peers
  peers: [{ host, port }, ...]
  // List of LAN Peers
  localPeers: [{ host, port }, ...]
}
```

#### DHT Bootstrap Nodes
dWeb's network utilized various `bootstrap` nodes to launch the initial dWeb network. These 3 bootstrap nodes are as follows:

-`dht1.dwebx.net`
-`dht2.dwebx.net`
-`dht3.dwebx.net`

These nodes going down would not affect the dWeb or a user's ability to announce or lookup dWeb network addresses, due to the way other DHT nodes on the network share data and information regarding each other. dWeb developers have the option of launching their own bootstrap nodes so that their apps have a point of entry into the dWeb's network of DHT nodes. Those nodes would initially use a set of already existing nodes to gain access to the DHT and would no longer need access to the nodes used for bootstrapping. Any node on the DHT can be used to bootstrap a new node.

#### DWDHT Reference Implementation
The DWDHT reference implementation was written in JavaScript and was used to launch the initial dWeb DHT. You can find it [here](https://github.com/distributedweb/dht).

You can use this reference implementation to allow dWeb-based applications (desktop, mobile or web) to act as DHT nodes.

You can also launch your own dWeb DHT node via the command-line, by using the DHT CLI [here](https://github.com/distributedweb/cli).


#### dWeb Swarm API
The dWeb swarm API, also known as "Swarm Programming," is a high level API built on top of the [DWDHT(#dwdht) Protocol used for finding and connecting to peers of a particular swarm and interacting with the dWeb DHT.

It expands the default set of parameters when creating a DHT node, adds specific swarm events where callbacks can be used to react to specific swarm-based events and introduces several functions, like `join`, `leave`, and `connect`, which make it easy to programmatically interact with the dWeb's DHT.

##### DHT Node Initiation Parameters
Using the dWeb Swarm API, a DHT node can be created with the `dwebswarm()` function, using the following parameters:

-`bootstrap` - An array of bootstrap servers used to initiate the dWeb DHT node.
-`ephemeral` - A boolean that is set to `false` if this is intended to be a long running DHT node.
-`maxPeers` - The total amount of peers that the node initiator will connect to.
-`maxServerSockets` - The number to restrict the number of server socket based peer connections. Set to `Infinity` by default.
-`maxClientSockets` - The number to restrict the number of client socket based peer connections. Set to `Infinity` by default.
-`validatePeer` - A function for applying filters before connecting to a peer.
-`queue` - An object for configuring peer management behavior.
--`requeue` - An array of backoff times in milliseconds every time a failing peer connection is retried.
--`forget` - An object for when to forget certain peer characteristics and treat them as fresh connections again.
---`unresponsive` - How long to wait before forgetting that a peer has become unresponsive.
---`banned` - How long to wait before forgetting that a peer has been banned.
---`multiplex` - Set to `true` in order to reuse existing connections between peers across multiple dWeb network addresses.

##### Swarm Event Emitters
Using the dWeb Swarm API, applications can listen for the following events:

-`connection` - A new connection has been created. Event should be handled by using the socket. This emits an `info` object that describes the connection using the following details:
```
-type (string) - Should be either "tcp" or "udp."
-client (boolean) - If true, the connection was initiated by this node.
-topics (array) - The list of dWeb network addresses associated with this connection, if "multiplex" was set to true, during DHT node initiation.
-peer (object) - Object describing peer (port, host, LAN peer details, referrer and the dWeb network address the peer was discovered under).
```
Using the `info` object, the `info.ban()` method can be called to ban the connected peer and will keep the DHT node from connecting to the peer in the future. The `info.backoff()` method can be called to get the DHT node to backoff from connecting to the peer.

-`disconnection` - A connection has been dropped. Emits an `info` object that is identical to the `connection` `info` object, describing the dropped connection.

-`peer` - A new peer has been discovered on the network and has been queued for connection. Emits a `peer` object, including the port, host referrer, and dWeb network address peer was discovered under. Also includes a boolean on whether the peer is a LAN-based peer.

-`peer-rejected` - A peer has been rejected as a connection candidate. Emits a `peer` object that is identical to the `peer` event's `peer` object, describing the rejected peer.

-`updated` - Emitted once a discovery cycle for a particular dWeb network address has completed. Emits a `key` object that identifies the dWeb network address the discovery cycle is related to.

For more information on dWeb Swarm's events, take a look at the README.md file within the reference implementation of the dWeb Swarm API [here](https://github.com/distributedweb/dwebswarm).

##### Swarm API Functions
There are several functions (methods in the reference JavaScript implementation) that can be used to interact with a dWeb DHT node. These functions and their required parameters are described below.

###### `join` - Join the swarm for a given `topic` (dWeb network address). This will cause peer to be discovered for the topic (`peer` event).

###### `join()` Parameters
-`topic` (Buffer) - The dWeb network address to list under. Must be 32 bytes in length.
-`options` (Object)
--`announce` (Boolean) - List this peer under the topic as a connectable target. Defaults to `false`.
--`lookup` (Boolean) - Look for peers in the topic and attempt to connect to them. If `announce` is false, this automatically becomes true.
-`onion` - A function that is called when your topic has been fully announced to the local network and the DHT.

###### `connect()` - Establish a connection to a given peer. You usually won't need to use this function, because the DHT node connects to discovered peers automatically. Accepts a `peer` object (identical to the `peer` object emitted by the `peer` event) for the peer the function is to establish a connection with. Also has a callback function that emits either a connection error (err), the established TCP or UDP socket (socket) and details regarding the established connection (details).

###### `leave` - Leave the swarm for a given dWeb network address.

###### `leave()` Parameters
-`topic` (Buffer) - The identifier of the peer-group to delist from.
-`onleave` - A function that is called when your topic has been fully unannounced to the local network and the DHT.

For more in-depth examples on how to call these functions within dWeb-based applications, view the `README.md` file within the reference JavaScript implementation [here](https://github.com/distributedweb/dwebswarm).

### dDatabase
A dDatabase is a distributed append-only log, also referred to as a distributed and immutable data feed, which can be exchanged between peers, using the [dDatabase Protocol](#ddatabaseprotocol) and validated via functions available in a dDatabase implementation.

At a very basic level, the dWeb is made up of dDatabase feeds, but it's important to note that a dWeb network address doesn't have to represent just a dataset, it can represent an entity like a peer or a device. Although, as the dWeb stands today, most of what you see distributed across it are dDatabases.

Below is a pseudo-representation of what a dDatabase would look like:
```
Index                 Entry
= = = = = = = = = = = = = = = = = = =
0                       Hello
1                       World
```

As you will see later in this paper with the [dDrive](#ddrive) Protocol, a distributed file system can be created using a dDatabase, by converting the content and metadata of a file system's files into a particular encoding format, such as binary, and storing in a dDatabase entry like so:
```
Index                 Entry
0                       {content:010110011110...}, {metadata:1110100101...}
1                       {content:11101001111001...}, metadata:0100110000...}
```

In other words, a dDatabase can easily be used to distribute an entire file system between peers, or any dataset for that matter. This section is intended to breakdown how dDatabases work, how data is determined to have the utmost integrity, and how data is exchanged between peers using the dDatabase Protocol.

A dDatabase, represented by a generated dWeb network address, can easily be announced and discovered via dWeb's DHT, where other peers can download the dDatabase from peers in the dDatabase's swarm who have download the data and subsequently announced their joining of the swarm itself. Put another way, a dDrive that contains a website can announce its dWeb network address on the dWeb DHT and peers can join the dDrive's swarm and thereby increase the number of peers the website can be downloaded (replicated) from. This is just an example, as a dDatabase can contain an endless amount of dataset types, such as an encrypted conversation or an encrypted voice call.

#### Distributed, Immutable Data Feed
dDatabase is a secure, distributed and append-only (immutable) feed that is designed for distributing large datasets and streams of real time data between peers of the dWeb.

#### Value Encoding
The value of an append entry to a dDatabase can be encoded into the following formats:

-`binary` (Default)
-`json`
-`utf-8`

#### Data Integrity
It is important that those who download a dDatabase can verify that it actually derived from its proclaimed author. This is made possible thanks to dDatabase's use of public key cryptography, which a dDatabase's dWeb network address is derived from, alongside Merkle trees and FIO trees, in order to verify a dDatabase's root hash signature(s). This insures that the data itself derived from the author of the dDatabase and the original announcer of the dWeb network address.

##### Merkle Trees and FIO Trees
A dDatabase uses a custom encoding method when laying out dDatabase-based update logs into a Merkle tree. This particular encoding method positions hashes into a scheme known as "binary in-order interval numbering," or just "bin" numbering for short. This is a deterministic method of positioning leaf nodes in a Merkle Tree.

A Merkle Tree with seven leaf nodes will always be laid out like this:
```
0L
      1L
2L
            3L
4L
      5L
6L
```

From a computer science perspective, dDatabases are binary append-only streams, the contents of which are cryptographically hashed and signed. Therefore, any dDatabase can be verified by anyone with access to the public key of the creator. Over the HTTP protocol, datasets are shared every day, although there is no built-in support for version control or the content-addressing of specific data. dDatabase is the solution to this, allowing multiple un-trusted devices to act as a single virtual host. For the dWeb to work, it requires a data structure that authenticates the content's integrity and one that keeps a historical log of the revisions - and dDatabase feeds certainly provide that.

dDatabases are identified internally by signed Merkle trees, are identified publicly over the dWeb by a public key, and are discovered over dWeb's DHT by a dWeb network address, which in turn derives from a dDatabase's public key. A dDatabase's public key is used to verify the signature that relates to the received data. A dDatabase's internal Merkle tree is output as a "Flat In-Order Tree" or "FIO Tree."

FIO trees, per PP5P RFC 7574, are defined as "bin numbers." These FIO trees allow for numerical-based identification of each leaf node within a binary-based Merkle tree and therefore create a simplistic way of representing a binary tree as a list. These properties of FIO trees are used in the simplification of "wire protocols" that of which utilize Merkle tree structures within distributed applications.

Below is an example FIO tree, that is sized at 4 blocks of data:
```
0L
1P
2L
3P
4L
5P
6L
```

In the above FIO tree example, even numbers (0, 2, 4, 6) represent leaf nodes on the tree and odd numbers represent parent-nodes that each contain two children.

Using binary notation, we can count the total number of "trailing 1s" to calculate the depth of the tree's nodes. For example, the following numbers below are converted to binary:
```
5 = 101
3 = 011
4 = 100
```

The number 5 has one trailing 1, the number 3 has two trailing 1s and the number 4 has zero trailing 1s. In the FIO example, 1 is the parent node of (0, 2), 5 is the parent node of (4, 6) and 3 is the parent node of (1, 5). The FIO tree would only have a single root if the leaf node count is a power of 2, otherwise a FIO tree will always have more than one root.

Below is another FIO tree (pseudo-representation) with a total of 6 leaf nodes:
```
0
1
2
3
4
5
6
7
8
9
10
```

In the above example, the roots are 3 and 9.

A Merkle tree, named after cryptographer and mathematician Ralph Merkle, is best described as a tree of binary-based data, where every "leaf" (an even-numbered tree node that has no children) is a hash of a data block and every "parent" (an odd-numbered tree node that has two children) is the hash of both of its children. dDatabases are feeds represented by Merkle trees that are ultimately encoded with "bin numbers."

For example, a dDatabase of 4 values would always map to the 0, 2, 4 and 6 leaf nodes. Below is a pseudo-representation:
```
fragment0 -> 0
fragment1 -> 2
fragment2 -> 4
fragment3 -> 6
```

When converting to FIO tree-style notation, a Merkle tree spanning these data blocks looks like:
```
0 = hash(0 + 2)
2 = hash(fragment1)
3 = hash(1 + 5)
4 = hash(fragment2)
5 = hash(4 + 6)
6 = hash(fragment3)
```

The even and odd nodes store different types of information:
-`Even Numbers` - List of data hashes [f0, f1, f2, ...]
-`Odd Numbers` - List of Merkle hashes (hashes of child nodes) [h0, h1, h2, ...]

The root node within a Merkle tree hashes the entire dataset. In the example of 4 fragments, node #3 hashes the entire dataset and node #3 is used to verify the rest of the dataset. Although the root node will change every time data is added to a dDatabase.

```
0
1
2
3 (root node)
4
5
6
7
8
9 (root node)
10
```

The Merkle tree's nodes are calculated as follows:
```
0 = hash(fragment0)
1 = hash(0 + 2)
2 = hash(fragment1)
3 = hash(1 + 5)
4 = hash(fragment2)
5 = hash(4 + 6)
6 = hash(fragment6)
7 = hash (6 + 8)
8 = hash(fragment4)
9 = hash(8 + 10)
10 = hash(fragment5)
```

When there are multiple root hashes, it is convenient to capture the entire state of a dDatabase as a `fixed-size hash` by hashing all of the root hashes into one single hash, where in the example above:

`root = hash(9 + 3)`
or
`tr = h(r1 + r2)`
(tr = top root; hash = hash function; r1 = root hash 1; r2 = root hash 2)

##### Root Hash Signatures
Merkle trees are used by dDatabases to create a way of indentifying the content of a dataset through hashes. The concept is simple, if the underlying content of a dDatabases changes, the hash changes. For example, a dDatabase acts as a list that calls the `append()` mutation when an entry is added to the database feed, thereby adding a new leaf to the tree, which ultimately generates a new root hash. When a dDatabase is created, a public/private keypair is generated. The public key is used as a public identifier for data validation, whereas the private key is used to sign the root hash every time a new one is generated. This digital signature is always distributed with the root hash to verify its integrity.

##### Data Validation
Data which is received that belongs to a dDatabase goes through the following process:
-1. The root hash's signature is verified.
-2. The received data is hashed with `ancestor hashes` in order to reproduce the `root hash`.
-3. If the root hash that was calculated is an exact match of the original root hash that was received, the data has been verified.

In an example of a dDatabase containing 4 values, our tree of hashes would look like this:
```
0
1
2
3 (root hash)
4
5
6
```

If we want to verify the data for 0 (fragment0), we first read (2), which is the sibling hash, then (5), which is the uncle hash and then (3), which is the signed root hash.

```
0 = hash(fragment0)
2 = (hash received)
1 = hash(0 + 2)
5 = (hash received)
3 = hash(1 + 5)
```

If what we calculate for 3 is equal to the signed root hash we received for 3, then fragment0 is valid.

It's important to note that all new signatures very the entire dDatabase since the signature spans all data in the Merkle tree. A dDatabase is considered corrupt if a signed mutation results in a conflict against previously verified trees. When a dDatabase is considered corrupt, the dDatabase protocol is designed to stop distribution.

##### Cryptography Specification
-The hash function uses BLAKE 2B-256 encryption, while signatures are ed25519 with the SHA-512 hash function.
-Hash function inputs are prefixed with different "constants" based on the type of data being hashed. The constants include:
--0x00 -Leaf
--0x01 -Parent
--0x02 -Root

This protects against a "second preimage attack."
-Hashes, a lot of the time, will include the sizes and indexes of their content so that the structure of a tree can be described along with the tree's data.

#### Data Replication
The easiest way to describe how dDatabase replication works between a set of peers using pseudo-code examples, is in the context of a file system - so we will use a dDrive abstraction in the following examples:

Let's say we create a dDrive and add two files to it:
-`jefferson.png`
-`trump.png`

Once those files are added to the dDrive, they are split into fragments, where the content data is represented within a constructed `content` object and the metadata is represented within a constructed `metadata` object, where each file itself is split into multiple dDatabase entries. Assuming `jefferson.png` and `trump.png` are both split into three fragments a piece, that each equate to a file size of 134 Kb, the representation would be placed in a list like this:

```
jefferson1.png
jefferson2.png
jefferson3.png
trump1.png
trump2.png
trump3.png
```

These fragments of the two image files each get hashed and those hashes are organized into a Merkle tree, like the example below:

```
0L                         -hash(jefferson1)
      1L                   -hash(0L + 2L)
2L                         -hash(jefferson2)
            3L             -hash(1L + 5L)
4L                         -hash(jefferson3)
      5L                   -hash(4L + 6L)
6L                         -hash(trump1)
8L                         -hash(trump2)
      9L                   -hash(8L + 10L)
10L                       -hash(trump3)
```

The next part of the process is to calculate the root hashes of the Merkle tree, which are `3L` and `9L`, then hash the number that derives from that. Lastly, we will cryptographically sign the `combined root hash` (sH). The signed hash is utilized to validate all other leaf nodes (parent/child) within the tree, where the signature easily proves who this dDrive was published by.

The pseudo-tree above is for the hashes that derive from the fragments of our two photos. A second Merkle tree is generated as a representation of the files and their metadata, known as a `metadata.log`.

An example of a metadata log-based Merkle tree is below:

```
0L - hash(contentLog: {`4f21f567...`})
1L - hash(0 + 2)
2L - hash({"jefferson.png", first: 0, length: 3})
4L - hash({"trump.png", first: 3, length: 3})
```

A few notes regarding the above pseudo-representation of the metalog's Merkle tree:
-The 1st entry in this log is the metadata entry point to a hash of the content log (the content log that the first Merkle tree represented).
-Notice that the 3rd leaf node is not included yet. That's because 3 is the hash of 1 + 5 and 5 does not exist yet, so it will be written at a later update.

All that's left in the replication process is to send the metadata to the other device (peer). The replicating peer-to-peer messaging protocol, known as the dDatabase Protocol, is tasked with communicating between peers over what is known as a "duplex binary channel." Below is a play-by-play of the communications between two peers, Bob and Alice.

-1. Bob sends the first message, known as the `announce` message that includes a dWeb network address.
-2. Alice sends Bob a `want` message, signaling that Alice wants all leaf nodes in the metadata log of the dDrive that Bob is announcing.
-3. Alice sends three `request` messages, one for each leaf node (0, 2, 4).
-4. Bob sends back three `data` messages. The data messages contain the `contentLog` key, the hash of the sibling, which in this case is (2), the hash of the uncle root (4), and the signature for the root hashes (1, 4).
-5. Alice can validate the integrity of the first data message by hashing the metadata received for the `contentLog` metadata to produce the hash for fragment0. She then hashes the leaf node (0), with the hash (2), that was included to reproduce hash (1) and hashes (1) with the value of (4). Lastly, the digital signature that was received is used in order to verify it was the same dDatabase that was originally requested.
-6. When the next `data` message is received, a similar replication process to the one shown above is performed to verify the content of the dDrive again. Alice now has a full list of the files in the dDrive and can choose which ones she wants to download.

#### dDatabase Data Structure
Data within a dDatabase is stored as `blocks`, which are indentified by an `index`. Each `block` is signed by its creator, so that an entire dDatabase feed can be audited, where all currently stored data matches the hashes in the Merkle tree. Put another way, all data blocks must match the hashes contained within the dDatabase Merkle tree, explained previously.

This data structure ensures that peers can download a specific block range, rather than the entire block base of the dDatabase itself. The `index` is an auto-incrementing number that is zero-based.

A pseudo-representation of a dDatabase feed uses binary encoding:
```
Index              Entry
0                    0111010010011...
1                    1011000110000...
```

As data is appended to the feed, a new index is created. Those who are live replicating the feed would receive the new index in real-time.

#### dDatabase Protocol
Before diving into the dDatabase Protocol, it is important to reiterate that a dDatabase is an append-only data feed, where the data within a data feed is an abstract "blob" of data.

**dDatabase feeds can:**
-Be distributed partially between peers.
-Be distributed fully between peers.
-Be distributed to/received by multiple peers at once.

The dDatabase Protocol is a process by which two or more peers exchange a dDatabase over binary duplex streams. Remote peers are able to identify with seeders of a dDatabase whether they are looking for specific portions of a dDatabase (partial) or the entire dDatabase.

Using various message types, peers are able to ask a specific peer for a specific portion of the feed, whereas the feed distributor(s) can signal whether they have that data and can subsequently fulfill the data request. The protocol uses a sender/handler lifecycle for message exchange and message handling.

Binary duplex streams can be encrypted, use a public/private keypair for stream authentication, and utilize a NOISE-based stream handshake by default.

This section will cover all of these aspects, including dDatabase's handshake phase and message exchange phase.

##### Handshake Phase
A dDatabase protocol duplex stream can use an optional NOISE-based handshake. For the handshake, NOISE uses the `XX` pattern. Each NOISE message is sent with variant framing. After the handshake is finalized between peers, a message exchange phase can begin.

The handshake phase allows two or more peers to authenticate each other and to securely negotiate an encryption and MAC algorithm, along with cryptographic keys to be used to protect the data sent between them. After the initial handshake transport, encryption is enabled to ensure a stream is private.

As an added plus, each NOISE session is unique and is identified by a unique `handshake hash` in order to enable channel binding.

##### Message Phase
The NOISE-based message exchange phase uses a basic variant length prefixed format to send messages over the wire.

All messages contain a header indicating the type and the feedID of the dDatabase feed, and a protobuf-encoded payload:

`message = header + payload`

A header is a variant that looks like this:

`header = feedID << 4 | numeric-type`

The `feedID` is just an incrementing number for every feed shared and the `numeric-type` corresponds to which protobuf schema should be used to decode the payload.

The message is then wrapped in another variant containing the length of the message:
`wire = length(message) + message + length(message2) + message2 + ...`

A good example of how the protocol functions can be found in [Data Replication](#data-replication).

###### Message Types
There are several message types that can be used to open a channel, request data and send data. Each of the below message types correspond with a specific [Message Handler](#message-handlers). The following message types are available:

###### `open` Type
Opens a channel and signals the other end that you are sharing a dDatabase feed. Accepts a `key` parameter, which is the dDatabase public key. Before being hashed, the public key of the dDatabase is hashed and sent as the dWeb network address for any connecting peers. This protects the dDatabase's public key from being learned by remote peers who do not already posses it. Also includes a cryptographic proof that the local possesses the public key, which can be implicitly verified by using the [removeVerified API](#removeverified-api).

Also accepts an object of functions, as a parameter, for handling incoming messages, using [Message Handler](#message-handlers).

###### `close` Type
Signals to the other side that the sender does not have the key corresponding to the dWeb network address.

###### `destroy` Type
Destroys the stream and closes all dDatabase feeds as well.

###### `finalize` Type
Gracefully end the stream and close all dDatabase feeds.

###### `options` Type
Sends an `options` message on a channel.

###### `status` Type
Sends a `status` message on a channel. This message is intended to indicate state changes.

###### `have` Type
Sends a `have` message on a channel, to see if the local can fulfill a particular request. Should reference the available index range (starting index and feed length). This should be in response to a `want` message. Feed length can be defined as `Infinity` for live replication.

###### `unhave` Type
Sends an `unhave` message on a channel, indicating an index range that is no longer available.

###### `want` Type
Sends a `want` message on a channel, indicating a specific index range that is wanted.

###### `unwant` Type
Sends an `unwant` message on a channel, indicating a specific index range that is no longer wanted.

###### `request` Type
Sends a `request` message on a channel, in order to request a specific piece of data from the dDatabase feed.

###### `cancel` Type
Sends a `cancel` message on a channel, in order to cancel a `request`.

###### `data` Type
Sends a `data` message on a channel, in order to fulfill a `request`. Includes a digital signature, accompanied by a Merkle tree representation of the data.

For more information on each of these messages, see the [dDatabase Protocol ProtoBuf Schema](https://github.com/distributedweb/ddatabase-protocol/blob/patriot1/schema.proto).

###### Message Handler
Message Handlers are triggered after specific [Message Types](#message-types) are received. The following handlers can be used:

###### `onopen`
Triggered after receiving an `open` message.

###### `onoptions`
Triggered after receiving an `options` message.

###### `onhave`
Triggered after receiving a `have` message.

###### `onunhave`
Triggered after receiving an `unhave` message.

###### `onwant`
Triggered after receiving a `want` message.

###### `onunwant`
Triggered after receiving an `unwant` message.

###### `onrequest`
Triggered after receiving a `request` message.

###### `oncancel`
Triggered after receiving a `cancel` message.

###### `ondata`
Triggered after receiving a `data` message.

#### The dWeb Lifecycle
It is crucial to understand how DWDHT and dDatabase can be used in tandem and how they both, when layered on top of one another, create a truly decentralized data transfer network, which help form the dWeb. Below is an example of what I call the `dWeb Lifecycle` - how data is created, announced, discovered by a remote, downloaded and then re-announced by the remote.

-1. Bob creates a dDatabase, and appends "Hello" to the feed, at index 0.
-2. Bob opens a channel to share his dDatabase, and generates a dWeb network address for the dDatabase in the process.
-3. Bob announces the dWeb network address, along with his public IP address and the port number peers can connect with him on, via dWeb's DHT.
-4. Alice learns of Bob's network address and wants to access it, so Alice performs a lookup for Bob's network address on dWeb's DHT.
-5. The DHT returns a list of peers, which for now only includes Bob's public IP address and port number.
-6. Using the dDatabase protocol, Alice sends a `request` message for the entire index range to Bob, using the connection details found on the DHT. This also creates a binary duplex channel between Alice and Bob.
-7. Bob responds with a `data` message, with all the indices from his dDatabase.
-8. Alice chooses to keep the channel open, so that she can listen for updates to Bob's dDatabase, which she can now receive in real-time.
-9. Alice now announces Bob's dDatabase (its dWeb network address) on dWeb's DHT, adding her public IP and port to the swarm now surrounding Bob's dDatabase.
-10. Jim comes along and learns about Bob's dDatabase and performs a lookup of its network address on dWeb's DHT.
-11. dWeb's DHT returns a list of peers who are announcing (broadcasting) Bob's dDatabase (now Alice and Bob), from whom Jim can now download the dDatabase. All Jim has to do is form a `request` message, detailing the `index` range he'd like to download.

### dWebTrie
dWebTrie is an abstraction layer that utilizes [Hash Array Mapped Tries](https://en.wikipedia.org/wiki/hash_array_mapped_trie) in order to provide a general purpose, distributed key/value store over the [dDatabase Protocol](#ddatabase). dWeb is a single writer key/value store which is able to map key/value data to a matching dDatabase index using a builtin rolling hash array mapped trie.

A dWebTrie utilizes the underlying dDatabase's public/private key pair, as well as its dWeb network address, and is represented by this single dDatabase feed. A [dAppDB](https://github.com/distributedweb/dappdb) is a multiwriter distributed key/value store that can be represented by multiple dDatabase feeds and therefore multiple key pairs, although its dWeb network address is derived from the dAppDB's initial dDatabase feed's public key.

dWeb was created as a way of storing an entire file system, with possibly thousands or even millions of files, within a dDatabase, where keys are path-like strings (e.g., /make/the/web/great/again) and values are arbitrary binary blobs. Without the dWeb abstraction, a distributed file system like [dDrive](#ddrive) simply wasn't possible, since there would have been too many complexities and bottlenecks if a protocol like dDrive would have used dDatabase directly. Optionally, we could have built a trie-structured key/value API within a dDrive implementation, but that would have been far too much overhead. It made more sense to develop a separate trie-structured key/value API, layered on dDatabase, so that many other protocols and applications could use dWeb as an off-chain distributed database management system.

#### Database Semantics
-Keys can be any UTF-8 string (e.g. "maga")
-If a key uses path segments (like a folder or file location) (e.g., /this/folder) path segments must be separated by the forward slash character ( /). Repeated slashes (//) are not allowed.
-Leading and trailing (/) are options (e.g., "/hello" and "hello" are equal).
-A key can be both a path segment and a key at the same time (e.g., /a/b/c and /a/b can both be keys at the same time).
-Values can be any binary blob, including an empty blob (zero bit length).
-Acceptable values can be UTF-8 encoded strings, JSON encoded objects, protobuf messages, or a raw uint64 integer (endianness does not matter).
-Length of value (in bits) is the only form of type or metadata stored about the value.
-Deserialization and validation are left to library and application developers.

#### dWebTrie Representation
Below is a pseudo-representation of a dWebTrie database:
```
Key                    Value
= = = = = = = = = = = = = = = = = = = =
Name                Jared Rice Sr.
Location            Dallas, TX
```

While this database is truly logical, since it is actually just a bunch of binary-based blob entries within a dDatabase feed, using dWeb's abstract layer, one could easily execute `get(name)`, which would return `Jared Rice Sr.` This is far simpler than working directly with the dDatabase feed, which would require farm more steps and much more programming overhead.

#### Database API
A dWebTrie-based database is created by opening an existing dDatabase feed with dWebTrie content or by simply creating a new dDatabase feed.

The following API calls are part of the dWebTrie standard:

##### `put(key, value)`
Inserts a value of an arbitrary byte size under the specified key. Requires read-write access. Returns an error via callback if there is an issue.

##### `get(key)`
Retrieves the value for a given key.

##### `delete(key)`
Retrieves the value for a given key.

##### `list(prefix)`
Returns a flat (non-nested) list of all keys currently in the database under the given prefix.

##### `batch(batch)`
Insert/delete multiple values atomically.

##### `watch(prefix)`
Watch a prefix of the DB (or key) and get notifications when it changes.

#### Overhead and Scaling
Depending on the size of the database, the metadata overhead can vary.

Consider the case of a two-path-segment key, with an entirely saturated trie and a uint32-sized feed and entry index points:

-`trie:` 4 * 2 * 64 bytes = 512 bytes
-`total:` 512 bytes

In a light-case, with few trie entries and single-byte varint feed and entry index pointers:
-`trie:` 2 * 2 * 4 bytes = 16 bytes
=`total:` 16 bytes

For a database with most keys having `N` path segments, the cost of a `get()` scales with the number of entries `M`, as `O(log(M))` with the best case `1` lookup and the worst case `4 * 32 * N - 128 * N` lookups.

The cost of `put()` or `delete()` is proportional to the cost of `get()`. The cost of `list()` is linear `(O(M))` in the number of matching entries, plus the cost of a single `get()`.

The total metadata overhead for a dWebTrie-based database with `M` entries, scales with the `O(M log(M))`.

#### dWebTrie Schemas
A dWebTrie-based dDatabase feed consists of a sequence of protobuf-encoded message of `Entry` or `InflatedEntry` type. A "protocol header" header entry should be the first entry in the feed, using "dWebTrie" as the `type`. dWebTrie itself does not specify the content of the optional header extension field, as higher level protocols are left to handle this portion of the protocol.

When data doesn't fit, due to the limited size constraint, for any given value, there is a second `content` feed associated with the dWebTrie key/value feed. The optional `contentFeed` field described in the schema below is used to identify a dWebTrie key/value feed, that needs a second `content` feed.

The sequence of entries includes an incremental index, where the most-recently appended entry in the feed contains metadata pointers that can be followed to efficiently locate any key in the database, without having to perform a linear scan across the database's entire history, or generate an index data structure that's independent of the database itself. Although it is completely up to the implementation on whether they choose to implement their own index or not.

The protobuf message schemas for `Entry` and `InflatedEntry` are:
```
message Entry {
  message InflatedEntry
  required string key = 1;
  optional bytes value = 2;
  optional bool deleted = 3;
  required bytes trie = 4;
  repeated uint64 clock = 5;
  optional uint64 inflate = 6;
  repeated Feed feeds = 7;
  optional bytes contentFeed  = 8;
}
```

The fields that are common to both message types are:
-`key` - UTF-8 key that this node describes. All slashes are removed before storing in message.
-`value` - Byte array of an arbitrary size.
-`deleted` - Boolean that converts entry into a "dead" entry, if `true`. It is recommended that if `false` to keep this value `undefined`, rather than using `false`. This can also be used to store user-defined metadata related to the deletion.
-`trie` - A structured array of pointers to other `Entry` entries in the dDatabase feed. This is used for navigating the tree of keys.
-`inflate` - A reference to the feed index number of the most recent `InflatedEntry` entry in the feed. **NOTE:** This should not be set on a feed's first `InflatedEntry` entry.
-`contentLog` - For applications that require a parallel `content` dDatabase feed. This filed is used to store the 43-byte public key for that feed. It is sufficient to write a single `InflatedEntry` message in the dDatabase feed, with feeds containing a single entry (a pointer to the current feed itself) and `ContentLog` optionally set to a pointer to a paired content feed. The `Entry` type can be used for all other messages, with `inflate` pointing back to the single `InflatedEntry` message.

#### Key Path Hashing
Every key path has a fixed-size hash representation that is used by the trie. When all path segments are concatenated, they are combined into what is known as a `path hash array`. Similar in many ways to that of a `hash map data structure`, a `path hash array` can have collisions where one key (string) and another key have the same hash, without suffering any issues because of it. This is because each hash is a reference (pointer) to a linked-list `container` of Entries, which can be linearly iterated over until the sought after value is found.

Each element (segment) in a path equates to 32 values, which also equates to a 64-bit hash. The key `/presidents/trump` has two path segments (presidents and trump) which equates to a 65  element path has array, made up of 32 element hashes and a terminator.

The `SipHash-2-4` hash algorithm is used, along with an 8-byte output and a 16-byte key. The corresponding input is the UTF-8 encoded path string segment, excluding slashes or other separators, as well as any terminators. A 16-byte secret key is required where all zeros is used, for this particular use-case. When an 8-byte outputted hash is converted to an array of 2-bit bytes, the ordering of the hash array is handled byte-by-byte, where for each byte, take the two lowest-value bits as `byte index 0` in the hash array and the next two bits as `byte index 1`, etc. When path hashes are combined into an array of greater length, the left-most path element hash will relation to byte indices `0` to `31`. The terminator, `4`, will have the highest index in the hash array (right-most).

**Note:** dWebTrie derived from a project known as HyperDB and the following examples have been lifted from [DEP-0004[(https://datprotocol.com/deps/0004-hyperdb), which was the original specification/proposal for HyperDB. A special thanks to the incredible work of Mathias Buus, Bryan Newbold and Stephen Whitmore.

-For example, consider the key `/tree/willow`. `tree` has the following hash:
`[0xAC, 0xDC, 0x05, 0x6C, 0x63, 0x9D, 0x87, 0xCA]`

-This converts into the following array:
`[0,3,2,2,0,3,1,3,1,1,0,0,0,3,2,1,3,0,2,1,1,3,1,2,3,1,0,2,2,2,0,3]`

-`willow` has a 64-bit hash:
`[0x72, 0x30, 0x34, 0x39, 0x35, 0xA8, 0x21, 0x44]`

-This converts into the following array:
`[2,0,3,1,0,0,3,0,0,1,3,0,1,0,3,0,1,1,3,0,0,2,2,2,1,0,2,0,0,1,0,1]`

-These two combine into the unified byte array with 65 elements:
`[0,3,2,2,0,3,1,3,1,1,0,0,0,3,2,1,3,0,2,1,1,3,1,2,3,1,0,2,2,2,2,0,3,2,0,3,1,0,0,3,0,0,1,3,0,1,2,3,0,1,1,3,0,0,2,2,2,1,0,2,0,0,1,0,1,4]`

In another example, the key `/a/b/c` converts into a 96-byte hash array, i.e., `32 + 32 + 32 + 1`. (1) in this case the terminator bit (4).

##### Incremental Index Trie
Each individual node stores a prefix trie that can be used to lookup other keys and can also be used to list all keys that are related to a given prefix. The prefix trie is stored in the `trie` field within an `Entry` message, as referenced in [dWebTrie Schema(#dwebtrie-schema).

Simply put, the `trie` is equal to the `path hash array`. As mentioned in the schema, each individual element within a `trie` is referred to as a `container`. Each container that isn't empty, is a pointer to the newest `Entries`, where the path is equal (identical) up to that specific prefix location. This is true, because each `trie` has 4 values at each node, which means there can be a pointer to up to 3 other values at a given element in the trie array. (Containers can be empty, if at that particular node, there are zero `branches`).

**NOTE:** Only non-null elements will be transmitted as stored on disk.

The trie data structure is a sparse array of pointers to other `Entry` entries. Each pointer references a specific feed, which indexes to the same value.

###### Looking Up A Key In A Database
The process for key lookups, is to:
-1. Calculate the `path hash array` for the key you are looking for.
-2. Select the most recent ("latest") `Entry` in the feed.
-3. Compare `path hash arrays` for exactly matching paths.
-4. If they match exactly, then compare keys. If keys match, the lookup was successful.
-5. Check whether the `deleted` flag of the `Entry` schema is set. If so, this entry actually represents the deletion of the `Entry`.
-6. If the path segments (concatenated) match, look for a pointer in the last `trie array index` and iterate from step #3 with the new `Entry`.
-7. If the path segments (concatenated) do not match, find the first index in each `path hash array` where both arrays differ and look up the corresponding element in this `Entry's` trie array.
-8. If the element is empty, or doesn't contain a pointer corresponding to your 2-bit value, then the key does not exist in the dWebTrie.
-9. If the trie element is not empty, then follow that pointer to select the next `Entry`. Recursively repeat this process from step #3, which will allow you to descend the trie in a search where the search will either terminate in your search or find that the key is not defined in the dWebTrie.

###### Writing A Key To A Database
In order to write a key to a dWebTrie database, follow the specified process below:
-1. Calculate the `path hash array` for the key to be stored and start with an empty trie array of the same length; where writing to this trie array will start at the current index of `0`.
-2. Select the most-recent ("latest") `Entry` in the feed.
-3. Compare `path hash arrays` for exactly matching paths.
-4. If they match exactly, then compare keys. If keys match, then you are overwriting the current `Entry` and can copy the `remainder` of its trie up to the current trie index. **NOTE:** When I say "overwriting", the previous `Entry` is not removed, for the simple reason that the dDatabase feed below the dWebTrie is immutable and append-only. A more-recent `Entry` is created with its own schema, which could, for example, set the `deleted` flag to true, which would then flag this `Entry` as deleted for future lookups. This works because in immutable datasets, where the management of state is paramount, we can see the entire lifetime of the data, from the moment it was created to the moment it was deleted. It's also important to note that during the lookup process, by selecting the "most-recent" `Entry`  for a key in the database, we are pulling its most-recent state.
-5. If the path segments (concatenated) match, but not the keys, then you are adding a new key to an existing `hash container`. Copy the trie array and extend it to the full length and add a pointer at the last index of the array of the same hash as the `Entry`.
-6. If the path segments (concatenated) do no match, compare both trie arrays and find the differing portion of the array. Copy all of the elements of the trie array, between the `current index` and the `diff index`, into a new trie array. **NOTE:** It doesn't matter whether the located trie array is empty or not.
-7. In this `Entry's` trie array, look up the corresponding element at the `diff index`. If empty, then the most similar `Entry` has been located.
-8. Create a pointer to this node, to the trie at the `diff index`, and the write process is finished. **NOTE:** All remaining trie elements will be empty and can be removed.
-9. If the `diff index` has a pointer, this means it isn't empty. If a pointer exists, follow that pointer to the next `Entry`. Recursively repeat this process from step #3.

**NOTE:** To delete a key/value, follow the same write procedure above and set `deleted` to `true` in the `Entry`. `Deletion nodes` will persist in the database forever.

##### Trie Encoding
Trie data structures are encoded into a variable length byte string as the `trie` field of an `Entry` message. It's important to reiterate that trie data structures are simply sparse, indexed arrays of pointers to entries.

**NOTE:** The following encoding schema and examples were also lifted from `DEP-0004`; only `buckets` in a dWebTrie are referred to as `containers`.

Consider a trie array with `N` `containers` and `M` non-containers `(O <= M <= N)`. In the encoded trie field, there will be `M` concatenated bytestrings in the following form:

`trie index (varint) | bucket bitfield * (packed in varint) | pointer sets **`

- * - Bitfield encodes which of the 5 values (4 values if the index is not mod 32) at this node of the trie have pointers.
- ** - Pointer sets each reference an entry at (feed index, entry index), where `feed index` is a varint with an extra "more pointers at this value" low bit, encoded as `feed_index << 1 | more_bit` and where `entry index` is simply a varint.

When dealing with a small/sparse dWebTrie, there will be a small number of non-empty containers; put another way, a small/sparse dWebTrie will have a small `M`. For a very large/dense dWebTrie (millions of key/value pairs), there will be many non-empty containers; put another way, `M` will approach `N` and containers may have up to the full 4 pointer sets.

-Consider an entry with a path hash:
`[1,1,0,0,3,1,2,3,3,1,1,1,2,2,1,1,1,0,2,3,3,0,1,2,1,1,2,3,0,0,2,1,0,2,1,0,1,1,0,1,0,1,3,1,0,0,2,3,0,1,3,2,0,3,2,0,1,0,3,2,0,2,1,1,4]`

-and the `trie`:
`[, ]`

In this case, `N = 64` (or you could count as 2, if you ignore trailing empty entries) and `M = 1`.

There will be a single bytestring fragment (chunk):

-`trie index` varint = 1 (second element in trie array).
-`bitfield` (varint 2) = 0b0010
-There is only `pointer set` for value `1` (the second value).
-There is a single pointer in the `pointer set` which is: `feed = 0 << 1 | 0, index = 1` or `(varint 2, varint 1)`.
-Combined, the `trie bytestring` will be:
`[0x01, 0x02, 0x02, 0x02]`

For a more complex example, consider the same `path hash array` with the `trie array`:
`[,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,]`

 If we `db.put ('/a/b', '24')`, we expect to see a single `Entry` of the `InflatedEntry` type at `index1` of the underlying dDatabase feed.

For reference, the first 104 bytes in the path match those of  the `/a/b/c` example from earlier, because it shares two of its 3 path segments. Since this is the second entry, the `entry index` is `1`.

**NOTE:** There is a major different between an `entry index` and an `array index`. An `entry index` is a record (new index) added to the dWebTrie's underlying dDatabase, while a specific index within a trie array, is its position from within the array. To make sure that I kill all confusion, consider the below example:

The example of `/a/b` being `put` into the key-value store, if data was in plain English and not binary, would look something like this pseudo-representation, once inside a dDatabase feed:

```
Index                           Value
= = = = = = = = = = = = = = = = = = = = = = = = = = =
1                                 {
                                     type: dwebtrie,
                                     key: Entry,
                                     value: 24,
                                     deleted: null,
                                     trie: [0x01, 0x02, 0x02, 0x02],
                                     ...
                                   {
```

The placement (index) with a dDatabase and the placement of an element in the above trie array, are clearly apples and oranges and I did not want there to be any confusion.

Now, if we `db.put('/a/c', 'hello')` and expect a second `Entry` of `Entry` type, the `path hash array` for this key (index 2) is:
`[1,2,0,1,2,0,2,2,3,0,1,2,1,3,0,3,0,0,2,1,0,2,0,0,2,0,0,3,2,1,1,2,0,1,1,0,1,2,3,2,2,2,0,0,3,1,2,1,3,3,3,3,3,3,0,3,3,2,3,2,3,0,1,0,4]`

The first 32 characters of this path are common with the first `Entry` (they share a common prefix, `/a`). `trie` is defined, but is mostly sparse. The first 32 elements of common prefix match the first `Entry` and then two additional hash elements ([0,1]) happen to match as well. There is not a `diff index`, until `index 34` of the array (zero-indexed). At this entry, there is a reference pointing to the first `Entry`. An additional 29 trailing null entries have been trimmed in the reduced metadata overhead.

Next, we insert a third node with `db.put('/x/y', 'other')`, and get a third `Entry`, where the `path hash array` is (index 3) is:
`[1,1,0,0,3,1,2,3,3,1,1,1,2,2,1,1,1,0,2,3,3,0,1,2,1,1,2,3,0,0,2,1,0,2,1,0,1,1,0,1,0,1,0,3,1,0,0,2,3,0,1,3,2,0,1,3,2,0,1,0,3,2,0,2,1,1,4]`

Consider the lookup process for `db.get('/a/b')`, which we expect to return `24`, as written in the first `Entry`. First, we calculate the path for the key `a/b`, which will be the same as the first `Entry`. Then we take the "latest" (most-recent) Entry, with entry `index 3`. We compare the `path hash arrays`, starting at the first element, and find the `diff index` at `index 1` of the array `(1 == 1, then 1 ! = 2)`.

We look at `index 1` in the current `Entry's` trie, and find a pointer to the entry at `index 2` of the underlying dDatabase itself, so we fetch the `Entry` and recurse. Comparing `path hash arrays`, we now get all the way to `index 34` of the array before there is a difference (`diff index`). We again look at the trie, find a pointer to the entry at `index 1` of the underlying dDatabase and fetch the first `Entry` and recurse. The path elements of the entry at `index 1` of the feed (dDatabase feed) match exactly and therefore, we have found the entry we are looking for and it has an existing value, so we return the value, which is `24`.

Lastly, consider a lookup for `db.get('/a/z')`. This key does not exist, so we expect dWebTrie to return with `key not found`. We calculate the `hash path array` for this key:
`[1,2,0,1,2,0,2,2,3,0,1,2,1,3,0,3,0,0,2,1,0,2,0,0,2,0,0,36,2,1,1,2,1,2,3,0,1,0,1,1,1,1,1,2,1,1,1,0,1,0,3,3,2,0,3,3,1,1,0,23,1,0,1,1,2,4]`

Similar to the first lookup, we start with the `entry index` 3, and follow the pointer to `entry index` 2. This time, when we compare `path hash arrays`, the first `diff index` is at `array index` 32. There is not a trie entry at this index, which tells us that the key does not exist in the database.

###### Listing A Prefix
Continuing on with the current state of the dDatabase below dWebTrie abstraction layer, we call `db.list('/a')`, to list all keys with the prefix `/a`.

We generate a `path hash array` for the key `/a` without the terminating symbol (4):
`[1,2,0,1,2,0,2,2,3,0,1,2,1,3,0,3,0,0,2,1,0,2,0,0,2,0,0,3,2,1,1,2]`

Using the same process as the `get()` lookup, we find the first `Entry` that entirely matches the prefix, which is `entry index` 2. If we had failed to locate any `Entry` with a complete prefix match, then we would return an empty list of matching keys. If we start with the first prefix-match node, we save that key as a match (unless `deleted` is `true` in the `Entry`). Then, select all trie pointers with a higher index than the length of the prefix and recursively inspect all pointer-to `Entries` and the `Entries` they point to and so on, until a complete list tree is built.

#### The Baseline Foundation For A Distributed File System
For a server-less web like the dWeb to work, it would have to find a way to not only transfer data between peers, but to transfer entire files and therefore, entire file systems between peers, just as servers do when teamed with the HTTP protocol on a server-to-client basis. Doing this on a peer-to-peer basis, as the dWeb has accomplished, not only requires a protocol for exchanging index abstract data blobs between peers (dDatabase), but another protocol (dWebTrie) layered above this for creating a file system-like logical and persistent database structure, which utilizes a key-value format, so that file-paths (keys) can be appended to a blob, with file data (values) and efficiently traversed by receiving peers performing specific lookups.

This way, an entire file system can be stored within a single file (a single dDatabase feed) and any peer in the world who is able to locate the feed via dWeb's DHT, can then communicate with the peers within dDatabase's swarm, request the dDatabase and subsequently download the dDatabase to their machine and easily consume the data via higher-level abstractions (like dWebTrie, or even higher levels like dDrive's actual file system layer).

dWebTrie was designed to store the data that derives from an entire file system within a dDatabase and allow higher-level abstractions like [dDrive](#ddrive) to consume that data and reproduce the entire file system on a remote peer's device. dWebTrie can traverse millions of records within a dDatabase and relate specific path segments via `list()`, without any bottlenecks. This makes dWebTrie the perfect file system database for a distributed file system, considering it is built on top of a distributed, append-only data feed that can be exchanged between peers in real-time. I can't make the rationale for dWebTrie as a file system for distributed file systems any clearer, other than to say that our current dWebTrie implementation scales with `O(N log(N))`.

###dDrive
dDrive is a secure, real-time distributed file system abstraction layer on top of dWebTrie, designed for the real-time exchange of file systems on a peer-to-peer basis. When comparing the dWeb to the World Wide Web, one could easily arrive at the conclusion that a dDrive is the equivalent to a server on the World Wide Web for some use-cases, like handling a client's request for a specific file. This is somewhat true, since a dDrive can be discovered on dWeb's DHT and a request can be sent to the peer(s) in its underlying swarm for specific files, or the entire file system for that matter - although the process of doing this on the dWeb is more secure and far more efficient.

**NOTE:** a dDrive cannot replace a server and its ability to act as a backend for both user authentication and remote application execution (currently, at the time of this writing). Web applications within a dDrive can utilize blockchain-based protocol suites like dWeb's [ARISEN](#arisen) to build truly serverless and decentralized applications that do require a backend and/or user authentication.

Just as I noted within my explanation of the dWebTrie, dDrive is also completely logical; in other words, it doesn't technically exist. A dDrive is an abstraction for storing files from a file system as dWebTrie-based `Entries` within a dDatabase and reproducing the actual file system and its files from the same `Entries`.

Consider the following pseudo-representation of how files in a dDrive are logically referenced via a dWebTrie and actually stored within a dDatabase:

-Alice creates a dDrive with her website files:
```
index.html
img/logo.png
```

-dDrive creates the following key-value entries into its underlying dWebTrie (put):
```
Key                              Value
= = = = = = = = = = = = = = = = = = = = = = = = = =
/index.html                 <html><head>...
/img/logo.png             8fe1e4... (image file converted to hexadecimal notation)
```

-dWebTrie then creates a dDatabase and the following entries:
```
Index                           Entry
= = = = = = = = = = = = = = = = = = = = = = = = = =
1                                 {
                                     type: dwebtrie,
                                     message: Entry,
                                     key: /index.html,
                                     value: <byte array of file data>
                                     deleted: null,
                                     ...
                                   }

2                                 {
                                     type: dwebtrie,
                                     message: Entry,
                                     key: /img/logo.png,
                                     value: <byte array of file data>,
                                     deleted: null,
                                     ...
                                   }
```

**NOTE:** The first entry in the feed (index 0), as noted in [dWebTrie Schema](#dwebtrie-schema) is a special protocol header entry, which is not shown in the above example in order to keep all pseudo-representations simple.

-dDatabase then encodes both entries in binary notation and stores them in the feed:
```
Index                           Entry
= = = = = = = = = = = = = = = = = = = = = = = = = =
0                                 01011101101010...
1                                 00101001110101110...
2                                 0101101011011001...
```

#### dWeb Network Address
Since a dDrive is simply a dDatabase, it uses the dDatabase's public/private keypair and therefore its dWeb network address.

Remember, a dWeb network address is simply a 32 byte hexadecimal hash and in the case of a dDrive, its a 32 byte hexadecimal hash of its underlying dDatabase's public key.

The following is an example of a dWeb network address:
`40a7f6b6147ae695bcbcff432f684c7bb529lea339c28c1755896cdeb80bd2f9`

#### dDatabase Protocol Headers
You will recall in [dWebTrie Schema](#dwebtrie-schema) that the first entry in its underlying dDatabase, contains a `protocol header`. Protocol headers at `index 0` are used to decipher what the proceeding entries in the dDatabase are related to.

Below are the fields included in a dDatabase's `protocol header`:

| Field No. | Name | Type | Description |
| --- | --- | ---- | --- |
| 1 | Type | Length-prefixed | The type of data in the dDatabase |
| 2 | Content | Length-prefixed | 32-byte public key of content feed (if type is `ddrive`) |

If the `type` is `dwebtrie`, then we know there is a single dDatabase feed and that the overlaying dWebTrie abstraction layer has formatted the data as a key-value store. If the `type` is `ddrive`, then we know there is a separate `content` feed (as explained in the next section [Content and Metadata Feeds](#content-and-metadata-feeds) ), whose 32-byte public key can be found in the `Content` field of the feed's header.

The `Content` field allows two or more feeds to be coupled together, while the overall `protocol header` is used to distinguish the data set type within an underlying dDatabase. This simple header, as you will see later in this paper, is giving way to the creation of many decentralized technologies, outside of just a decentralized web, like the dWeb.

That brings me to dDrive's coupling of dDatabase feeds.

#### Content and Metadata Feeds
Contrary to the pseudo-representation that was shown in the previous section, a dDrive doesn't use a single dDatabase, it uses two coupled feeds to represent files and folders. One feed is for metadata and the other represents files and folders. Put another way, one feed is for file metadata and the other feed is for the data related to files. The `metadata` dDatabase feed contains the names, sizes and other metadata for each file, and is typically sparse, even when the overlaying dDrive contains a large amount of files and folders. The `content` dDatabase feed contains the actually file contents and is of the plain "ddatabase" type, instead of the `dwebtrie` or overlaying `dDrive` type.

When a dDrive is created, dWebTrie is initiated and creates two underlying dDatabase feeds. In the protocol header of the `metadata` feed, the `Type` field is sent to `ddrive` and the `Content` field is set to the 32-byte public key of the `content` feed.

When a file is added to a dDrive, a dWebTrie `Inflated Entry` is created for the file metadata in the `metadata` feed and the actual file data is stored as an arbitrary data blob within the `content` feed. If you recall in the [dWebTrie Schema](#dwebtrie-schema), an `Inflated Entry` contains the `contentLog` field, to point to the location in the `content` dDatabase feed for a given dDrive, where the file contents, related to the `metadata` `Inflated Entry`, are located. Put another way, each `metadata` feed array points to where in the `content` feed the file data is located, so you only need to fetch the contents of files you are interested in.

Here is an updated pseudo-representation of a dDrive with a simple `index.html` file, so you can update your mental model of how a dDrive is actually constructed:

-A blank dDrive is created, where dWebTrie initiates the creation of a blank dDatabase for the `content` feed, and a blank dDatabase feed for the `metadata` feed.

-The following `protocol header` is added as the first entry in the `metadata` feed:
```
Index                            Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                  {
                                      Type: dDrive,
                                      Content: <public-key-of-content-dDatabase>,
                                    }
```

-A file called `index.html` is added to the dDrive and the following entries are added to the `content` and `metadata` feeds:
```
Metadata Feed:
--
Index                            Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                  { // Protocol Header
                                      Type: ddrive,
                                      Content: <public-key-of-content-database>
                                    }
-- newly added entry:
1                                  { // Metadata Inflated Entry
                                      key: /index.html,
                                      value: metadata-for-file,
                                      deleted: null,
                                      trie: <pointers to other files or folders where the path is created> (none in this case),
                                      contentLog: <index number in content feed, where file data is> (1)
                                    }
```
```
Content Feed
--
Index                            Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                  { // Protocol header
                                      Type: ddatabase
                                    }

1                                  010110... (Binary blob of index.html file content)
```

-In actuality, both dDatabases are truly in binary when it's all said and done:

```
Content Feed:
--
Index                             Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                   010110111...
1                                   1101110001...
```
```
Metadata Feed:
--
Index                             Entry
= = = = = = = = = = = = = = = = = = = = = = = = = = = =
0                                   01011101110...
1                                   10110111011...
```

There are many reasons the `content` feed doesn't use the dWebTrie abstraction layer/format for appending data, one of which is the limited value constraint of the `value` field in a dWebTrie's `InflatedEntry`. Other reasons include the fact that there is no need for pointers via the `trie array`, since related path segments are sorted via the `metadata` feed where a `trie array`, via each `Inflated Entry's` `trie` field, can be found.

It's actually quite simple to build a tree of files and folders via the `metadata` feed by relating `path segments`. After a tree is calculated, thanks to the location of the `content` feed in the protocol header and the `contentLog` field in each `Inflated Entry`, the contents of a file can easily be located.

#### Metadata Value Field Schema
In the previous pseudo-representation of how a dDrive is created, you will notice that in the `Inflated Entry`, within the `Metadata` feed of the `index.html` file, the `value` field is where the metadata is located. This field carries its own schema, as follows:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | Mode | uint32 (varint) | Unix permissions (setuid or setgia) |
| 2 | UID | uint32 (varint) | User ID (Unix) |
| 3 | GID | uint32 (varint) | Group ID (Unix) |
| 4 | Size | uint64 (varint) | File size in bytes |
| 5 | Blocks | uint64 (varint) | The number of object blocks |
| 6 | Offset | uint64 (varint) | Similar to Node's FS module |
| 7 | ByteOffset | uint64 (varint) | Similar to Node's FS module |
| 8 | MTIME | uint64 (varint) | Modified time |
| 9 | CTIME | uint64 (varint) | Created time |

This is packaged within a `protocol Buffers` encoded message and used as the value `field` within a dWebTrie-based `InflatedEntry` message, which is also encoded with `Protocol Buffers` and appended as an entry in the `metadata' feed.

#### Version Controlled
As mentioned in [dDatabase](#ddatabase), files transferred over `HTTP` are not versioned, which means the historical state of a file or files is not available. `HTTP` is used to transfer a file's or files' current state from the server it/they exist on, to the client they're being requested from. When it comes to the `DWEB` protocol and its underlying protocols, the underlying data is [immutable](#the-case-for-immutability), which means that any rendition of the underlying data's state can be requested and constructed. In the case of a dDrive, each time a file is added to a dDrive or the metadata/content of a file is modified, entries are made to its underlying dDatabase feeds, where each entry represents an instance of the dDrive's state.

Each modification to a dDrive represents a new version of the dDrive. String names can also be assigned to dDrive versions and these can be stored within the drive itself, creating a straightforward way to switch between semantically meaningful versions. Simply put, a dDrive is built on top of append-only logs where old versions of files are preserved by default. You can get a read-only snapshot of a specific version of a dDrive at any time. Additionally, as mentioned above, you can tag versions with string names, making them more parseable.

Tags are stored inside the dDrive's `hidden trie`, meaning they are not enumerable using dDrive's [standard filesystem methods](#standard-filesystem-methods). They will replicate with all other data in the drive, though.

For more information on version control method's, see dDrive's JavaScript implementation and its official documentation [here](https://github.com/distributedweb/ddrive#version-control).

#### Sparse Downloading
Now that it has been explained how a dDrive's file system is stored and versioned within two coupled dDatabase feeds, it becomes easier to understand how one peer can download a specific version of specific files from a specific dDrive, otherwise referred to as `Sparse Downloading`. Simply put, since all of the layers that overlay a dDatabase feed utilize its underlying [dDatabase Protocol](#ddatabase-protocol) for the request and fulfillment of data between peers, any of these abstraction layers can make a request for a specific index range, for whatever that equates to at a specific abstraction layer. For example, when a specific file or folder is requested at the dDrive layer, it's common for the entire `metadata` feed to be requested from the dDrive's swarm. Once received, the file or folder path is passed to the dWebTrie layer, which searches the `metadata` feed for the matching path. If the current version of the file or folder is wanted, rather than older versions, the `most-recent` `Inflated Entry` that matches the path is chosen. This `Inflated Entry` should contain a `contentLog` pointed to the `index` where the file's data is stored in the `content` feed. From there, a `request` message can be sent to the dDrive's swarm, using an underlying dDatabase Protocol-based duplex stream, where the specific `index` from the `contentLog` can be requested.

For example, if the link `dweb://<dweb-key>/index.html` is requested via a ` DWEB`-ready client, the entire dDrive in this instance is not downloaded; as a matter of fact, only the `index.html` file and any files linked within it are downloaded by the requesting peer.

This is how the process would take place:
-1. A remote peer requests `dweb://<dweb-key>/index.html` via a DWEB-ready client.
-2. The client looks up the dWeb key via dWeb's DHT and a list of peers who are announcing this dDrive (its swarm) are returned.
-3. The client chooses a peer from the returned list and requests to open a dDatabase protocol-based duplex channel on channel 1 with the peer (or even multiple peers).
-4. The client asks for only the `metadata` feed, once the peers accept the connection request, by sending a `request` message on the channel, requesting the entire `metadata` feed's index range.
-5. The peer sends a `data` message back over the duplex channel containing the `metadata` feed.
-6. The client passes the `/index.html` path segment to the dWebTrie abstraction layer, so it can perform a lookup in the `metadata` feed.
 -6. An `InflatedEntry` is found, with a `contentLog` field within the entry, pointing to `index 6` of the `content` feed.
-8. The client locates the `content` feed's public key in the `protocol header` of the `metadata` feed and opens another duplex channel on channel 2, with the `content` feed's public key, with the same peers from step #3 and sends a `request` message for `index 6`.
-9. The `content` feeds is sent in a `data` message to the client, containing **ONLY** `index 6`.

It should be noted, that at step #8, one could request the entire `content` feed, which would eliminate future remote requests, as long as both feeds are replicated live.

#### Drive Nesting
dDrive has a built-in "mounting" system, which allows one or more dDrive(s) to be mounted or "nested" within a parent dDrive. This allows Bob, to nest Alice's dDrive full of her published photos within his dDrive full of published photos. What is great about this, is that this enables true peer-to-peer collaboration, since when Alice adds a photo to her dDrive of published photos, they are automatically replicated within Bob's dDrive, since Alice's drive is nested within Bob's.

**NOTE:** It's important to note that Alice's dDrive is a totally separate dDrive from Bob's and can be accessed directly. Bob is simply a peer (seeder) of Alice's dDrive and mounts it within one his own dDrives, so that when a peer downloads Bob's dDrive, they also receive the most recent version of Alice's. In truth, this means that a portion of Bob's dDrive is downloaded from his dDrive's swarm and another portion from the swarm of Alice's dDrive.

For more information on dDrive mounting, please see dDrive's reference JavaScript implementation and its [Mounting docs](https://github.com/distributedweb/ddrive#ddrive-mounting).

#### Standard Filesystem Methods
What one will find is that a dDrive implementation, like our JavaScript implementation, truly mirrors the standard filesystem methods of the Node.js `fs` module. Below is a quick explanation of those methods.

##### `createReadStream`
Read a file out of the dDrive, as a stream. Similar to `fs.createReadStream`.

##### `readFile`
Read an entire file into memory. Similar to `fs.readFile`.

##### `createWriteStream`
Write a file as a stream. Similar to `fs.createWriteStream`.

##### `writeFile`
Write a file from a single buffer. Similar to `fs.writeFile`.

##### `unlink`
Unlinks (deletes) a file. Similar to `fs.unlink`.

##### `mkdir`
Creates a directory. Similar to `fs.mkdir`.

##### `rmdir`
Deletes an empty directory. Similar to `fs.rmdir`.

##### `readdir`
Lists a directory. Similar to `fs.readdir`.

##### `stat`
Stat an entry. Similar to `fs.stat`.

##### `lstat`
Stat an entry but do not follow symlinks. Similar to `fs.lstat`.

##### `info`
Get mount information about an entry.

##### `access`
Similar to `fs.access`.

##### `open`
Open a file and get a file descriptor back. Similar to `fs.open`.

##### `read`
Read from a file descriptor into a buffer. Similar to `fs.read`.

##### `write`
Write from a buffer into a file descriptor. Similar to `fs.write`.

##### `symlink`
Create a symlink from a link name to a target name.

##### `mount`
Mounts another dDrive at the specified mountpoint. A mount can be a specific version of a dDrive.

##### `unmount`
Unmount a previously-mounted dDrive.

#####  `createMountStream`
Create a stream containing content/metadata feeds for all mounted ddrives.

##### `getAllMounts`
Returns a `Map` of the content/metadata feeds for all mounted ddrives, keyed by their mountpoints.

##### `close`
Close a file. Similar to `fs.close`.

##### `version`
Returns the current version of the drive.

##### `key`
Returns public key indentifying the drive.

##### `discoveryKey`
Returns a key derived from the public key (the dweb network address).

##### `writable`
Returns a boolean indicating whether the drive is writable.

##### `peers`
Returns a list of peers currently replicating with a dDrive (via dweb's DHT).

##### `checkout`
Checkout a read-only copy of the dDrive at an old version.

##### `createTag`
Create a tag that maps to a given version. If a version is not provided, the current version will be used.

##### `getTaggedVersion`
Returns a version, corresponding to a tag.

##### `deleteTag`
Delete a tag. If the tag doesn't exist, this will be a no-op.

##### `getAllTags`
Return a `Map` of all tags.

##### `download`
Download all files, in path of current version. If no path is specified, this will download all files.

For more detailed documentation, please see dDrive's JavaScript implementation [here](https://github.com/distributedweb/dDrive).

#### Live Replication
A dDrive can be live-replicated, by keeping two duplex streams open with online peer(s) of a dDrive's swarm, while sending a content `request` message to said peers, for the entire index range of both the `content` and `metadata` feeds. This is easily done via the dDrive layer by simply using the [`replicate`](https://github.com/distributedweb/ddrive#replication). method in a dDrive implementation.

Replication can also take place on a per-file or per-path basis, rather than downloading updates for an entire dDrive's underlying files/paths within its filesystem.

##### dDrive Auditing
Since a dDrive is technically two dDatabase feeds, the data is easily  audited and validated as to having derived from the holder of the public-private keypair, and the dweb network address that mathematically derived from the public key.

Also, since a dWebTrie and a dDatabase are both single-writer, it's an easily proven fact that only the creator of a dDrive can write to it, preventing outside forces from, for example, manipulating a website's content. It is also easily provable that data within a dDrive has not been altered, since it's an append-only log. For more information on how data within a dDatabase is validated, please read about dDatabase's [Merkle Trees & FIO Trees](#merkle-tree-and-fio-trees).

### ARISEN
ARISEN is a suite of blockchain protocols, powered via smart contracts, that also acts as a development framework for dWeb-based applications, by providing a distributed virtual machine (global computer), a smart contract engine, a universal authentication layer, decentralized network consensus, decentralized payments, a decentralized domain name system and a decentralized reporting system.

#### On-Chain Vs. Off-Chain Protocols
While dWeb's DHT (DWDHT), dDatabase (DDB), dWebTrie (DWT) and dDrive (DDRIVE) form dWeb's "off-chain" protocol suite, ARISEN forms the dWeb's "on-chain" protocol suite. It's important to explain the difference between `off-chain` protocols and `on-chain` protocols, other that what is obvious. While blockchain technology is great for many use-cases, as are peer-to-peer networks like the one formed by dWeb's `off-chain` protocol suite, both of these have obvious deficiencies and crystal-clear bottlenecks when either is used independently in the formation of a decentralized web. Although, when both are combined, a decentralized web, with its own distributed and decentralized application development suite, is brought to life. Here were some of the issues we were able to solve through the combination of ARISEN and dWeb's `off-chain protocols`:

-1. Since an application and its files that are distributed to users via a dDrive are completely open source, a distributed database of some sort would have to be distributed in the dDrive so that users of the web application and their actions within the app could be written to this nested database. (**NOTE:** this database would be located within a separate dDrive, which would be nested with the app's dDrive. The database would have to be a multi-writer distributed database, like [dAppDB](https://github.com/distributedweb/dappdb), that allows multiple users to write to the database, since a dAppDB is make up of a constantly growing web of dDatabases, each of which is written to by a unique user (more on this in [Multi-Writer Databases](#multi-writer-databases)). This requires the initial creator of the dAppDB to constantly authorize new keys and therefore introduces a slight bit of centralization, since the dDrive creator would have to approve the authorization from their end; a change that would update the state of the nested dDrive, which would replicate to all of the app's users in real-time as well. This creates an issue where an app's creator could decide which users can use their application, through their ability to choose who they authorize to write to the app's database.

As you will see in subsequent issues within this section, ARISEN easily provides a decentralized, on-chain data persistence layer for applications which CAN allow for the use of off-chain distributed databases, that which the application itself would be unable to control.

-2. Applications have users and need a decentralized user management system of some sort, where users are in control of their authentication details. Centralized applications store user credentials within a centralized database, typically have their own integrated user and user permissions system and therefore control who can and who cannot use their applications. In the world of decentralized applications, a user management system cannot be managed by the app developer, even if it's open and based around public key cryptography, due to the fact an application could still manage to choose who can and cannot use their application. dDrives are controlled by their creator, wile a blockchain provides a decentralized and trusted third party for user authentication. A blockchain protocol suite like ARISEN, that has an onboard smart contract engine, allows an application developer to isolate actions that require user input and therefore user authentication within a smart contract that's stored on ARISEN's blockchain. Actions within a smart contract (i.e., creating a post on a social network) can be executed from the dDrive's application files (outside of the blockchain), where the action is initiated by a blockchain-based account, signed with the private key of the account and broadcasted by the blockchain, where it can be validated by trusted (in the case of ARISEN, elected) members of the blockchain network. Where, if verified, the data that derives from the action is stored in a database that is related to the smart contract on ARISEN. This clearly solves the database issue faced in #1, where multiple users can write to an application's database without the application developer preventing certain users from doing so, since the application developer is not in control of the blockchain, like he is in control of the app's dDrive. The on-chain database could be used to point to off-chain distributed databases that only users are in control of for data storage. This also insures that an application doesn't have to rely solely on a blockchain, instead using it as a pointer to off-chain databases. Beyond ARISEN providing a user management system, it provides `human-readable` usernames and an access-control system (permission system), which applications can easily be designed around.

-3. The dWeb, for it to be used by non-technical users, needs a decentralized domain name system (dDNS), since 64-character hexadecimal addresses are too difficult, if not impossible, to memorize. Centralized domain names would not work for several reasons, one being that the standard DNS system is not designed to work with dWeb addresses and two, as I noted in the [Preface](#preface) of this paper, centralized domains can be seized by tyrannical entities. dWeb's off-chain protocols like DWDHT could be used for a domain name system, but I have determined that actions of domains, like the actions of accounts, need to be cryptographically validated. Also, with DHT, someone would have to control the "issuance" of dTLDs. ARISEN allows for the creation of premium accounts that can be won via auction, which can be used to create a sub-account that contains a period (.). For example, the account "dcom" can be won, and used to create a subaccount like "website.dcom", which resembles a domain and has its own permission-levels and associated keypairs, since it can authenticate with ARISEN (like any other account). The won name clearly acts as a dTLD and auctions clearly decentralize the issuance. A smart contract can then be developed for creating dDNS records for a domain, where the domain itself has to sign for the creation of a record. For example, this contract would have an `add` action that accepts the following parameters:
-`domain` (account used for validation)
-`record_name`
-`ttl`
-`class`
-`type`
-`rdata` (dWeb Network Address)

Like I mentioned in #2, each action would require an account (in this case a domain) to sign for the action. Once signed and validated, the above record would be stored in a database on ARISEN, logically associated with the contract and the domains as well, so that record lookups via ARISEN's API can be unique to a domain - more on this in [On-Chain Data Persistence])#on-chain-data-persistence).

-4. Then comes the issue of payments. Since applications on the dWeb are distributed openly within a dDrive, integrating centralized forms of payment simply isn't possible, since the integration with these types of systems would reveal API keys (secret keys) used to integrate and authenticate with these services (e.g., PayPal). This forces applications to utilize decentralized forms of payments via networks like ARISEN (RIX), Bitcoin (BTC), EOS (EOS) and others. Obviously, the dWeb is blockchain agnostic when it comes to application development, but the dWeb protocol, as explained in (DWEB)(#dweb), bridges both the off-chain and on-chain protocols discussed in this paper, for reasons discussed later. While off-chain currencies like CloudCoin could certainly be integrated, there are still questions regarding the centralization of the DNS servers CloudCoin uses as a way of validating the authenticity of CloudCoins; although, this doesn't create an issue where the apps developers or an outside entity can seize CloudCoins. Technically, anyone could launch their own validation system and, since CloudCoin is distributed within image files, there is no ledger or transaction history, introducing anonymity, which might be avoided by some developers.

While a similar system like CloudCoin could be developed using dWeb's DHT, we have not embarked on such a project but we certainly will in the future.

-5. The downsides of a decentralized web with no checks in place would allow for the illegal distribution of narcotics and child pornography. I will certainly mention terrorist activity, but I won't over-emphasize since it is consistently over-exaggerated by various government entities to circumvent the Bill of Rights and discourage the use of certain privacy-enabling technologies. If the dWeb only consisted of off-chain protocols, the dWeb would act as a Torrent network for websites and web applications and would certainly be a free-for-all for criminals. Again, ARISEN's blockchain protocol became the solution to this issue, through its builtin election/voting system and 21-member elected governance, which has the power through a 15/21 majority vote to reverse transactions (actions). Since dWeb network addresses are registered on ARISEN (see [DWEB Protocol](#DWEB) and domains, as well as dDNS records are controlled and stored on ARISEN (see [dDNS](#ddns)), the governance could halt the activity taking place on specific dWeb network addresses and domains, further protecting the network and dWeb's users from illegal and nefarious activity.

It became important, in our eyes, to create a reporting system using an ARISEN smart contract and integrate it with other contracts, like `dWeb` and `arisen.wrap`, so that community members could report illegal activity and vote on its submissions to the governance for a `removal vote`. This gave way to the creation of dWeb's ratified [Constitution](#constitution), which governance members must abide by in the removal of illegal content. The Constitution protects free speech and many other human rights.

dWeb's off-chain protocols are incapable of providing this sort of solution, since it requires all of the solutions explained in #1 through #4.

It was essential that I started off the ARISEN section explaining why a blockchain protocol suite was needed, along with the differenced between `on-chain protocols` and `off-chain protocols`. `On-chain protocols` provide a decentralized and trusted network that help bring to life the necessary facilities needed to allow for the development of web applications that provide end-to-end decentralization for their users, while also placing checks on the content that is distributed between peers via `off-chain protocols`. This bridge between both protocol types is what forms what we call the `dWeb`, hence the reason why we named the protocol that bridges the `off-chain` and `on-chain` networks the [DWEB Protocol](#dweb).

The sub-sections that follow explain the ins-and-outs of ARISEN and its underlying protocols, algorithms and features.

**NOTE:** If it is your view that #1 through #5 introduced too much too quickly, it is my view that many developers discourage the inclusion of blockchain in to many projects. I felt like it was appropriate to explain first, why the dWeb combined with `on-chain` and `off-chain` protocols and second, why `off-chain` protocols were incapable of accomplishing what `on-chain` protocols can with exceptional simplicity and efficiency. If any of the previous section confused you, due to a lack of understanding of a `singleton computer` like ARISEN, all of what was explained in these sections is thoroughly detailed in subsequent sections.

#### The Global Computer
ARISEN can be described as a global computer, in other words, it's much more than just a blockchain. You can think of a block as a bunch of protocols that form decentralized information exchange platforms. From a computer science perspective, ARISEN is a deterministic but practically unbounded state machine, consisting of a globally accessible singleton state, along with a globally distributed virtual machine (see [RSN VM](#rsn-vm)) which is designed to programmatically apply changes to ARISEN's overall state.

From a practical perspective, you could think of ARISEN as a massive, multiwriter dDatabase that forms a ledger that is written to by a trusted and elected network of computers that have the capability of executing application code which is ultimately stored within the ledger itself and subsequently used to update the ledger (the ARISEN state). This ledger is referred to as a "blockchain" in scientific terminology and is used to synchronize and store the system's state changes. A cryptocurrency known as RISE (RIX) is used to meter and charge for the cost of storing computed data on the ledger, to insure that ARISEN's global computer resources cannot be abused by bad actors.

#### Turing Completeness
The term "Turing Completeness" refers to the father of computer science, Alan Turing. When working with ARISEN, you will be lucky if you don't stumble across the terms "Turing Completeness" or "Turing Complete." In 1936, Turing created a mathematical model of a state machine that manipulated symbols by reading and writing them on sequential memory (meant to resemble an infinite-length paper tape). With this construct, Turing would then release a mathematical foundation to answer questions about universal computability; meaning, all problems are solvable. Turing ultimately proved that there are classes of problems that are incomputable.

In this model, Turing specifically provided the "halting problem," whether it was possible, given an arbitrary program and its inputs, to determine whether the program will ever stop running, is not solvable. Turing further defined a system to be "Turing Complete" if it can be used to simulate any Turing Machine. Such a system is called a "Universal Turing Machine" (UTM). ARISEN's ability to execute a stored program (smart contract) in a state machine (ARISEN's global computer), while reading and writing data to memory (ledger), makes it a Turing Complete system and therefore a UTM. ARISEN can compute algorithms that can be computed by an Turing Machine, given the limitations of finite memory.

Turing proved that you cannot predict whether a program will terminate by simulating it on a computer. In other words, we cannot predict what the true outcome of a program will be without running it. Turing Complete systems can run in infinite loops or in simpler terms, without a termination point. They say it's trivial to develop a program that runs in a loop that never ends (which I disagree with), but intended never-ending loops can arise without warning, due to the complex interactions between the starting conditions and the code. ARISEN cannot predict if a smart contract will terminate or how long it will run without actually running it. This means that smart contracts can be designed to purposely run forever, after a node attempts to validate them, so that they consume all of ARISEN's available network resources.

To attack this issue, `Singleton Computers` (such as Ethereum) introduced the `gas` metering mechanism, whereas the ARISEN network uses the `NET`, `CPU` and `RAM` metering mechanisms, discussed further in [Distributing Computing Resources](#distributed-computing-resources). ARISEN can account for every instruction (computation, data access). In order to execute contract-based transactions on the network, a user is required to have `NET`, `CPU` and `RAM` resources, so that infinitely-looping contracts can eventually be halted. `NET`, `CPU` and `RAM` can be acquired through the `staking` of RIX on the network.

#### Smart Contract Engine
When it is said that ARISEN is also a smart contract engine, it must be pointed out that much of ARISEN, like any other computer - distributed or not - gains its functionality from an underlying set of programs that are stored within it. I like to say that Bitcoin is also a distributed computer like ARISEN, but Bitcoin is limited to a single program that centers around the minting and validated transacting of Bitcoins. The entire state of the Bitcoin computer contains a current historical record of every transaction that has ever occurred from the moment it was started. ARISEN is capable of running an infinite number of programs (smart contracts) which can be compiled and published into ARISEN's memory (ledger). All of the functionality of Bitcoin (minting and validated transacting) of Bitcoins, is combined into a single program on ARISEN known as `arisen.token`, the only difference being that `arisen.token` allows the minting and issuance of an infinite number of currency types, while Bitcoin is limited to a single coin.

Aside from the minting and validation of various token-types, it is important to note that all of ARISEN's core system features store all smart contracts that were used to boot up ARISEN and maintain its complex methods of constant operation. In the sub-sections that follow, I will explain the purpose of each of these core programs.

##### `arisen.bios`
-The `arisen.bios` contract is a minimalistic system contract that simply supplies the actions that are absolutely critical in the bootstrapping of an arisen-based blockchain.
-Introduces the data structure of ARISEN's [Block Header](#block-header), weighted permissions, weighted keys, blockchain authorities and The ABI (application binary interface) hash structure.
-Enables the following actions:
--Account creation using [Human-Readable Names](#human-readable-names).
--Update permissions and associated keys for a specific account.
--Delete account permissions.
--Assign specific actions from a specified contract to a specific account permission.
--Publish a smart contract on the ledger.
--Sets the ABI for a contract, identified by account name.
--Cancel a deferred transaction.
--Sent error notification when an error occurs while executing a deferred transaction.
--Set privileged status for an account.
--Set resource limits of an account (RAM, NET and CPU).
--Set a new list of active block producers, by proposing a schedule change.
--Set blockchain's parameters.
--Check if an account has authorization to access a current action.
--Activate a protocol feature.
--Assert whether a protocol feature was activated.

##### `arisen.system`
-This account defines the structure and actions needed for an ARISEN-based blockchain's core functionality. Some of the BIOS functionality can be found here, since the system contract largely takes over for the BIOS contract, after the chain has been bootstrapped.
-Introduces the constants for blocks per day, minimum amount of RAM to activate block production, annual rate of inflation (for native currency), the inflation pay factor, producer pay (percentage of inflation) and the time delay for refunds.
-Introduces the data structure for name bids, bid refunds, bids, global blockchain parameters, product details, voter info, delegated bandwidth and refund request.
-Enables the following actions:
--Init actions for initializing the system contract for a specific version and a symbol.
--`On Block` action for paying producers and calculating the missed block of other producers.
--Set RAM limits for an account.
--Set NET limits for an account.
--Set CPU limits for an account.
--Activate a protocol feature.
--Delegate bandwidth (NET) or CPU.
--Undelegate bandwidth.
--Buy RAM.
--Sell RAM.
--Refund pending, unstaked RIX after delegation period.
--Register block producer.
--Unregister block producer.
--Set RAM supply.
--Set RAM rate.
--Vote for a block producer.
--Register voting proxy.
--Set blockchain parameters.
--Claim block production rewards.
--Set privilege status for an account.
--Create name bid (auction).
--Bid name refund.

##### `arisen.token`
-The `arisen.token` contract defines the structure and actions that allow users to create, issue and manage cryptocurrencies on ARISEN-based blockchains.
-Introduces the data structure for account balances and currency statistics.
-Enables the following actions:
--Create a cryptocurrency.
--Issue a specific amount of coins.
--Transfer a cryptocurrency from one account to another.
--Get supply of a cryptocurrency.
--Get the balance of an account (for a specified cryptocurrency).

##### `arisen.wrap`
-Allows elected block producers to change an account (and domain's) keys, modify a contract, an account's owner and reverse any transaction, through the execution of an action that bypasses regular authorization checks. This can only be successfully executed by a majority (15/21) of the 21 elected block producers.

#### The Blockchain
It is the programs, aka smart contracts, that when executed, output data which is packaged into transactions, subsequently packaged into blocks and ultimately validated by nodes that are controlled by the network's elected block producers. This section will attempt to explain ARISEN's built-in blockchain from block headers and the block production, to the consensus algorithms used to validate blocks, while still meeting ARISEN's rigorous performance requirements.

##### An Example Smart Contract

```c++
#include <arisen/arisen.hpp>

using namespace arisen;

class [[arisen::contract]] hello : public contract {
  public:
    using contract::contract;

    [[arisen::action]]
    void hi(name user) {
      require_auth(user);
      print("Hello, ", name{user});
    }
};
```

The execution of the above `hi` action is quite simple to understand. One would simply provide the `hi` action with a `user` of the `name` type (an ARISEN account). As long as the `user` can sign the action with a key that matches one of the public keys associated with their stored account on ARISEN, this action will output "hi, name" with the name of the executing user, will generate a transaction, and will eventually be validated within a block. As discussed in the previous section, ARISEN's core functionality is handled most by the `arisen.bios` and `arisen.system` contracts. As the computer runs, regardless of whether users are interacting with it or not, blocks are being produced by ARISEN's 21 block producers, every half second, in an attempt to capture the blockchain's state on a per half second basis.

What is not shown in the smart contract is how data is saved to ARISEN using contract-supplied data structure(s). Contracts can supply specific data structures and can initiate the creation of a database on ARISEN where the data that derives from the execution of an action is ultimately stored. For example, when executing the `newacct` action within the `arisen.system` contract, a valid account name and two public keys are submitted, and if accepted as valid, are stored in the `accounts` table. Other actions within other contracts can now lookup users via the `accounts` table and validate the digital signature accompanying the execution of actions, to see if it derived from the user executing the action.

Simply put, everything that happens on ARISEN, whether it's the creation of an account, a vote for a block producer, or even the uploading/activation of a new smart contract, these actions are handled by various smart contracts that are stored within ARISEN's low-level block-based database structure according to their own data structure and logical database names. As confusing as that may sound, a smart contract does indeed enable the uploading and activation of other smart contracts that can be created by anyone on the network to extend the capabilities of the ARISEN computer itself. Just like an account or a vote for a block producer, newly updated contracts have their own data structure and are stored in their own database as well.

Before any action or its outputted data can be stored in their specified formats, the data must be validated as having derived from the specified user, and is then packaged in a transaction and transmitted by the executing user to the network. A block producer then confirms this data, places it in a block and then cryptographically verifies the legitimacy of the block. Once a block has been validated, it is then stored in the blockchain. For ARISEN, the blockchain represents a gigantic database of transactions but if you dive a little deeper, each of these transactions represents a data entry that derived from a contract formatted in a specific data structure, each of which are intended to be organized in a sub-database with other entry types. This creates what amounts to a massive distributed database management system, with an infinite number of possible data collections, all of which derive their data from the executions of programs which are stored in that very same system.

Actions, in most instances, depend on data that derived from a past execution of the same action or a different action. Take the `hi` action for example. The `hi` action requires the `user` parameter, which is an account. Accounts only exist on ARISEN, because they can be created via the `enact` actions in the `arisen.system` contract. When created, they use the `accounts` structure and are stored in the `accounts` database. The `hi` action uses another action from the `arisen.system` contract called `require_auth()`, which performs a lookup on the `accounts` table and authenticates a given user. All rocket science aside, one could make the argument that ARISEN is a dumb robot and only knows what a given action (or actions) allows it to know. Every single action builds upon the data that derives from other actions. As developers add programs to ARISEN, there are more actions and more data to manipulate. Like a [dDatabse](#ddatabase)), all of this data is immutable, unless of course a block producer majority chooses to remove specific portions of it.

##### The Block Header
The blockchain is a synchronized collection of blocks, each of which consist of a collection of transactions, with each transaction consisting of a collection of actions, from the period of time a block was produced. Each block has the following header:

| Field No. | Type | Description |
| --- | --- | --- |
| timestamp | uint32_t | Block timestamp of when block was produced |
| producer | name | Name of producer who validated block |
| confirmed | uint16_t | The amount of block confirmations |
| previous | checksum256 | Link to the previous block |
| transaction_mroot | checksum 256 | Link to the transaction's Merkle root |
| action_mroot | checksum 256 | Link to the action's Merkle root |
| scheduled_version | uint32_t | A schedule version |
| new_producers | producer_schedule | Producer's schedule |

Like any other blockchain, the block header allows one block to to be linked to another, whereas within the block, a Merkle Tree is created of all transactions, another is created of all actions, and the block pointer (block hash) is a hash of the block header. Like with a dDatabase, the Merkle Tree hashes for both actions and transactions within an ARISEN-based block efficiently summarize and verify the integrity of the transactions and actions that derive them. The `new_producers` field sets the elected producers and their schedule for the next block. Since ARISEN's election is conducted on a per-minute basis, and blocks are produced every 0.5 second, the producer roll and the producer schedule can change often. Either way, the ARISEN software, and therefore the producers themselves, follow the producer schedule on the previous block for the current block. In any blockchain, synchronization and scheduling is key, which brings me to consensus.

##### Consensus
ARISEN is based around the Delegated Proof of Stake (DPOS) algorithm, which dictates who is allowed to validate blocks on the network and who receives the delegated authority to make governance-based decisions on behalf of the network. DPOS, when compared to other consensus algorithms, is by far the best choice when it comes to meeting the performance requirements of a completely decentralized web.

A few facts about ARISEN'S rendition of DPOS algorithm:
-Those who hold RIX coins may elect "block producers" (governance members) through a "continuous approval voting system."
-Any member (account) on ARISEN can become a "block producer" candidate and can gain the vested power of a governance member, as long as they're in the top 21 candidates.
-ARISEN's blockchain produces blocks every 0.5 second and only one block producer has the authority to produce a single block.
-If a particular block is not produced at a scheduled time, then the block for that time slot is skipped.
-When one or more blocks are skipped, there is a 0.5 second or more gap in the blockchain.
-Blocks on ARISEN are produced in rounds of 126 (6 blocks each, multiplied by 21 block producers).
-21 block producers are chosen through the votes casted by members of the network at the start of each round.
-The 21 selected block producers are then scheduled to verify blocks within that specific round, in an order agreed to by 15 of the 21 block producers.
-If a particular block producer "misses" a block, or has not produced a block within the previous 24 hour period, they are removed from the governance and must inform the blockchain that they're ready to be a candidate once again. The performance and reliability of the blockchain depends on the perfected network operations, where only provably reliable blocker producers are in position to keep the network running properly.
-Block producers do not compete for blocks, rather they work collectively to validate a round of blocks. Typically, this keeps the blockchain from experiencing forks.
-If the blockchain does experience a fork, the algorithm automatically switches to the "longest chain." In other words, the longest chain relates to the chain with the most blocks, which means it has produced the most blocks, which also means it has a higher percentage of block producers who prefer that particular chain and have ultimately reached consensus around it. This works because a fork of ARISEN with more block producers will be longer than other forks and will grow in length much faster, due to it missing fewer blocks.
-Block producers caught producing blocks on two forks at the same time will likely be voted out, which can be backed by cryptographic evidence as well.

###### Byzantine Fault Tolerance (BFT)
BFT stands for Byzantine Fault Tolerance, which prevents a block producer from signing two blocks with the same timestamp or the same block height. With BFT, once 15 block producers have signed a block, that block is deemed irreversible. Any block producer under this model could in fact produce two blocks with the same timestamp or block height, but will create cryptographic evidence of their "treason." By using B FT, an irreversible consensus is typically reached within 1 second.

###### Transaction Confirmation
DPOS blockchains like ARISEN, typically have 100% block producer participation. Within a quarter of a second, transactions are considered confirmed 99.99% of the time. All EOS.IO-based blockchains like ARISEN, have the added luxury of utilizing an aBFT (asynchronous Byzantine Fault Tolerance) for faster achievement of irreversibility. The aBFT algorithm provides 100% irreversibility within a single second.

###### Transaction as Proof of Stake (TaPOS)
ARISEN requires every transaction to include part of the hash of a recent block header. This hash servers two purposes:
-1. Prevents a replay of a transaction on forks that do not include the reference block.
-2. Signals the network that a particular user and their stake are on a specific fork.

Over time, all users end up directly confirming the blockchain which makes it difficult to forge counterfeit chains, as the counterfeit chains would not be able to migrate transactions from the legitimate chain.

#### Accounts, Authentication and User Permissions
In previous sections, I explained how ARISEN's account and user management systems were formed via the actions and data structures within the `arisen.system` contract. Without accounts, no other action within ARISEN contracts, including those within the system contract, would function since they all require account-based authentication. With that in mind, accounts and ARISEN's authentication protocols are single-handedly the most important feature because without them, nothing you're reading in this paper would be possible.

A few facts about accounts:
-Accounts on ARISEN, unlike many other blockchains, are unique "human-readable names" that must be 12 characters in length.
-Users choose their username when creating an account.
-A new account can only be created by an existing account, due to the fact that any action-generated data stored on ARISEN requires a certain amount of RAM to be staked by the user initiating the action. RAM is like disk storage on ARISEN and is needed to write to the blockchain's memory database for any action that results in stored data in memory, including the create account (newacct) action.
-This gives way to account creation services that charge a service fee, or ones that simply sponsor account creation, like [PeepsID](https://signup.peepsid.com).

When it comes to decentralized applications, application developers will pay a small cost for account creation to signup a new users but the cost is nominal when compared to the costs needed to acquire new users via advertising or free services, etc. The greatest feature in relation to this model is that once a user has created an an arisen-integrated dApp, that same account can be used to log in to other arisen-integrated dApps. This means that only one app covers the cost and therefore application developers don't have to pay for the creation of every account of their users.

Names less than 12 characters are considered premium names and must be won at auction.

A few facts about premium names:
-Can be any number of characters (a-z, A-Z, 0-5).
-Must be won at auction. Any user can start an auction for a name and must be the highest bidder after 72 hours.
-Once won, other accounts can be created as sub accounts of the premium name, which must include a period (.). For example: `jared.rice`, in this case `rice` is the premium name.

##### Public Key Cryptography & ECDSA
While cryptography has been around for ages and public key cryptography since the 1960s, its use in blockchains has revolutionized how users are able to authenticate with blockchain-integrated web apps. The blockchain acts as a `Public Key Authority`, where any account's public key can be openly found on the blockchain itself, by anyone in the general public. This way, a user can `sign` anything (e.g., an action transaction) and the blockchain (or anyone for that matter) using `Elliptical Curve Digital Signature Algorithm` (ECDSA) can decipher that the owner of the private key, related to to the public key, was the creator of the digital signature.

ARISEN accounts, when created, require two permission levels, both of which have unique public keys associated with them. These permission levels are known as `owner` and `active`. Actions within ARISEN contracts each require the executing user to authenticate themselves using a specific permission level; in other words, the action must be signed with the private key that was used to derive the public key associated with the permission level selected for action-based authentication.

A few facts about arisen keypairs:
-Public keys start with the `RSN` prefix.
-Private keys are seeded with 128 random bits.
-Private keys start with the number `5`.

##### Role-Based Permissions Management
ARISEN uses a "role-based permissions management" system to determine if account-based actions are authorized. ARISEN's software provides a "declarative permissions management system" that gives accounts high-level control over who can do what and when. It is critical that authentication and permission management be standardized and separated from the business logic of an application, which enables the development of tools that manage permissions, generally, while also giving way to a significant boost in performance. An account can be controlled by multiple other accounts at once. Each one of these "controlling accounts" can have different weighted permissions, so that accounts have "multi-user control."

Every account can be controlled by any weighted combination of other accounts and private keys. This ultimately forms a hierarchical authority structure that reflects how permissions are organized and makes multi-user control easier than ever. Multi-user control is the single biggest contributor to security and when used properly, can greatly reduce the risk of theft due to hacking. ARISEN allows accounts to decide which keys and other accounts are allowed to send a particular action type to another account. For example, it's possible to have one key for a user's social media account and another for access to a ride sharing account. You could also give one account the ability to act on behalf of another account, without assigning a single key.

##### Vanity Permission Levels
ARISEN accounts are capable of creating "vanity permissions levels" whose origins are that of higher-level vanity permissions. Each vanity permission, or VPL, must define an "authority." Authorities of ARISEN are essentially "multi-signature threshold checks" that are comprised of keys and/or VPLs of other ARISEN accounts. For example, as ARISEN account could have a VPL known as "friend" that could be correlated to a specific "Action" on the account, and could be mutually controlled by any of the account's friends. The Steem blockchain (home of the Steemit social network) has three hard-coded VPLs known as `owner`, `active` and `posting, where the `active` VPL can do anything but change the `owner` permission's key, which is the most powerful authority on the account and the `posting` permission can only perform social actions like voting and posting. ARISEN allows accounts to create their own custom-named (vanity) permission levels, by allowing account holders to define their own hierarchy of permissions, as well as a customized group of actions.

##### Permission Mapping
With ARISEN, each account is able to outline a specific "mapping" between a contract/action or contract of any other ARISEN account and their own VPL. A user, using a social media app, could "Map" his/her social media app to the "friend" VPL. This would allow his/her friends to post on behalf of this user to his/her social media page. While the friend's post still appears as the account holder, they would still be identified by their keys that were used to sign the "Action," therefore you could easily identify users who are abusing their permissions.

##### Evaluating Permission Levels
If a VPL is used, which doesn't exist, the "active" DPL is automatically used to authorize an "Action." Once a mapping is determined, the signing authority is validated using the threshold multi-signature process as well as the authority associated with the VPL. If this is unsuccessful, it traverses up to the `active` DPL and ultimately the `owner` permission.

##### Parallel Evaluation of Permissions
The process behind the permission evaluation process is immutable, where transactions based around the change of permissions are not completed until the end of a block, so that all key/permission evaluations for every transaction can be completed in parallel. The quick validation of permissions is possible without costly contract executions that would ultimately be rolled back, and transaction permissions can be evaluated as pending transactions are received so they don't need to be re-evaluated when they're confirmed. Parallelizing the permissions process, while making it "read-only," dramatically increased performance on the network, because of the significant percentage of computation that is required to validate permission-based actions. To add to this, when "replaying" the blockchain from a log of actions, the permissions do not need to be reevaluated. This also significantly lowers the computational load brought by the replaying of a constantly growing blockchain.

##### Stolen Key Recovery
ARISEN allows users to restore control of their account when their keys are stolen. An account owner can use the stolen `owner` key along with approval of a designated `account recovery partner`. It is important to note, that the account recovery pattern cannot reset control of the account without the help of the user that possesses the `owner` key for the stolen account. Even if a hacker went through this process, it would be pointless due to the fact they they already control the account; although, even if the hacker did go through this process, the `account recovery partner` would probably demand some sort of multi-factor authentication; that is, if the account recovery partner is a dApp itself. A recovery partner has no authority over day-to-day transactions; rather, the recovery partner is only a "party" to the recovery process. This will ultimately reduce legal liabilities, costs, and lost currency that would otherwise be unrecoverable on blockchains that lack this feature.

#### Universal Authentication Layer & Transport
While public key cryptography is by far one of the most secure forms of authentication in the world, the process of securely managing one's private keys can be cumbersome and can introduce many security risks. As was mentioned in [Role-Based Permission Management](##role-based-permission-management), ARISEN gives way to the development of tools for the management of accounts and permissions, and if developed correctly, can greatly simplify the management of private keys and greatly reduce, if not eliminate, the security threats surrounding the management of private keys. Although, this is only half the problem.

dWeb-based apps that are integrated with ARISEN would require their user to copy and paste a 64-character private key when logging in, and each and every time they execute a specific action within an application (e.g., the `post` action within a social networking app). Just so one can build a mental model surrounding how this process works on the dWeb, try to follow the following description:

-1. A developer writes a simple application that allows someone to post a message to the page. The app itself is written in JavaScript and HTML and utilizes ARISEN's [JavaScript Library](https://github.com/arisenio/arisenjsv1) for executing contract-based actions via ARISEN's API.

-2. The developer writes a simple smart contract called "postit" with a `post` action, that requires an ARISEN account to authenticate using their `active` permission when submitting the action's `message` parameters. The developer deploys the "postit" contract to ARISEN and publishes his app files in a dDrive and announces on the dWeb.

-3. There is a "Post" button in the application, that when clicked brings up a popup that asks for the account username, the `active` private key and the message being posted. When the "send" button is pushed, the app then packages the `post` action with the entered message, along with a digital signature created with the entered private key, and transmits this action to ARISEN so that the `post` action can be executed.

-4. Once executed and validated, it is stored on ARISEN, where the app can now retrieve this data and display on the page.

This is a horrible user experience, even though the user was using a secure account/permission management software, the eventual exposing of the private key in the browser eliminated all of the security advantages provided by the management of software in the first place. This problem was solved by authentication apps like MetaMask and Scatter, which apps can use to communicate with the Ethereum and EOS blockchains. Using MetaMask and Scatter, the process goes like this:

-1. Instead of a "Post" button, there is a "Login" button. Bob clicks the "Login" button and the MetaMask app comes up, where Bob is asked to choose an account (and permission level) and enter a password.

-2. Bob is brought back to the app where he now see a "Post" button.

-3. Bob clicks "Post" and a popup appears with only a "Message" box.

-4. Bob enters the message and clicks "Send."

-5. The app then packages the `post` action with the entered message and the "logged-in" user via step 1 (note: there isn't an actual session, the app simply remembers which account/permission was chosen), along with the corresponding permission to MetaMask.

-6. The MetaMask app pops up, showing the packaged action and which user/permission is packaged with the action as the executor.

7. Once Bob clicks "Execute," Bob enters a password (for MetaMask) and MetaMask signs the action with the private key matching Bob's chosen account/permission level and the transaction is sent to ARISEN for confirmation and eventual inclusion into a future block.

**NOTE:** Authenticators like MetaMask use a password to encrypt the private keys it stores, so that when a user wishes to sign an action, or complete a transaction for that matter, the private key is decrypted with the entered password, used to sign the transaction and MetaMask subsequently removes the decrypted key from the temporary memory or off-line storage. The password is never saved.

This definitely made the process far more secure since Bob's private key was never revealed, but the user experience went from 4 very unsecure steps to 7 very secure steps and requires way too many back and forth interactions with the authenticator application (MetaMask). It was a major reason for the creation of ARISEN's Authentication Transport Protocol and what we refer to as the "Universal Authentication Layer."

##### Simplifying Authentication & Authenticators
As was seen in the previous section, there are many authenticator apps, each of which use their own protocol for transporting action data between apps and the authenticator itself. A simplified transport protocol would eliminate many steps in the authentication process, while also making the authentication process with decentralized applications even simpler than the authentication process with centralized web applications. The authenticator discussed in the previous section was for other blockchains, but hopefully helped expose the issues most decentralized applications face when it comes to user experience. The following sub-sections will discuss how various ARISEN protocols, tools and features have helped simplify the authentication process for dWeb-based applications.

###### Human-Readable Names
Unlike most blockchains that use 64-character (or more) hexadecimal addresses for "account addresses," ARISEN associates a username that is `human-readable` with a set of permissions, each of which is associated with a unique public key.

The use of `human-readable` usernames - rather than computer-generated hexadecimal or byte-formatted addresses - means that we're able to further simplify the authentication onboarding, and app-to-authenticator / authenticator-to-app request and response transport lifecycle, and the overall authentication transport process.

###### Universal Authentication Layer
The goal of ARISEN's `Universal Authentication Layer` is to bring to life a universal means for authenticating users, by standardizing the transport between dWeb-based applications and ARISEN-integrated authenticators via a universal bridge, which makes the authentication process between authenticators seem identical, regardless of how the underlying authenticator is designed. Put another way, it provides a UI layer for giving users a consistent UI/UX flow, independent of the authenticator they are using or the website they're on.

The Universal Authentication Layer brings to life support for Biometric Hardware Secured Keys (e.g., Apple Secure Enclave), and a way for insuring a user is completely aware of the app they're using, as well as the related smart contract they're executing an action within.

The Universal Authentication Layer involves the following components:
-[Chain Manifest Specification](#chain-manifest-specification)
-[Ricardian Specification](#ricardian-specification)
-[Authentication Transport Protocol](#authentication-transport-protocol)
-[Universal Authentication Library](#universal-authentication-library)

###### Chain Manifest Specification
An `off-chain manifest`, or manifests, is metadata about an application that is displayed in the root of a [dDrive](#ddrive) (e.g., dweb://<dweb-key>/chain_manifest.json). The location of these files, along with their checkout hash, is referenced in the smart contract that is utilized by the app in what is referred to as an `on-chain manifest`.

Both `off-chain manifest` files are contained in the `chain_manifests.json` and `app-metadata.json` files within the root of a dDrive. The `app-metadata.json` file contains the following fields:

-`spec_version` - The specification version of the `chain manifest`.
-`name` - The full name of the application. This will be user-facing in app listings, history, etc.
-`shortname` - A shorter name for the application.
-`scope` - An absolute path relative to the application's root. (`/` or `/app`, but not `../`).
- `apphome` - Tells the browser where your application should start when it is launched. This must fall within the `scope`.
-`icon` - An HTTPS url, DWEB url or absolute path relative to the application's root, followed by a SHA-256 checksum hash. May be displayed in app listings, history, favorites, etc. (.e.g, `dweb://dsocial.dcom/icon.png#SHA256HASH` or `/icon.png#SHA256HASH`, but not `../icon.png#SHA256HASH`).
-`appIdentifiers` (optional) - For native applications, an array of whitelisted app identifiers (e.g., bundled identifiers for iOS apps or package names for Android apps).
-`description` (optional) - A paragraph about your application. May be displayed in app listings, etc.
-`sslfingerprint` (optional) - Your app domain's SSL SHA-256 fingerprint as a hex string. If present, the user agent may check that the SSL fingerprint of the domain submitting the transaction matches that in the provided manifest.
-`chains` - An array containing the `chainID`, `chainName` and `icon` for any ARISEN chain for which your application's smart contract is located on and therefore, where your application plans to require signing. User agents (like an authenticator) will use this for presenting a friendly chain name and icon to the user when prompted for signing.

An `on-chain manifest` must then be registered using the [arisen.assert::add.manifest](https://github.com/ARISENIO/arisen.assert) action on every ARISEN chain where smart contracts for an app exist and on which an app will transact. Furthermore, an array of `chain manifests` must be declared in a publicly available JSON file at the root of the application's `declaredDomain`. See [Authentication Transport](#authentication-transport-protocol) for the `declaredDomain` parameter, which must match the domain referenced in the `off-chain manifest`.

The `chain-manifests.json` file has the following fields:
-`account` - The chain account name. This is the account that published this `on-chain manifest`.
-`domain` - The uri or bundle identifier.
-`appmeta` - The DWEB or HTTP address of the application's metadata JSON file and its hash. Must be an absolute path.
-`whitelist` - An array containing the `contract` and `action` for each allowed contract action(s) that your app can propose.
--The `contract` and/or `action` fields within the `whitelist` on the manifest may contain a wildcard. Wildcards are denoted by " " (empty string).

For a more detailed description of Chain Manifests, please read the full specification [here](https://github.com/ARISENIO/manifest-spec).

###### Ricardian Specification
The Ricardian Specification gives a set of human-readable requirements (that a smart contract should follow in order to be considered valid) and easily consumable metadata about the actions a contract contains (action title, description and a URL of an icon that describes an action; for example, a pencil icon for the `post` action described earlier in this section). Ricardian Metadata is written in HTML (in .html file(s) separate from the contract code itself) and combined with the contract code in a WASM compiled output format by ARISEN's smart contract compiler. Once deployed to an ARISEN blockchain, it can easily be viewed in an ABI (application binary interface) format (similar to JSON).

For a deeper drive into ARISEN's Ricardian Specification, please refer to the latest version of the spec [here](https://github.com/ARISENIO/ricardian-spec).

###### Authentication Transport Protocol
ARISEN Authentication Transport Protocol (AATP) provides a protocol that ARISEN-based authenticators, like [dWebID](#dwebid), can utilize to respond to requests from dWeb-based apps, with consistent handling logic in a request-response lifecycle. It aims to improve the user's overall experience and security when authenticating and signing actions that derive from dWeb-based websites and apps.

Below is an explanation of how an authenticator like [dWebID](#dwebid) uses AATP, Chain Manifests and Ricardian-compliant contracts in the process of authenticating a user.

###### `Request Transports`
A dWeb application can use either a dWebID Link, dWeb Link, Apple Universal Link or a Deep Link to invoke an authenticator application, including the transaction payload in the query string as a hex-encoded value, and the recipient public key, if the payload is encrypted.

-`dwebid://request?payload={hexPayload}&Key={publicKey}`
-`{customProtocol}://request?payload={hexPayload}&Key={publicKey}`
-`dweb://{siteUrl}/auth?payload={hexPayload}&Key={publicKey}`
-`https://{siteUrl}/auth?payload={hexPayload}&Key={publicKey}`

###### `Response Transports`
Authenticator applications will return a response to the request's `returnUrl` with the payload appended as a hex-encoded URL hash fragment identifier. If the payload is encrypted, the public key is provided at the end, preceded by `-`.

-`dweb://{siteUrl}/some-resource/resource-id#{hexPayload}-{publicKey}`
-`https://{siteUrl}/some-resource/resource-id#{hexPayload}-{publicKey}`
-`{customProtocol}://transaction-response#{hexPayload}-{publicKey}`

###### `Request Envelope`
The top-level properties of each request payload make up the `request envelope`. Envelopes may contain several keys:

-`version` (required) - The protocol version.

-`id` (required) - The unique UUIDv4 of the request, which insures that the requesting application is able to connect a response to the initial request.

-`declaredDomain` (required) - Integrating applications (both web and native) must self-report a `declaredDomain`. Authenticator applications should not blindly trust this URL.

-`returnURL` (required) - The URL to which the authenticator application will return the user after the request has been processed and the user had taken any necessary action.

-`responseKey` (optional) - An elliptic curve 65 byte public key. If provided, the response will be encrypted with this key using the agreed upon algorithm.

-`request` (required) - The request payload, consisting of one or more request types.

###### `Request/Response Types & Authenticator Behavior`
There are four different types of request, each of which have different payloads. Those requests, the behavior of an authenticator app and each response type are explained below:

-`Transport Authorization Response` - This request must include the preferred response transport (e.g., dWeb Link, Custom Protocol, etc.) as well as an array of contracts and their associated actions involved in the request.

This request type carries out two functions:
--1. It negotiates and establishes communication with an authenticator application over one or more transports.
--2. It requests user authorization for the transaction actions that may be requested through each transport. The idea here is that integrating applications or users may restrict which actions are authorized over less secure transports.

**This is an example Request/Response lifecycle for a `Transport Authorization Request`:**
-1. The request must include a prioritized list of app-supported response transports and a list of requested action permissions for each response transport, which must be white-listed in the `on-chain manifest`. The request may be sent with one or more `Selective Disclosure` or `Authentication` request types.
-2. Upon the authenticator app receiving the request, it will prompt a user for approval if this is the first time encountering this `Transport Authorization Request` for the given `chain manifest`. The authenticator may respond automatically if the user has previously approved an identical `Transport Authorization Request` for the given `chain manifest`.
-3. A `Transport Authorization Response` sent from the authenticator app back to an application must include a prioritized list of vault-supported request transports and vault-supported response transports, and MAY be sent through one of the app-supported response transports.

-`Authentication Request` - This request type allows an integrated dWeb-based application to request proof of a user's possession of one or more private keys corresponding to any public keys that they have disclosed. This enabled passwordless authentication flows so that integrating applications can display private data to the authenticated user.

-`Selective Disclosure Request` - Allows an app to request private user data (e.g., availableKeys, authorizers).

**This is an example Request/Response lifecycle for a `Selective Disclosure Request`:**
-1. The request must include one or more requested attributes (e.g., availableKeys).
-2. The authenticator app must prompt the user to approve any disclosures they have not previously approved for an identical `Selective Disclosure Request`. It may respond automatically if the user has previously approved an identical `Selective Disclosure Request` for the `chainmanifest` `scope`.

-`Transport Request` - Allows a dWeb-based application to request a user signature for a transaction. In this case, the authenticator application must:
-1. REJECT the transaction request automatically if the transaction contains any actions not whitelisted in the app's `chainmanifest` for a given ARISEN chain.
-2. REJECT the transaction request automatically if the transaction contains any actions that have not been allowed by a previous `Transport Authorization Request` for the given transport.
-3. PROMPT the user for permission to sign if the transaction contains any actions that have not been allowed by a previous Action Permission Request, without autosign privileges. This prompt may be ignored and a `Transaction Request` may be approved automatically if the transaction contains only actions that have been allowed by a previous `Action Permission Request` with autosign privileges.

##### The ARISEN Authentication Process
Below is a description of the simplified authentication process for dWeb-based apps that utilize an AATP-ready authenticator:

-1. Bob opens the post app again at `dweb://postit.dcom` in his mobile Safari browser.
-2. Bob downloads dWebID, an AATP-ready authenticator for iOS.
-3. The PostIt app asks Bob to create an account. He chooses a username and his `owner` and `active` keys are automatically imported into [dWebID](#dwebid) using its builtin import API, and both keys are securely stored on the device's `Secure Enclave` chip.
-4. Bob sees a "Login" button and clicks it. Upon clicking the login button, a `Transport Request`, along with a `Selective Disclosure Request` is sent to the dWebID mobile application using the `dwebid://` protocol.
-5. dWebID opens and performs the following checks immediately in the background:
--assert that the `referrerUrl` and `reuturnUrl` are all paths of the `declaredDomain`.
--fetches the `chain-manifests.json` file from the root of the `declaredDomain`.
--asserts that the values for `domain` declared therein all match one another and the `declaredDomain`.
--fetches the `app-metadata.json` from the `appMeta` URL in the `chain-manifests.json` file and asserts that the file's hash matches the hash declared in both manifests.
--asserts that the file's hash matches the hash declared in the `on-chain manifest`.
-6.  If all checks pass, dWebID shows a screen that takes on the following format:

```
Allow <appnames> to log in
using dWebID?

<app-icon>

Domain Requesting:
<app-url> (request URL)

<Deny Button> <Allow Button>
```
The above data is retrieved from the `chain-manifests.json` and `app-metadata.json` chain manifest files in the root of the app's dDrive.

-7. dWebID asks Bob which account he would like to disclose with PostIt. Bob selects the @bob account and dWebID sends a `Selective Disclosure Request` with the public key for @bob back to PostIt.
-8. Bob is returned back to the PostIt app in Safari, where he now sees a "Post" button.
-9. Bob clicks "Post," which brings up a popup where he can type a message.
-10. Upon clicking "send," the `post` action related to PostIt app's smart contract on ARISEN is packaged in a `Transaction Request` and sent to dWebID.
-11. The dWebID is opened automatically and a `Transaction Approval Request` screen is shown that takes on the following format:
```
<app icon> <app domain>
                 <app url>
--
<action item> <action name>
<action description>
<action contract>

Signing as <username> on <chain name>
--
<Cancel Button> <Approve Button>
```
The above app icon, app domain and app url derive from the website's `chain-manifests.json` and the `app-manifests.json` file with the app's dDrive, while the action icon, action name, action description and action contract clause derive from the Ricardian metadata stored with the app's smart contract on a specified ARISEN chain.

-12. Bob clicks "Allow" and is prompted for biometric confirmation (face or fingerprint).
-13. Once Bob is biometrically authenticated, the transaction is signed with the keys from the Secure Enclave.
-14. The transaction and signatures will be returned to the requesting app, which may, in turn, broadcast the transaction to the chain (NOTE: dWebID broadcasts by default).
-15. Bob is returned to the PostIt app, which can now retrieve Bob's post remotely from the `postit` database on ARISEN and displays the posted message to Bob.

In the future, any time Bob accesses the PostIt app and initiates the `post` action, Bob is only asked to verify himself biometrically. Bob's entire experience with PostIt is now passwordless and extremely secure.

Keep in mind, Bob probably has no idea he's using a blockchain, nor does he ever see his cryptographic keys. All he ever sees are user-friendly, human-readable prompts and is only prompted twice. Once when he first logs into the app and second, the first time he initiates an action. He is only prompted again when an action is initiated and a `Transaction Request` is sent and he needs to verify his biometrics. ARISEN's Universal Authentication Layer allows dWeb-based apps to simplify their authentication process with ARISEN and allows everyday users of the Internet to experience the power of decentralized apps for the first time.

##### dWebID
At PeepsLabs, we're developing the dWeb's first authenticator using AATP which can be used as a means for decentralized and universal authentication on the dWeb.

dWebID has the following features:
-Enables seamless, multi-network support. In fact, the app itself does not even communicate with chains or nodes directly.
-Securely stores private keys and employs the use of biometrics to sign transactions.
-Displays richly formatted Ricardian Contracts, which provide users with a human-readable explanation of the actions the app is proposing and allows users to approve or reject the terms of the contract(s).
-By following the `Manifest Specification`, it displays metadata about the requesting application to end users any time they are prompted to trust an app or sign a transaction from one. This provides users with an improved sense of trust in the requesting application and the signing ceremony itself. It also runs pre-flight security checks, comparing the contents of a transaction request with what integrating apps have declared about themselves.

#### Smart Contracts & RSN VM
In previous sections, I described out how Smart Contracts power the entire ARISEN computer, but it is the computer itself I have yet to explain. Blockchain-based programs must be compiled, validated and computed; and are entirely dependent upon single-threaded performance, fast compilation and validation of assembly code, as well as the low-overhead calls to native bytecode. ARISEN contracts are for the most part, written in C++, a low-level programming language also known as a systems programming language, whose most pervasive uses are deep in the infrastructure [STRO13]. The language's focus on static types and compile-time type checking help give way to the secure execution of programs (contracts) and help to eliminate the threat of stack overflow attacks, discussed subsequently. ARISEN ships with its own compilers, like [ARISEN CDT](https://github.com/arisenio/arisen.cdt), which compile C++-based contracts into the WebAssembly (WASM) assembly language, a programming language that is one step away from machine language (bytecode), allowing each instruction to be translated into one machine instruction by an assembler [STALL16].

RSN VM, a fork of EOS VM, is a high performance WASM engine with predictable compile times, where contracts do no have to be compiled every time the process restarts. Unlike more popular WASM engines, RSN VM is designed from the ground up for the rigorous demands of blockchain applications, which require far more from a WASM engine than those that were designed for web browsers or standards development.

When it comes to smart contracts, it's a blockchain's lifeline to insure that any non-deterministic behavior, unbounded computations or unbounded use of RAM be prevented, as it can bring down an entire blockchain in seconds. As discussed in [Turing Completeness](#turing-completeness) and [Distributed Computing Resources](#distributed-computing-resources), RSN VM uses a builtin metering mechanism to avoid some of these issues, but at even lower-levels, other issues remain.

The RSN VM has no physical existence. It is a distributed and virtual computation engine, and is not hugely dissimilar to the virtual machines of Microsoft's .NET framework, or interpreters of other bytecode-compiled programming languages such as Java. RSN VM, at a basic level, is in charge of both smart contract deployment and smart contract execution. At a high-level, it is essentially a global, decentralized computer, containing millions of executable objects, each of which utilizes its own data store. Furthermore, RSN VM, in one way or another, is a Turing-complete state machine because every single execution process is limited to a certain amount of computations and completely dependent on the CPU, NET and RAM that's available to the user who is initiating the execution.

RSN VM was designed with the goal of creating a highly deterministic and secure environment for the parallel execution of contracts. Given what derives from the execution of a contract must be deterministic - and the non-deterministic nature of denomals, NaNs and rounding modes as it relates to floating-point arithmetic, in addition to the underlying physical computer's ALU - RSN VM relies on the `softfloat` implementation of IEEE-754 float-point arithmetic, which is even further constrained to insure determinism. To add to that, secondary limits and constraints such as stack size and call depth can cause consensus failures if they differ from a previous backend, which is solved through RSN VM's user-definable constraints at either compiler-time or run-time. User-definable constraints are defined based on use-case and the data-type involved.

While deterministic execution is crucial, time-bounded execution is just as important when it comes to a blockchain-based computer, in order to insure that the deterministic execution of a contract doesn't "over run" the CPU time that is allotted for a given contract. Since blockchains are limited in resources, an `instruction counter`, at a very minimum, is needed to insure the time-bounded execution of a contract. RSN VM allows users to use a simple instruction counter for counter-based time-bounding in a single-threaded environment, but can be avoided by using a `watchdog timer` that doesn't introduce any performance overhead like the latter option.

Even more important is the secure execution of contracts. RSN VM was designed to avoid unbounded memory allocation, extremely long load times, and stack overflows deriving from a syntax analysis (like recursive descent parsing or execution), thanks to a type system that was prebuilt from the onset to insure RSN VM's foundational data types were invariant (exact type matching, where `T = T`). This insures developers don't have to worry about explicit type checks and validations since RSN VM's underlying type system insures that each data type maintains these invariants and kills the execution of a contract that violates this integrated invariance. Given the problems that can occur in C++ with unsafe arrays and pointer references, there have been a number of proposals to augment compilers to automatically insert range checks on such references [STALL18].

RSN VM has special purpose allocators that utilize the security of the physical CPU and the underlying operating system that RSN VM exists upon, to insure that a contract is not able to store data beyond the limits of a fixed-size buffer or attempt to overwrite adjacent memory locations that may hold other variables, parameters or control flow data from the contract itself, which could contain return addresses and pointers to previous stack frames. Buffer overflow attacks are one of the most dangerous and popular forms of security attacks and a guard paging mechanism, via the CoreOS, is used so that memory is properly sandboxed.

There is never a point where, during either the parsing or evaluation phases, RSN VM uses unbounded recursion or loops. RSN VM is constrained to limit or eliminate the ability for a bad or corrupt contract to cause a crash or infinitely hang the machine. It's custom allocators and memory management facilities allow for different access patterns and allocation requirements. RSN VM's allocators are used to back the core data types (fast vector, WASM stack, fast variant and WASM module) and, as such, do not "own" the memory that they use for operations. This gives way to maximizing the performance of the interpreter implementation.

These non-owning data structures allow for the ability to use the memory cleanly, while not having to concern the data type with destructing when going out of scope, which creates a performance increase for portions of RSN VM, without loss of generality for the developer. Since the data is held by these allocators and has lifetimes that match that of a WASM module, no copies of these heavy weight data types are ever needed. Once an element in an RSN VM is constructed, that is its home and final resting place for the lifetime of the WASM module.

##### Contract Storage & On-Chain Databases
As you may recall, a smart contract itself (`arisen.system`) and its `set` action are used to deploy other contracts to the network and subsequently store the WASM-derived bytecode in an on-chain database associated with the uploading user. The contract itself has its own database associated with it, where the data that derives from the execution of its actions is stored. Although, each account on ARISEN has its own private on-chain database associated with it, for which contracts can optionally choose to store action-derived data via a contract-specific scope.

On-chain databases associated with accounts can only be accessed by its own action handlers. Action handlers are scripts that send actions form one account to another. ARISEN is able to define smart contracts through the combination of action handlers and automated action handlers. Each account can ultimately send structured actions to other accounts and may define scripts to handle actions when they're received. To support parallel execution, each ARISEN account can also define any number of scopes within their database. The block producer will schedule transactions in such a way that there is no conflict over memory access to scope and therefore can be executed in parallel.

As an example, dWeb's [dDNS](#ddns) contract stores domain records within a scope named `ddnsrecords` within each domain's on-chain private database (considering domains in the context of ARISEN are actually accounts), so that records serve as a unique, searchable index for each domain.

##### Deterministic Parallel Execution of Appliances [BLOC18]
RSN VM has no scheduling capabilities, because execution ordering is handled externally by ARISEN clients. ARISEN has to execute transactions (transactions derive from the execution of an action), considering its consensus algorithm is dependent upon deterministic (reproducible) behavior, which means all parallel execution must remain without mutexes or locking primitives. When transactions are executed in parallel, it's important that these transactions don't create non-deterministic results, considering the absence of locks.

When ARISEN's mainnet begins producing in parallel, block producers will organize action delivery into individual `shards` in order to evaluate transactions in parallel. The scheduling of transactions will be deterministically executed according to the output of a block producer, although protocol for the generation of transaction scheduling will not be deterministic. Because of this, block producers can take advantage of parallel algorithms to schedule transactions.

With parallel execution, it's important to note that when a new action is generated by a "script" or "contract," it does not get delivered instantly; rather, it's scheduled to be delivered in the next cycle because the receiver may be actively modifying its own state within a completely different shard.

##### Mining Communication Latency [BLOC18]
Latency is the time it takes for one account to send an action to another account, and then receive a response. The goal is to enable two accounts to exchange actions back and forth within a single block, without having to wait 0.5 second between each action (ARISEN's block production turnaround time). To enable this, ARISEN divides each block into cycles. Each cycle is divided into shards and each shard contains a list of transactions. Each transaction contains a set of actions to be delivered.

Below is a pseudo-representation of a block's layers and how each layer is processed:
```
Block
    Region
        Cycles (sequential)
            Shards (parallel)
                Transactions (sequential)
                    Receiver and Notified Accounts (parallel)
```

Transactions generated in one cycle can be divided in any subsequent cycle or block. Block producers will keep adding cycles to a block until the maximum wall clock time has passed, or there are no new generated transactions to deliver.

It is possible to use static analysis of a block to verify that within a given cycle, no two shards contain transactions that modify the same account. So long as that invariant is maintained, a block can be processed by running all shards in parallel.

##### Read-Only Actions Handlers [BLOC18]
Some accounts may be able to process an action on a pass/fail basis without modifying their internal state. If this is the case, then these handlers can be executed in parallel, so long as the read-only action handlers for a particular ARISEN account are included in one or more shards within a particular cycle.

##### Atomic Transactions with Multiple Accounts [BLOC18]
Sometimes it is desirable that actions are delivered to and accepted by multiple accounts atomically. In this case, both actions are placed in one transaction and both accounts will be assigned the same shard and the actions applied sequentially.

##### Partial Evaluation of Blockchain State [BLOC18]
Scaling blockchain technology necessitates that components are modular. Everyone should not have to run everything, especially if they only need a subnet of the contracts. A social networking application developer runs full nodes for the purpose of displaying the entire state of its application to users. This social networking application has no need for the state associated with, for instance, a ride-sharing application's contract(s). ARISEN's software allows any full node to pick any subset of applications to run. Actions delivered to other applications are safely ignored if an application never depends upon the state related to another contract.

##### Subjective Best Effort Scheduling [BLOC18]
ARISEN cannot obligate or force a block producer to deliver any action to any other ARISEN account. Each block producer makes their own subjective measurement of the computational complexity and time required to process a transaction. This applies whether a transaction is generated by a user or automatically by a smart contract.

At the network level, all transactions are billed a computational bandwidth cost based on the number of WASM instructions executed. However, each block producer may calculate resource usage using their own algorithm and measurements by adjusting RSN VM's compile-time and run-time constraints. When a block producer concludes that a transaction or account has consumed a disproportionate amount of computation capacity, they simply reject the transaction when producing their own block; however, they will still process the transaction if other block producers consider it valid.

In general, so long as even one block producer considers a transaction as valid and under the resource usage limits, then all other block producers will also accept it, but it may take up to one minute for the transaction to find that producer. In some cases, a producer may create a block that includes transactions that are an order of magnitude outside of acceptable ranges. In this case, the next block producer may opt to reject the block and the tie will be broken by the third producer. This is no different than what would happen if a large block caused network propagation delays. The community would notice a pattern of abuse and eventually remove votes from the rogue producer.

This subjective evaluation of computation cost frees the blockchain from having to precisely and deterministically measure how long something takes to run. With this design, there is no need to count instructions at the VM-level, which drastically increases opportunities for optimization without breaking consensus.

##### Deferred Transactions [BLOC18]
ARISEN supports deferred transactions that are scheduled to execute in the future. This enables computation to move to different shards and/or the creation of long running processes that continuously schedule a continuance transaction.

##### Context Free Actions [BLOC18]
A Context Free Action involves computations that depend only on transaction data, but not upon the blockchain state. Signature Verification, for instance, is a computation that requires only the transaction date and a signature to determine the public key that singed the transaction. This is one of the most expensive individual computations a blockchain must perform, but because this computation is context free, it can be performed in parallel.

Context Free Actions are like other user actions, except they lack access to the blockchain state to perform validation. Not only does this enable ARISEN to process all Context Free Actions, such as signature verification, in parallel, but more importantly, this enables generated signature verification.

With support for Context Free Actions, scalability techniques such as Sharing, Raiden, Plasma, State Channels and others become much more parallelizable and practical. This development enables efficient inter-blockchain communication, as well as unlimited scalability for on-chain activity.

##### Schema-Derived Actions [BLOC18]
All actions sent between ARISEN accounts are defined by a schema that is part of the blockchain consensus state. This schema allows seamless conversion between binary and JSON representation of the actions held within a smart contract.

##### Schema-Derived On-Chain Database [BLOC18]
Database state is also defined using a similar schema. This insures that all data stored by all contracts/accounts, is in a format that can be interpreted as human-readable JSON, but stored and manipulated with the efficiency of binary.

##### Separating Authentication From Application [BLOC18]
ARISEN segregates validation logic into three separate segments to maximize parallelization opportunities and minimize the computation debt associated with reengineering application state from the transaction log:
-Validating that an action is internally consistent;
-Validating that all preconditions are valid; and
-Modifying the application state.

Validating the internal consistency of an action is read-only and requires no access to blockchain state. This means that it can be performed with maximum parallelism. Validating preconditions within a smart contract, such as required balance, is read-only and will certainly benefit from parallelism. Write access is only required when the modification of an application's state is taking place and therefore would be processed sequentially for each application in question.

Authentication is simply a read-only process, executed from within a smart contract in order to verify that a specific action can ultimately be applied. Under this model, websites and web applications are doing the work, not the contracts, or ARISEN for that matter. In real-time, the calculation of computation debt and the regeneration of application state from the transaction log are both required to be performed; although, once a transaction has been included in the blockchain, it is no longer necessary to perform the authentication operation(s).

##### VM & Language Agnostic
From a computer science perspective, ARISEN is responsible for coordinating the delivery of authenticated messages (called "actions") between ARISEN accounts. Truly, any programming language or virtual machine can be implemented to work with ARISEN's software, as these implementation specific details, for the most part, are independent from ARISEN's core design philosophy. Although, languages and virtual machines must be deterministic and properly sandboxed with sufficient performance if they are to be integrated with ARISEN's APIs.

#### Governance
ARISEN is ultimately designed to be a decentralized democracy, where the people are able to reach consensus concerning issues that involve the community at-large, as well as the safety and welfare of the community itself. The users of the dWeb are governed through a "governance" process concerning subjective matters that require collaborative action, the power to proceed forth with the decisions that are agreed upon by the governance themselves, and the vested power to create amendments to a network-wide Constitution that was ratified in order to protect the rights of the network's users.

Elected block producers that are instituted through the DPOS algorithm are also known and considered as elected Governance members. Essentially, Governance members must approve all changes to dWeb-based software that has been requested by the community itself. This model works because if Governance members go against the wishes of RIX holders, users of the network will most likely remove their votes from those specific Governance members who ultimately went against their wishes. As a safety value, non-producing full node validators (e.g., dApps that run their own nodes) have the power to reject changes that are made without the permission of RIX holders.

##### Elections
ARISEN's election system is very unique and coincides with ARISEN's DPOS algorithm.

**A few facts about ARISEN's election system:**
-Each ARISEN account can vote for up to 30 different block producers.
-The number of votes an account has directly correlates to the number of staked RIX in the account.
-A staked vote remains valid for an entire year.
-After one year, a staked vote decays to half a vote and so on.
-This is to maintain a need to validate the performance of block producers.
-If the RIX used for voting are unstaked, those votes are removed from the continual counting process since the RIX becomes transferrable liquidity.
-The counting process happens every 126 seconds so, in theory, block producers can be regularly interchanged with new candidates.
-Candidates who didn't quite make the block producer list act as backup block producers.
-Voting can be carried out via wallets like [dWallet](https://peepsx.com/dwallet).
-Voting relies on users creating a digital signature via a wallet or voting application that broadcasts their vote/signature to the ARISEN mainnet, providing cryptographic evidence of the authenticity of the vote, as well as the RIX ownership, as it is an interaction/transaction in the user's ARISEN account.

##### Freezing Accounts
The freezing of accounts on the EOS network was a heated debate amongst blockchain enthusiasts. In truth, every single blockchain is designed to allow block producers to pick and choose if specific transactions are included within a specific block. Therefore, block producers or miners on any blockchain have the ability to freeze accounts and reverse transactions simply through consensus. ARISEN formalizes this authority by making sure the process of freezing accounts has to be approved by a 15/21 vote amongst active governance members. ARISEN's blockchain expands upon EOS's vision, enabling people of the network to police for many illegal activities, that of which are within the [dWeb Constitution](https://github.com/arisenio/constitution).

You can read more about how the [dWeb Protocol](#DWEB) and its [Reporting System](#reporting-system) utilize this feature later in this paper.

##### Updating Contract Code
Contracts on ARISEN that act maliciously can shutdown the network entirely. ARISEN allows its governance to replace malicious code within contracts through a 15/21 vote of elected members. This enables ARISEN to proceed forth without a hard fork, which would otherwise be needed to remove the malicious contract.

##### dWeb Constitution
The rights of users on the dWeb are ratified in the [dWeb Constitution](https://github.com/arisenio/constitution) and are immutably available within ARISEN's state. While the Constitution is immutable, it can be amended with a 15/21 vote.

##### Process for Constitutional Amendments & Software Upgrades
The dWeb and ARISEN are designed around the idea that "the code is the law." For this reason, the protocol itself, as defined in the canonical source surrounding dWeb's on-chain and off-chain protocols and dWeb's Constitution, can only be updated through the following process:
-1. A change to the dWeb Constitution is proposed by governance members and approved by a 15/21 vote.
-2. The governance is able to maintain this approval count for 30 consecutive days.
-3. All accounts on ARISEN, when processing a transaction, would have to accept this new Constitution as a condition for future transactions.
-4. Governance members must adopt changes to the overall ARISEN software, as well as `off-chain` implementations, to reflect the change in the dWeb Constitution and must propose it to the network using a hash of the dWeb Constitution.
-5. Governance members maintain 15/21 approval of the new code for 30 consecutive days.
-6. Since the approval for the new code has maintained approval for 30 days, the code will take effect 7 days later, giving all non-producing full nodes exactly one week to upgrade after the new changes to various canonical implementations.
-7. All ARISEN nodes on the network who do not upgrade to the new ARISEN release will shutdown automatically.

The process for software upgrades (updating the blockchain) will take anywhere from 2 to 3 months, while updates to fix non-critical bugs that do not require changes to the constitution can take 1 to 2 months.

##### Emergency Changes
Governance members may accelerate the process of a software upgrade if it involves a harmful bug or security exploit that is actively harming users. Although this does go against the dWeb Constitution. it is generally up to the elected governance to make these emergency decisions.

##### Governance Rewards
Being that governance members are also block producers, they're awarded new RIX coins every time they produce a block on ARISEN. The number of RIX that are minted is determined by the median of the desired pay published by all governance members. ARISEN may be configured to enforce a cap on governance rewards to where the total annual increase in RIX's coin supply does not exceed 5%.

##### dWeb Improved Proposal (DWIP)
While RIX holders can elect governance members, they can also craft dWeb Improvement Proposals (DWIP) that can be voted on by community members, whom can elect a number of DWIPs in order to advance the ideals of the community. Winning DWIPs will receive a certain amount of RIX via ARISEN's `savings account`, which is completely funded via inflation. DWIPs will only receive RIX proportional to the amount of votes each DWIP has received from RIX holders.

#### Distributed Computing Resources
Running contracts requires server capacity and there need to be safeguards against spam generated by these contracts. The resources needed are defined as RAM, Bandwidth (NET) and CPU. Each are explained below:

##### RAM
RAM is required to write data to the blockchain database, which takes up system capacity (change of state). Every account must have RAM to be a useable account (a minimum of 5 KB). The price of RAM fluctuates on an open market. The price of RAM fluctuates automatically depending on supply and demand. Extra RAM can be purchased for accounts, and this RAM can be sold at whatever the current market price is, when it is no longer needed.

Developers who deploy contracts will require more RAM than the standard account because RAM comes under the scrutiny of the account carrying out the function (change of state). This means the RAM requirements of a contract are paid for by the developer of the account that deployed it, not the user interacting with it. RAM is not traditional silicon RAM; rather, it refers to storage in ARISEN's distributed memory database, which is all the data currently being processed by the blockchain's collective CPU.

##### CPU
Refers to computation (processing power). This is accessed by staking a minimum of 3.0 RIX (depending on network load). This can be defined as how long transactions/actions run for. CPU is measured by average consumption in microseconds over a 3 day period, decreasing to 0 over time. This is the amount of time a transaction runs for with network bandwidth; in other words, it is the size of the transaction.

##### NET
The average consumption in bytes over a 3 day period, decreasing to 0 over time. This can be referred to as bandwidth and log storage required when sending a transaction (the number of transactions/actions). This is accessed by staking a minimum of 0.1 RIX (depending on network load).

#### Cryptocurrencies & Decentralized Payments
Like other blockchains, at the very foundation of ARISEN is the ability for developers to create their own digital currencies, also known as "tokens." For this reason, ARISEN is considered a multi-asset network. P2P currencies give way to decentralized payments, which enable developers to add economic features within their decentralized applications. For example, a decentralized social network built on ARISEN could launch its own currency. This currency could be used to pay users for their posts, thereby providing them incentive to join the network.

##### Cross-Chain Transfer Protocol (CTTP)
RIX and other ARISEN currencies can be transferred to/from wholly unrelated blockchains like Ethereum, EOS and TRON, using the Cross-Chain Transfer Protocol (CTTP). You can read the full specification [here](https://github.com/arisenio/cttp-whitepaper).

#### Blockchain Interoperability
ARISEN was designed to coordinate and facilitate blockchain interoperability through the generation of both `action existence` and `action sequence`. This combo, teamed with ARISEN's contract architecture designed around `action passing`, enables high-level abstractions that are ultimately presented to developers by concealing all blockchain communications and proof validations.

#### LCV
EOS first presented the concept of Merkle Proofs for Light Client Validation (LCV) - with the idea that all clients don't need to process all transactions - in order to make the integration with other blockchains much easier. For example, a decentralized social network that operates on an ARISEN-based blockchain would only be concerned with the transactions that are taking place within the social network itself. This is supported by the idea that a blockchain's block producers would want the smallest possible overhead when syncing with another blockchain.

ARISEN uses LCV to prove the existence of any transaction with a proof of less than 1024 bytes in size (a valid proof on the Bitcoin network is about 512 bytes). By using LCV, ARISEN can prove that a block is included in another blockchain by simply utilizing a specific `blockID`, as well as the corresponding headers of a block, as long as the block is trusted and irreversible. This sort of proof takes `ceil(log 2(N))` digests for its path, where `N` is the number of blocks in a blockchain that contains 100 million blocks, in 864 bytes.

Therefore, tracking all block headers is cheap at 420 MB/year and will ultimately keep proof sizes small. Although, over time it makes more sense to have one blockchain that contains the entire histories of other blockchains to completely eliminate the need for proofs. By minimizing the frequency of inter-chain proofs, we can further boost the network's performance.

##### Latency
Thanks to ARISEN's consensus algorithm, ARISEN-based blockchains are able to provide rapid irreversibility. This means there is only 0.5 second latency between ARISEN blockchains that use the same consensus model. This is because when one blockchain communicates with another, block producers have to wait until a transaction's irreversibility has been confirmed by the blockchain.

##### Proof of Completeness (POC)
ARISEN uses Proof of Completeness to prove there are no gaps in the transaction history when verifying transactions from a remote blockchain rather than trying to attempt to prove that all transactions on the other blockchain are valid, since it's impossible to prove that all of the most recent transactions are known. By identifying each action sent to an account by sequencing these actions, an ARISEN user is able to prove that each action intended for a particular ARISEN account has ultimately been processed in order.

##### SegWit
ARISEN utilizes the concept of Segregated Witness (SegWit) to eliminate the need for storing the SHA256 hashes used by proofs to derive blockchain state. Once a Merkle Proof is accepted and deemed irreversible, the 2KB of hashes used for derivation are no longer stored. As it applies to blockchain interoperability, savings are 32x greater than using typical signatures.

## DWEB
The previous sections identified the foundational protocols that together, form the DWEB protocol suite. This section will serve as an explanation for how these protocols work together to enable the simple and secure exchange of data between a web of peers, which in turn forms a decentralized web for the peer-to-peer exchange of information.

The dWeb is made possible through the following processes and models which take place across different dWeb protocol and data layers:

-[dWeb Network Addresses](#dweb-network-addresses)
-[Address Registration](#address-registration)
-[Address Announcement](#address-announcement)
-[Address Lookup](#address-lookup)
-[Peer Messaging Structure](#peer-messaging structure)
-[dWeb Data Model](#dweb-data-model)
-[Peer Data Exchange](#peer-data-exchange)

The sub-sections that follow explain how all of the above areas work, including a review of the [dWeb Data Model](#dweb-data-model) (that was explained in the [dDatabase](#ddatabase) section), which together ultimately allow for peers to host and distribute datasets, websites and web applications amongst themselves without a single central point of failure.

**NOTE:** This section covers version 8 of the DWEB protocol suite, which now includes a NOISE-based handshake for protocol-level encryption.

### dWeb Network Addresses
As discussed in [dDatabase](#ddatabase), a dWeb network address is a 32-byte hexadecimal address that can represent a device, distributed dataset (e.g. a distributed key/value database, distributed file system, etc.) or simply a binary data feed (e.g. a plain dDatabase feed).

A dWeb network address is identified by it's protocol identifying prefix: `dweb://40a7f6b6147ae695bcbcff432f684c7bb5291ea339c22c1755896cc`

If a network address represents a distributed file system like a [dDrive](#ddrive), the address may include a suffix that identifies specific files and/or folders within a dDrive, like so:
`dweb://40a7f6b6147ae695bcbcff432f684c7bb5291ea339c22c1755896cc/index.html`

In the case of a dDrive, this suffix could also include its version number:
`/?version=1`; or if the version is tagged, `/?version=live`

The `dweb://` identifier makes dWeb URLs easily identifiable, so that dWeb-capable applications can register protocols with an underlying operating system, which enables `dweb://` links to be programmatically recognizable (e.g., [dBrowser](http://dbrowser).

It's important to note that a dWeb address suffix can be used by abstractions other than a dDrive to represent other data structures (other than files/folders) on behalf of a dWeb-capable application that is ultimately designed to digest a particular data structure via a dWeb network address suffix. For example, a message app could receive new messages using the following format:
`dweb://<key>/{messageID}/{messagePayload}`

### Address Registration
When announcing a dWeb network address on dWeb's DHT, if the address being announced does not exist, an address must be announced on ARISEN prior to it being announced on the DHT. This not only provides a bridge between dWeb's off-chain peer discovery network and on-chain computing infrastructure, but further empowers dWeb's [decentralized domain name system](#ddns) and [decentralized reporting system](#reporting-system).

This is made possible by the `dweb` contract on ARISEN and the `register` action. The `register` action accepts the following parameters:

| Parameter Name | Data Type | Description |
| --- | --- | --- |
| address | name | The dWeb network address |
| structure | name | The data structure type |
| status | uint8 | 0 for live and 1 for blacklisted |

The action does not require the address creator to authenticate with the contract since the contract self-authenticates with the contract creator; in this case, the `@dweb` account on ARISEN auto-authenticates every time an action is executed and pays all RAM, CPU and NET fees.

A dWeb network address can then be blacklisted via a 15/21 vote by governance members, who can then modify the entry by switching the status to `1`.

### Address Announcement
Once an address is registered, it can be announced via dWeb's DHT using the DWDHT protocol. An announcing peer, whether it is the creator of the dWeb network address or a peer joining an already existent address, provides the following announcement parameters to the DHT:
```
{
  key: <dweb-key>,
  ip address: <peer-public-ip>,
  port: <peer-port-number>,
  localAddress: {
    localIP: <peer-local-ip>
  }
}
```

To stay subscribed to a particular dWeb network address, a peer (creating or joining peer), must re-announce themselves every 60 seconds. The DHT server will also cycle its tokens periodically, so a client should remember the token it received last and update it when they receive a new one.

### Address Lookup
When a dWeb client (e.g., a web browser) performs a lookup, all it needs is a dWeb network address. A dWeb client queries dWeb's DHT for the address and the DHT will return the following response for any given dWeb network address:

{
  node: {host, port},
  // List of peers
  peers: [{host, port}, ...],
  // List of LAN peers
  localPeers: [{host, port}, ...]
}

### Peer Messaging Structure
Once a peer has discovered another peer's IP address and port number, it will open a TCP connection to the other peer, which allows peers to exchange data. This peer messaging protocol was briefly described in [dDatabase Protocol](#ddatabase-protocol), and handles a large portion of DWEB's data exchange process. All message structures adhere to the Protocol Buffers data serialization format.

Each half of a peer exchange has the following structure, which repeats until the end of the connection:

| Field | Description |
| --- | --- |
| Length | Number of bytes until the start of the next length field |
| Channel and type | A single number (up to 11 bits long) that encodes two sub-fields |

-The `channel` and `type` subfields are as follows:

| Field | Description |
| --- | --- |
| Channel | Peer can exchange multiple data feeds using the same TCP connection. The channel number is `0` for the first feed, `1` for the second and so on.
| Type | Number relating to the type of message being sent |

-The following message types can be used in the `type` field:

| Type | Name | Meaning |
| --- | --- | --- |
| 0 | Feed | I want to talk to you about a dWeb address |
| 1 | Handshake | I want to negotiate how we can communicate over TCP |
| 2 | Info | Whether I'm starting or stopping uploading or downloading |
| 3 | Have | I have some data that you said you wanted |
| 4 | Unhave | I no longer have the data I said I had |
| 5 | Want | This is the data I want |
| 6 | Unwant | I no longer want this data |
| 7 | Request | Please send me this data now |
| 8 | Cancel | Cancel this `Request |
| 9 | Data | Here is the data you requested |
| 10-14 | (unused) |
| 15 | Custom extension message, not a part of the core protocol |

#### Bit Notation
Throughout the underlying dDatabase protocol, multiple fields will be packed into a single number. Looking at this number as a sequence of bits makes the fields within the message far more visible. It is important to note that the most significant bit is always on the left (big endian).

Eight bits make up an entire byte, however this number and many others are `varints` which can be up to 64 bits long. The fields on the right are always a fixed number of bits, but the leftmost field can be as much as 64 bits.

#### Varints
The first two fields are encoded as `variable-length integers` and therefore do not have a fixed size. Each field must be read from the beginning in order to determine the length of the field and where the next field begins.

Varints have the abilities to represent small numbers with a few bytes and large numbers by using more bytes. The only disadvantage is the overhead introduced during the process to encode and decode integers.

##### Keepalive
A keepalive message is a BGP-4 Message used to acknowledge an open message, and to periodically confirm that the other peer is still a part of the TCP relationship.

In the case of the dDatabase protocol, it's an empty message containing no channel number, type or body. They are discarded upon being received by the remote. This can also be used by peers looking to live sync a dDatabase feed, which means they would have to keep a TCP connection open inevitably, which in turn would equate to a steady stream of `keepalive` messages. The actual rate is determined by the underlying TCP settings.

#### Body Structure
Within each `Body` field, is a series of field tags and values:

-`Field Tag ` - varint
-The most significant indicates which field within the message this is:
--`1` = dWeb network address
--`2` = nonce
--The least significant bits are the type of the field.

The two types of fields are:
| Varint | The field tag followed by a varint value. Used for simple numeric values & booleans. |
| Length-prefixed | The field tag followed by a varint to say how many bytes the field contains. |

##### Message Pseudo-Representation
A visual representation of DWEB protocol messages is as follows:
```
-Length
-Channel/Type
--Channel
--Message Type
-Message Body
```
As you will see in subsequent sections, the `Message Body` contains different fields and content, dependent on the `Message Type`.

### Peer Connection Initiation
When a peer discovers a remote peer, a TCP connection is opened with the remote peer and the following takes place in order to prepare the connection for the exchange of data between peers:

#### 1. Feed Message Sent To Remote Peer
After opening the TCP connection, the first message sent to the remote peer is of the `Feed` type (0). The `Feed` `Message Body` has two fields:

| Field No. | Name | Type | Description |
| 1 | dWeb network address | Length-prefixed | dWeb network address |
| 2 | Nonce | Length-prefixed | 24-byte random nonce generated for this TCP connection |

This allows the remote peer to know which dWeb network address address you're interested in.

#### 2. Peers Exchange Handshake Message
After the `Feed` message is sent, a `Handshake` message is sent on each side of the TCP connection.

The `Handshake` `Message Body` has the following fields:

| Field No. | Name | Type | Description |
| --- | --- | --- | --- |
| 1 | ID | Length-prefixed | Random ID generated by this peer upon starting; 32 bytes long; used to detect multiple connections to the same peer.
| 2 | Live | Varint | 0 = end connection when neither peer is downloading; 1 = keep connection open indefinitely. |
| 3 | User data | Length-prefixed | Arbitrary bytes that can be used by higher-level applications for any purpose. |
| 4 | Extensions | Length-prefixed | Name of any extension the peer wants to use. |
| 5 | Acknowledge | Varint | 0 = no need to acknowledge each fragment of data received; 1 = must acknowledge each fragment of data. |

-The `Extensions` field (#4) can appear multiple times, one for each extension. Both peers need to request an extension in their handshake messages for it to become active.

This same structure, using DWEBv8, can be used for a NOISE-based handshake so that all messages are encrypted and MAC'd using NOISE's `XX` pattern. In the process, both peers exchange a keypair for the purpose of stream authentication (message authentication code). This way, both peers can be assured that each is receiving a message from the other, since both will end up generating the same MAC (checksum) from the actual message contents using the exchanged private key. For example, PeerA would encrypt the message using the secret as follows:

`MAC = E(K,M)` (where `K` is the key, `M` is the message and `E` is the encryption function)

The MAC is attached to the end of the received message (as a checksum) and sent to PeerB. PeerB, using the exchanged secret key, can perform `E`, derive a MAC checksum from the received message, and compare to the received MAC checksum. This confirms that the message derived from PeerA.

### dWeb Data Model
Technically, peers can exchange any kind of data. A [dDatabase](#ddatabase) is the first standardized distributed dataset for the dWeb and is used by abstractions like [dDrive](#ddrive) and [dWebTrie](#dwebtrie). Other custom abstractions of a dDatabase feed can be created, but at the end of the day, it's still a dDatabase, which means it's a bunch of entries that each equate to an abstract blob of data.

The dWeb protocol can work with any dDatabase-alternative as long as it contains a list (log or feed) of variable-sized fragments (entries) of bytes. A dDatabase is an immutable, cryptographically secure, append-only log, which means new fragments can be added to the end by dDatabase's author, but existing fragments can't be deleted or modified.

Therefore, the following data mode is used in the exchange of data between peers:

-Each fragment (index or entry) in the feed is separated by boundaries so that a feed can be sent over the course of several protocol messages.
-Each fragment has a corresponding hash to verify the integrity of the data.
-There are also parent hashes which verify the integrity of two other hashes. Parent hashes form a tree structure known as a Merkle tree.
-Fragment hashes are even-numbered and parent hashes are odd-numbered.
-Hash trees eliminate the need for downloaders to verify a specific fragment without having to download the entire Merkle tree.
-Each time the dDatabase creator appends data (adds a new fragment), a new root hash is calculated and signed with the creator's private key.
-A remote peer who is downloading an entire feed or a specific fragment can use the creator's public key to verify the signature, which, as a result, verifies the integrity of other fragments and hashes.
